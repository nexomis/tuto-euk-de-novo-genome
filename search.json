[
  {
    "objectID": "tertiary-analysis/genome-annotation.html",
    "href": "tertiary-analysis/genome-annotation.html",
    "title": "Genome Annotation",
    "section": "",
    "text": "1 Genome Annotation\nContent for Genome Annotation."
  },
  {
    "objectID": "secondary-analysis/scaffolding.html",
    "href": "secondary-analysis/scaffolding.html",
    "title": "Scaffolding",
    "section": "",
    "text": "1 Scaffolding\nContent for Scaffolding."
  },
  {
    "objectID": "secondary-analysis/concepts-genome-assembly.html",
    "href": "secondary-analysis/concepts-genome-assembly.html",
    "title": "Genome Assembly Concepts",
    "section": "",
    "text": "1 Concepts in Genome Assembly\nContent for Concepts in Genome Assembly."
  },
  {
    "objectID": "primary-analysis/preliminary-analysis.html",
    "href": "primary-analysis/preliminary-analysis.html",
    "title": "Preliminary Analysis: Contamination Detection and Genome Scoping",
    "section": "",
    "text": "Before diving into the computationally intensive process of genome assembly, a preliminary analysis of the sequencing data is crucial. This step helps in identifying potential issues like contamination and in estimating key characteristics of the genome. This “genome scoping” allows for a more informed approach to assembly, helping to choose the right tools and parameters.\n\n\nSequencing samples can be contaminated with DNA from various sources, such as bacteria, viruses, or even DNA from other organisms handled in the lab. Detecting and understanding this contamination is essential as it can significantly impact the quality of the final assembly.\n\n\nAs we saw in the primary analysis chapter, tools like FastQC (Andrews 2010) provide a first look at the data quality. The “Per sequence GC content” plot is particularly useful for detecting contamination. A unimodal distribution of GC content is expected for a pure sample. If the plot shows multiple peaks, it may indicate the presence of DNA from different organisms with different GC contents.\n\n\n\nContamination from FASTQC\n\n\n\n\n\nA more direct way to identify contamination is to screen the sequencing reads against a panel of genomes from potential contaminants. Fastq-Screen (Wingett and Andrews, n.d.) is a tool designed for this purpose. It takes a subset of the reads and aligns them against multiple reference genomes using an aligner like Bowtie2 (Langmead and Salzberg 2012) or BWA (Li and Durbin 2010). The output shows the percentage of reads mapping to each genome, giving a clear picture of the sample’s composition.\n\n\n\nHow to interprete fastqc_sreen from: https://www.bioinformatics.babraham.ac.uk/projects/fastq_screen/\n\n\nWhile effective, this alignment-based approach can be time-consuming, especially with large datasets and many potential contaminant genomes.\n\n\n\nModern tools have shifted towards alignment-free methods for faster and more efficient taxonomic classification. Kraken2 (Wood, Lu, and Langmead 2019) is a widely used tool in this category. It uses a k-mer-based approach to assign a taxonomic label to each read.\n\n\nA k-mer is a substring of length k from a DNA sequence. For example, the sequence AGATTACA has the following 5-mers (k=5): AGATT, GATTA, ATTAC, TTACA.\nThe core idea behind k-mer-based methods is that every species has a unique set of k-mers in its genome. By breaking down reads into their constituent k-mers and looking them up in a pre-built database, we can rapidly identify the species of origin for each read.\n\n\n\nKraken2 achieves its remarkable speed and memory efficiency through a more sophisticated approach than simply storing all k-mers. Here are the key concepts:\n\n\nInstead of storing every k-mer, Kraken2 works with minimizers. A minimizer is the lexicographically smallest substring of length ℓ found within a k-mer (including its reverse complement). This concept dramatically reduces storage requirements.\nLet’s illustrate with a simple example using 5-mers and minimizers of length 3:\n\n\n\nPosition\n5-mer\nAll 3-mers in 5-mer\nMinimizer (smallest)\n\n\n\n\n1\nATCGG\nATC, TCG, CGG\nATC\n\n\n2\nTCGGA\nTCG, CGG, GGA\nCGG\n\n\n3\nCGGAT\nCGG, GGA, GAT\nCGG\n\n\n4\nGGATC\nGGA, GAT, ATC\nATC\n\n\n\nNotice how consecutive k-mers often share the same minimizer (CGG appears in positions 2 and 3). This property means that instead of storing 4 different 5-mers, we only need to store 2 minimizers, significantly reducing storage requirements.\n\n\n\nDifferences in operation between Kraken and Kraken2\n\n\nHow Kraken2 uses minimizers:\n\nStorage of Minimizers: For each k-mer, Kraken2 computes its minimizer and stores only the minimizer in the database, not the full k-mer. The Lowest Common Ancestor (LCA) of a k-mer is assumed to be the same as its minimizer’s LCA.\nSpaced Seeds: To improve classification accuracy, Kraken2 uses spaced seeds where certain positions in the minimizer are masked (not considered during search). Starting from the next-to-rightmost position, every other position is masked until the specified number of positions are masked. This approach increases sensitivity while decreasing specificity, allowing better detection of related sequences with mutations.\nCompact Hash Table: The database itself is not a simple list but a compact hash table. This is a probabilistic data structure that maps minimizers to their taxonomic LCA. It allows for extremely fast queries and low memory usage. However, being a probabilistic structure, it has a very small chance of returning an incorrect result. This is a trade-off for its speed, and users can apply confidence scoring thresholds to filter out low-confidence assignments and mitigate this possibility.\n\nThis combination of techniques avoids the need for slow alignment algorithms and makes it possible to classify millions of reads in minutes, even on a standard laptop.\nThe output of Kraken2 is a report that details the taxonomic composition of the sample, allowing for precise identification of the target organism and any contaminants.\n\n\n\n\nOne of the key advantages of Kraken2 is the availability of pre-built databases that can be easily downloaded and used. The Index Zone by Ben Langmead provides regularly updated databases with different taxonomic coverage:\n\nMinusB: Contains RefSeq archaea, viral, plasmid, human, and UniVec_Core sequences (~10 GB). This database excludes bacterial genomes, making it useful for detecting non-bacterial contamination.\nStandard: The most commonly used database, containing RefSeq archaea, bacteria, viral, plasmid, human, and UniVec_Core sequences (~87 GB). This provides broad taxonomic coverage for most contamination detection needs.\nPlusPF: Extends the Standard database with protozoa and fungi (~93 GB), useful for eukaryotic samples that might contain these contaminants.\ncore_nt: A massive database containing sequences from GenBank, RefSeq, TPA, and PDB (~278 GB). This comprehensive collection demonstrates the true scalability advantage of k-mer-based methods.\n\nThe ability to use such large databases like core_nt highlights a fundamental advantage of Kraken2 over alignment-based methods. While building Bowtie2 or BWA indexes for hundreds of gigabytes of reference sequences would be computationally prohibitive and memory-intensive, Kraken2’s k-mer-based approach makes this scale of analysis practical and fast. This scalability is what enables comprehensive contamination screening that would be unrealistic with traditional alignment approaches.\n\n\n\n\n\nBeyond contamination detection, k-mer analysis is a powerful technique for estimating key genome characteristics directly from the raw sequencing reads, without the need for an assembly. Tools like KmerGenie and KAT (K-mer Analysis Toolkit) are specialized for this purpose.\n\n\nThe foundation of genome scoping is fast and efficient k-mer counting. Tools like KAT are often built upon specialized k-mer counting engines like Jellyfish. Jellyfish is designed to count k-mers using an order of magnitude less memory and time compared to other methods. It achieves this through several key innovations:\n\nMemory-Efficient Hash Table: It uses a highly optimized, memory-efficient hash table structure to store and count the k-mers, minimizing the RAM footprint. Jellyfish exploits the fact that a k-mer’s position in the hash table already encodes part of its information. By using bijective hash functions, it can store only the high-order bits of the encoded k-mer, significantly reducing the storage space per k-mer.\nParallelism: It is designed for modern multi-core processors and uses lock-free data structures by exploiting the “compare-and-swap” CPU instruction. This allows multiple threads to count k-mers in parallel with minimal overhead, dramatically increasing speed.\n\nBy using such an efficient engine, tools like KAT can process massive sequencing datasets quickly to generate a k-mer spectrum—a frequency plot showing how many k-mers appear a certain number of times. This plot provides a wealth of information:\n\nGenome Size: The total number of unique k-mers in the genome can be estimated from the main peak of the k-mer spectrum.\nPloidy and Heterozygosity: In a diploid organism, heterozygous sites will create unique k-mers for each allele, resulting in a peak at half the coverage of the main homozygous peak. The size of this heterozygous peak relative to the homozygous peak can be used to estimate the overall heterozygosity of the genome.\nContamination: Contamination can sometimes be seen as additional, smaller peaks in the k-mer spectrum.\nRepeat Content: A large number of high-frequency k-mers can indicate a high proportion of repetitive elements in the genome.\n\n\n\n\nExample of k-mer spectrum from https://kat.readthedocs.io/en/latest/kmer.html\n\n\nThis analysis is invaluable for planning the assembly. For example, a highly heterozygous or repetitive genome might require a specific type of assembly algorithm or long-read data to resolve correctly.\n\n\n\n\nSequencing technologies are not perfect and introduce errors into the reads. These errors can complicate the assembly process by creating false k-mers and breaking contiguity. Read correction algorithms aim to identify and fix these errors before assembly.\nThese algorithms often work by using a k-mer-based approach. They build a k-mer spectrum from the data and assume that low-frequency k-mers are likely to be erroneous. They then try to “correct” these erroneous k-mers by changing them to high-frequency k-mers that are a small edit distance away.\nWhile standalone read correction tools exist, it’s important to note that most modern genome assembly software, such as SPAdes or MaSuRCA, have their own built-in error correction modules. These integrated modules are often optimized for the specific assembly algorithm being used. Therefore, we will not run a separate read correction step but will instead rely on the capabilities of the assemblers we will use in the upcoming chapters."
  },
  {
    "objectID": "primary-analysis/preliminary-analysis.html#contamination-detection",
    "href": "primary-analysis/preliminary-analysis.html#contamination-detection",
    "title": "Preliminary Analysis: Contamination Detection and Genome Scoping",
    "section": "",
    "text": "Sequencing samples can be contaminated with DNA from various sources, such as bacteria, viruses, or even DNA from other organisms handled in the lab. Detecting and understanding this contamination is essential as it can significantly impact the quality of the final assembly.\n\n\nAs we saw in the primary analysis chapter, tools like FastQC (Andrews 2010) provide a first look at the data quality. The “Per sequence GC content” plot is particularly useful for detecting contamination. A unimodal distribution of GC content is expected for a pure sample. If the plot shows multiple peaks, it may indicate the presence of DNA from different organisms with different GC contents.\n\n\n\nContamination from FASTQC\n\n\n\n\n\nA more direct way to identify contamination is to screen the sequencing reads against a panel of genomes from potential contaminants. Fastq-Screen (Wingett and Andrews, n.d.) is a tool designed for this purpose. It takes a subset of the reads and aligns them against multiple reference genomes using an aligner like Bowtie2 (Langmead and Salzberg 2012) or BWA (Li and Durbin 2010). The output shows the percentage of reads mapping to each genome, giving a clear picture of the sample’s composition.\n\n\n\nHow to interprete fastqc_sreen from: https://www.bioinformatics.babraham.ac.uk/projects/fastq_screen/\n\n\nWhile effective, this alignment-based approach can be time-consuming, especially with large datasets and many potential contaminant genomes.\n\n\n\nModern tools have shifted towards alignment-free methods for faster and more efficient taxonomic classification. Kraken2 (Wood, Lu, and Langmead 2019) is a widely used tool in this category. It uses a k-mer-based approach to assign a taxonomic label to each read.\n\n\nA k-mer is a substring of length k from a DNA sequence. For example, the sequence AGATTACA has the following 5-mers (k=5): AGATT, GATTA, ATTAC, TTACA.\nThe core idea behind k-mer-based methods is that every species has a unique set of k-mers in its genome. By breaking down reads into their constituent k-mers and looking them up in a pre-built database, we can rapidly identify the species of origin for each read.\n\n\n\nKraken2 achieves its remarkable speed and memory efficiency through a more sophisticated approach than simply storing all k-mers. Here are the key concepts:\n\n\nInstead of storing every k-mer, Kraken2 works with minimizers. A minimizer is the lexicographically smallest substring of length ℓ found within a k-mer (including its reverse complement). This concept dramatically reduces storage requirements.\nLet’s illustrate with a simple example using 5-mers and minimizers of length 3:\n\n\n\nPosition\n5-mer\nAll 3-mers in 5-mer\nMinimizer (smallest)\n\n\n\n\n1\nATCGG\nATC, TCG, CGG\nATC\n\n\n2\nTCGGA\nTCG, CGG, GGA\nCGG\n\n\n3\nCGGAT\nCGG, GGA, GAT\nCGG\n\n\n4\nGGATC\nGGA, GAT, ATC\nATC\n\n\n\nNotice how consecutive k-mers often share the same minimizer (CGG appears in positions 2 and 3). This property means that instead of storing 4 different 5-mers, we only need to store 2 minimizers, significantly reducing storage requirements.\n\n\n\nDifferences in operation between Kraken and Kraken2\n\n\nHow Kraken2 uses minimizers:\n\nStorage of Minimizers: For each k-mer, Kraken2 computes its minimizer and stores only the minimizer in the database, not the full k-mer. The Lowest Common Ancestor (LCA) of a k-mer is assumed to be the same as its minimizer’s LCA.\nSpaced Seeds: To improve classification accuracy, Kraken2 uses spaced seeds where certain positions in the minimizer are masked (not considered during search). Starting from the next-to-rightmost position, every other position is masked until the specified number of positions are masked. This approach increases sensitivity while decreasing specificity, allowing better detection of related sequences with mutations.\nCompact Hash Table: The database itself is not a simple list but a compact hash table. This is a probabilistic data structure that maps minimizers to their taxonomic LCA. It allows for extremely fast queries and low memory usage. However, being a probabilistic structure, it has a very small chance of returning an incorrect result. This is a trade-off for its speed, and users can apply confidence scoring thresholds to filter out low-confidence assignments and mitigate this possibility.\n\nThis combination of techniques avoids the need for slow alignment algorithms and makes it possible to classify millions of reads in minutes, even on a standard laptop.\nThe output of Kraken2 is a report that details the taxonomic composition of the sample, allowing for precise identification of the target organism and any contaminants.\n\n\n\n\nOne of the key advantages of Kraken2 is the availability of pre-built databases that can be easily downloaded and used. The Index Zone by Ben Langmead provides regularly updated databases with different taxonomic coverage:\n\nMinusB: Contains RefSeq archaea, viral, plasmid, human, and UniVec_Core sequences (~10 GB). This database excludes bacterial genomes, making it useful for detecting non-bacterial contamination.\nStandard: The most commonly used database, containing RefSeq archaea, bacteria, viral, plasmid, human, and UniVec_Core sequences (~87 GB). This provides broad taxonomic coverage for most contamination detection needs.\nPlusPF: Extends the Standard database with protozoa and fungi (~93 GB), useful for eukaryotic samples that might contain these contaminants.\ncore_nt: A massive database containing sequences from GenBank, RefSeq, TPA, and PDB (~278 GB). This comprehensive collection demonstrates the true scalability advantage of k-mer-based methods.\n\nThe ability to use such large databases like core_nt highlights a fundamental advantage of Kraken2 over alignment-based methods. While building Bowtie2 or BWA indexes for hundreds of gigabytes of reference sequences would be computationally prohibitive and memory-intensive, Kraken2’s k-mer-based approach makes this scale of analysis practical and fast. This scalability is what enables comprehensive contamination screening that would be unrealistic with traditional alignment approaches."
  },
  {
    "objectID": "primary-analysis/preliminary-analysis.html#genome-scoping-with-k-mer-analysis",
    "href": "primary-analysis/preliminary-analysis.html#genome-scoping-with-k-mer-analysis",
    "title": "Preliminary Analysis: Contamination Detection and Genome Scoping",
    "section": "",
    "text": "Beyond contamination detection, k-mer analysis is a powerful technique for estimating key genome characteristics directly from the raw sequencing reads, without the need for an assembly. Tools like KmerGenie and KAT (K-mer Analysis Toolkit) are specialized for this purpose.\n\n\nThe foundation of genome scoping is fast and efficient k-mer counting. Tools like KAT are often built upon specialized k-mer counting engines like Jellyfish. Jellyfish is designed to count k-mers using an order of magnitude less memory and time compared to other methods. It achieves this through several key innovations:\n\nMemory-Efficient Hash Table: It uses a highly optimized, memory-efficient hash table structure to store and count the k-mers, minimizing the RAM footprint. Jellyfish exploits the fact that a k-mer’s position in the hash table already encodes part of its information. By using bijective hash functions, it can store only the high-order bits of the encoded k-mer, significantly reducing the storage space per k-mer.\nParallelism: It is designed for modern multi-core processors and uses lock-free data structures by exploiting the “compare-and-swap” CPU instruction. This allows multiple threads to count k-mers in parallel with minimal overhead, dramatically increasing speed.\n\nBy using such an efficient engine, tools like KAT can process massive sequencing datasets quickly to generate a k-mer spectrum—a frequency plot showing how many k-mers appear a certain number of times. This plot provides a wealth of information:\n\nGenome Size: The total number of unique k-mers in the genome can be estimated from the main peak of the k-mer spectrum.\nPloidy and Heterozygosity: In a diploid organism, heterozygous sites will create unique k-mers for each allele, resulting in a peak at half the coverage of the main homozygous peak. The size of this heterozygous peak relative to the homozygous peak can be used to estimate the overall heterozygosity of the genome.\nContamination: Contamination can sometimes be seen as additional, smaller peaks in the k-mer spectrum.\nRepeat Content: A large number of high-frequency k-mers can indicate a high proportion of repetitive elements in the genome.\n\n\n\n\nExample of k-mer spectrum from https://kat.readthedocs.io/en/latest/kmer.html\n\n\nThis analysis is invaluable for planning the assembly. For example, a highly heterozygous or repetitive genome might require a specific type of assembly algorithm or long-read data to resolve correctly."
  },
  {
    "objectID": "primary-analysis/preliminary-analysis.html#a-note-on-read-correction",
    "href": "primary-analysis/preliminary-analysis.html#a-note-on-read-correction",
    "title": "Preliminary Analysis: Contamination Detection and Genome Scoping",
    "section": "",
    "text": "Sequencing technologies are not perfect and introduce errors into the reads. These errors can complicate the assembly process by creating false k-mers and breaking contiguity. Read correction algorithms aim to identify and fix these errors before assembly.\nThese algorithms often work by using a k-mer-based approach. They build a k-mer spectrum from the data and assume that low-frequency k-mers are likely to be erroneous. They then try to “correct” these erroneous k-mers by changing them to high-frequency k-mers that are a small edit distance away.\nWhile standalone read correction tools exist, it’s important to note that most modern genome assembly software, such as SPAdes or MaSuRCA, have their own built-in error correction modules. These integrated modules are often optimized for the specific assembly algorithm being used. Therefore, we will not run a separate read correction step but will instead rely on the capabilities of the assemblers we will use in the upcoming chapters."
  },
  {
    "objectID": "more/reference-enabled-bioinformatics.html",
    "href": "more/reference-enabled-bioinformatics.html",
    "title": "Reference-enabled Bioinformatics",
    "section": "",
    "text": "1 Reference-enabled Bio-informatics analysis\nContent for Reference-enabled Bio-informatics analysis."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "The objective of this tutorial is to learn de novo genome assembly for relatively small genomes (e.g. Fungii).\nThis resource will provide you with the basic concepts in de novo genome assembly from and will distinguish three critical step for analysis.\n\nPrimary analysis: In this step we are processing raw data from sequencing technology to obtain high quality data for later processing. Every step at this point is not specific of de novo assembly and common to many bio-informatics analysis.\nSecondary analysis: Secondary analysis refer here the the main type of analysis which is here de novo assembly i.e. the obtention the la largest continuous or scaffold sequence representative of the target individual. That includes quality controls.\nTertiary Analysis refers to what can be done beyond the resolution of an assembly such as comparative genomics or annotation.\n\n\n\n\nMaster every steps of de novo assembly for eukaryote genomes\nInterpret the results at every steps\nMaster the use the CLI tools\nBecome fluent with apptainer/docker\nBe autonomous and independent for debugging\n\n\n\n\n\nBe fluent in Unix/Linux\nWork on linux or eventually WSL2 on windows\nBe familiar with the use of apptainer/docker or conda\nMeets the performance criteria, at least 8 threads & 32GB RAM would be good for starts\n\n\n\n\n\n\n\nTry tool -h, tool –help, tool -help, or just tool or eventually man tool. Eventually in combination with grep.\nTry the tool’s website https://www.bioinformatics.babraham.ac.uk/projects/fastqc/\nOr maybe the GitHub/GitLab: https://github.com/s-andrews/FastQC\nLook eventually at the paper describing the tool.\n\n\n\n\n1. File Not Found (FileNotFoundError or “No such file or directory”)\n\nExample error message: bash: ./script.sh: No such file or directory or python: FileNotFoundError: [Errno 2] No such file or directory\nTypical cause: Incorrect path, typo in filename, or missing input files.\n\n2. Out of Memory (OOM_kill or “Killed”)\n\nExample error message: Killed (on Linux, when a process is stopped by the system’s out-of-memory killer), or MemoryError (in Python, R).\nTypical cause: Trying to process files or data too large for the available RAM.\n\n3. Cannot Find Shared Library (cannot open shared object file: No such file or directory, cannot find xxx.so)\n\nExample error message: error while loading shared libraries: libXYZ.so: cannot open shared object file: No such file or directory\nTypical cause: Missing dependencies, or environment (e.g., LD_LIBRARY_PATH) not set correctly after software installation.\nhints: Search for any package that contains this file.\n\n4. Permission Denied (PermissionError or “Permission denied”)\n\nExample error message: bash: ./some_script.sh: Permission denied, or in Python, PermissionError: [Errno 13] Permission denied\nTypical cause: Trying to run or write to a file/folder without the necessary permissions.\n\n5. Segmentation Fault (Segmentation fault (core dumped))\n\nExample error message: Segmentation fault (core dumped)\nTypical cause: Bugs in compiled programs (common in C/C++ binaries used in bioinformatics), or sometimes hardware issues.\nhints: Check for corrupted or incorrect input files and formats.\n\n\n\n\n\nGeneral Search Engine such as DuckDuckGo or Google or Bing\nGitHub/GitLab issues or Discussions\nCommunity chat (Slack, Discord, Matrix) or forums (Reddit r/bioinformatics, Slack Overflow, Biostars)\nEventually but EXTRA CAREFULLY: AI assistance & LLMs"
  },
  {
    "objectID": "index.html#objectives",
    "href": "index.html#objectives",
    "title": "Home",
    "section": "",
    "text": "Master every steps of de novo assembly for eukaryote genomes\nInterpret the results at every steps\nMaster the use the CLI tools\nBecome fluent with apptainer/docker\nBe autonomous and independent for debugging"
  },
  {
    "objectID": "index.html#requirements",
    "href": "index.html#requirements",
    "title": "Home",
    "section": "",
    "text": "Be fluent in Unix/Linux\nWork on linux or eventually WSL2 on windows\nBe familiar with the use of apptainer/docker or conda\nMeets the performance criteria, at least 8 threads & 32GB RAM would be good for starts"
  },
  {
    "objectID": "index.html#important-notes",
    "href": "index.html#important-notes",
    "title": "Home",
    "section": "",
    "text": "Try tool -h, tool –help, tool -help, or just tool or eventually man tool. Eventually in combination with grep.\nTry the tool’s website https://www.bioinformatics.babraham.ac.uk/projects/fastqc/\nOr maybe the GitHub/GitLab: https://github.com/s-andrews/FastQC\nLook eventually at the paper describing the tool.\n\n\n\n\n1. File Not Found (FileNotFoundError or “No such file or directory”)\n\nExample error message: bash: ./script.sh: No such file or directory or python: FileNotFoundError: [Errno 2] No such file or directory\nTypical cause: Incorrect path, typo in filename, or missing input files.\n\n2. Out of Memory (OOM_kill or “Killed”)\n\nExample error message: Killed (on Linux, when a process is stopped by the system’s out-of-memory killer), or MemoryError (in Python, R).\nTypical cause: Trying to process files or data too large for the available RAM.\n\n3. Cannot Find Shared Library (cannot open shared object file: No such file or directory, cannot find xxx.so)\n\nExample error message: error while loading shared libraries: libXYZ.so: cannot open shared object file: No such file or directory\nTypical cause: Missing dependencies, or environment (e.g., LD_LIBRARY_PATH) not set correctly after software installation.\nhints: Search for any package that contains this file.\n\n4. Permission Denied (PermissionError or “Permission denied”)\n\nExample error message: bash: ./some_script.sh: Permission denied, or in Python, PermissionError: [Errno 13] Permission denied\nTypical cause: Trying to run or write to a file/folder without the necessary permissions.\n\n5. Segmentation Fault (Segmentation fault (core dumped))\n\nExample error message: Segmentation fault (core dumped)\nTypical cause: Bugs in compiled programs (common in C/C++ binaries used in bioinformatics), or sometimes hardware issues.\nhints: Check for corrupted or incorrect input files and formats.\n\n\n\n\n\nGeneral Search Engine such as DuckDuckGo or Google or Bing\nGitHub/GitLab issues or Discussions\nCommunity chat (Slack, Discord, Matrix) or forums (Reddit r/bioinformatics, Slack Overflow, Biostars)\nEventually but EXTRA CAREFULLY: AI assistance & LLMs"
  },
  {
    "objectID": "hands-on/10-annot-functional.html",
    "href": "hands-on/10-annot-functional.html",
    "title": "Functional Annotation",
    "section": "",
    "text": "In this chapter, we will perform functional annotation of the predicted proteins from our genome assembly. This process involves assigning biological functions to the proteins based on their similarity to known proteins and the presence of conserved domains. We will use two widely used tools for this purpose: InterProScan and eggNOG-mapper.\n\n\n\n\nWe will define environment variables for our sample name, the number of CPUs, and the path to the proteome file generated in the previous step (gene prediction).\nexport SN=Fo47\nexport NCPUS=12\nexport proteome=\"$PWD/09-braker/${SN}/annot.prot.fa\"\n# Note: The path to the proteome file assumes it was generated by BRAKER in the previous chapter.\n# Adjust the path if your file is located elsewhere.\n\n\n\nWe will use Apptainer to run InterProScan and eggNOG-mapper. We will define aliases for them.\nalias interproscan_app=\"apptainer run docker://interpro/interproscan:5.75-106.0\"\nalias eggnog-mapper=\"apptainer run docker://quay.io/biocontainers/eggnog-mapper:2.1.13--pyhdfd78af_0\"\n\n\n\n\nInterProScan is a tool that scans protein sequences against InterPro’s predictive models, which are provided by its member databases. This provides a comprehensive overview of protein function by identifying protein domains and families.\nMore information can be found in the InterProScan documentation.\n\n\nFirst, we need to download the InterProScan data. This is a large download, so it may take some time.\nmkdir -p 10-functional-annotation/interproscan\ncurl -o 10-functional-annotation/interproscan/interproscan-data-5.75-106.0.tar.gz http://ftp.ebi.ac.uk/pub/software/unix/iprscan/5/5.75-106.0/alt/interproscan-data-5.75-106.0.tar.gz\n\ntar -pxzf 10-functional-annotation/interproscan/interproscan-data-5.75-106.0.tar.gz -C 10-functional-annotation/interproscan\n\n\n\nNow we can run InterProScan on our proteome file. We will create directories for input, temporary files, and output.\nmkdir -p 10-functional-annotation/interproscan/{input,temp,output}\ncp $proteome 10-functional-annotation/interproscan/input/\n\n# We need to bind the data, input, temp, and output directories to the container.\n# Note that we are using the full path to the directories.\nDATADIR=$PWD/10-functional-annotation/interproscan/interproscan-5.75-106.0/data\nINPUTDIR=$PWD/10-functional-annotation/interproscan/input\nTEMPDIR=$PWD/10-functional-annotation/interproscan/temp\nOUTPUTDIR=$PWD/10-functional-annotation/interproscan/output\n\n# We build a full command with bind mounts instead of using the simple alias\napptainer run \\\n    --bind $DATADIR:/opt/interproscan/data \\\n    --bind $INPUTDIR:/input \\\n    --bind $TEMPDIR:/temp \\\n    --bind $OUTPUTDIR:/output \\\n    docker://interpro/interproscan:5.75-106.0 \\\n    --input /input/$(basename $proteome) \\\n    --output-dir /output \\\n    --tempdir /temp \\\n    --cpu $NCPUS \\\n    --goterms \\\n    --pathways\n\nTo restrict the analysis to specific databases (e.g., those most relevant for fungi), you can use the --applications flag followed by a comma-separated list of databases (e.g., --applications Pfam,SUPERFAMILY,CDD).\n\n\n\n\n\neggNOG-mapper is a tool for fast functional annotation of novel sequences using precomputed orthologous groups from the eggNOG database.\nMore information can be found in the eggNOG-mapper documentation.\n\n\nWe need to download the eggNOG-mapper data. We will use the download_eggnog_data.py script provided in the container. As requested, we will create a database specifically for Fungi to refine our analysis.\nmkdir -p 10-functional-annotation/eggnog\n\n# Download the main eggnog databases\neggnog-mapper download_eggnog_data.py -y --data_dir 10-functional-annotation/eggnog\n\n# Create a diamond database for Fungi.\n# This will download all eggNOG proteins first if not present, then create the fungi-specific database.\neggnog-mapper create_dbs.py -m diamond --dbname fungi --taxa Fungi --data_dir 10-functional-annotation/eggnog\n\n\n\nNow we can run eggNOG-mapper on our proteome file using the custom fungi database.\nmkdir -p 10-functional-annotation/eggnog/output\n\n# We need to bind the data directory and the input/output directories.\nDATADIR=$PWD/10-functional-annotation/eggnog\nINPUTFILE=$proteome\nOUTPUTDIR=$PWD/10-functional-annotation/eggnog/output\nOUTPUT_PREFIX=${SN}_eggnog\n\n# We build a full command with bind mounts to ensure correct paths inside the container.\napptainer run \\\n    --bind $DATADIR:/data \\\n    --bind $(dirname $INPUTFILE):/input \\\n    --bind $OUTPUTDIR:/output \\\n    docker://quay.io/biocontainers/eggnog-mapper:2.1.13--pyhdfd78af_0 \\\n    emapper.py \\\n    -i /input/$(basename $INPUTFILE) \\\n    -o /output/$OUTPUT_PREFIX \\\n    --data_dir /data \\\n    --dmnd_db /data/fungi.dmnd \\\n    --cpu $NCPUS \\\n    --override"
  },
  {
    "objectID": "hands-on/10-annot-functional.html#configuration-and-setup",
    "href": "hands-on/10-annot-functional.html#configuration-and-setup",
    "title": "Functional Annotation",
    "section": "",
    "text": "We will define environment variables for our sample name, the number of CPUs, and the path to the proteome file generated in the previous step (gene prediction).\nexport SN=Fo47\nexport NCPUS=12\nexport proteome=\"$PWD/09-braker/${SN}/annot.prot.fa\"\n# Note: The path to the proteome file assumes it was generated by BRAKER in the previous chapter.\n# Adjust the path if your file is located elsewhere.\n\n\n\nWe will use Apptainer to run InterProScan and eggNOG-mapper. We will define aliases for them.\nalias interproscan_app=\"apptainer run docker://interpro/interproscan:5.75-106.0\"\nalias eggnog-mapper=\"apptainer run docker://quay.io/biocontainers/eggnog-mapper:2.1.13--pyhdfd78af_0\""
  },
  {
    "objectID": "hands-on/10-annot-functional.html#interproscan",
    "href": "hands-on/10-annot-functional.html#interproscan",
    "title": "Functional Annotation",
    "section": "",
    "text": "InterProScan is a tool that scans protein sequences against InterPro’s predictive models, which are provided by its member databases. This provides a comprehensive overview of protein function by identifying protein domains and families.\nMore information can be found in the InterProScan documentation.\n\n\nFirst, we need to download the InterProScan data. This is a large download, so it may take some time.\nmkdir -p 10-functional-annotation/interproscan\ncurl -o 10-functional-annotation/interproscan/interproscan-data-5.75-106.0.tar.gz http://ftp.ebi.ac.uk/pub/software/unix/iprscan/5/5.75-106.0/alt/interproscan-data-5.75-106.0.tar.gz\n\ntar -pxzf 10-functional-annotation/interproscan/interproscan-data-5.75-106.0.tar.gz -C 10-functional-annotation/interproscan\n\n\n\nNow we can run InterProScan on our proteome file. We will create directories for input, temporary files, and output.\nmkdir -p 10-functional-annotation/interproscan/{input,temp,output}\ncp $proteome 10-functional-annotation/interproscan/input/\n\n# We need to bind the data, input, temp, and output directories to the container.\n# Note that we are using the full path to the directories.\nDATADIR=$PWD/10-functional-annotation/interproscan/interproscan-5.75-106.0/data\nINPUTDIR=$PWD/10-functional-annotation/interproscan/input\nTEMPDIR=$PWD/10-functional-annotation/interproscan/temp\nOUTPUTDIR=$PWD/10-functional-annotation/interproscan/output\n\n# We build a full command with bind mounts instead of using the simple alias\napptainer run \\\n    --bind $DATADIR:/opt/interproscan/data \\\n    --bind $INPUTDIR:/input \\\n    --bind $TEMPDIR:/temp \\\n    --bind $OUTPUTDIR:/output \\\n    docker://interpro/interproscan:5.75-106.0 \\\n    --input /input/$(basename $proteome) \\\n    --output-dir /output \\\n    --tempdir /temp \\\n    --cpu $NCPUS \\\n    --goterms \\\n    --pathways\n\nTo restrict the analysis to specific databases (e.g., those most relevant for fungi), you can use the --applications flag followed by a comma-separated list of databases (e.g., --applications Pfam,SUPERFAMILY,CDD)."
  },
  {
    "objectID": "hands-on/10-annot-functional.html#eggnog-mapper",
    "href": "hands-on/10-annot-functional.html#eggnog-mapper",
    "title": "Functional Annotation",
    "section": "",
    "text": "eggNOG-mapper is a tool for fast functional annotation of novel sequences using precomputed orthologous groups from the eggNOG database.\nMore information can be found in the eggNOG-mapper documentation.\n\n\nWe need to download the eggNOG-mapper data. We will use the download_eggnog_data.py script provided in the container. As requested, we will create a database specifically for Fungi to refine our analysis.\nmkdir -p 10-functional-annotation/eggnog\n\n# Download the main eggnog databases\neggnog-mapper download_eggnog_data.py -y --data_dir 10-functional-annotation/eggnog\n\n# Create a diamond database for Fungi.\n# This will download all eggNOG proteins first if not present, then create the fungi-specific database.\neggnog-mapper create_dbs.py -m diamond --dbname fungi --taxa Fungi --data_dir 10-functional-annotation/eggnog\n\n\n\nNow we can run eggNOG-mapper on our proteome file using the custom fungi database.\nmkdir -p 10-functional-annotation/eggnog/output\n\n# We need to bind the data directory and the input/output directories.\nDATADIR=$PWD/10-functional-annotation/eggnog\nINPUTFILE=$proteome\nOUTPUTDIR=$PWD/10-functional-annotation/eggnog/output\nOUTPUT_PREFIX=${SN}_eggnog\n\n# We build a full command with bind mounts to ensure correct paths inside the container.\napptainer run \\\n    --bind $DATADIR:/data \\\n    --bind $(dirname $INPUTFILE):/input \\\n    --bind $OUTPUTDIR:/output \\\n    docker://quay.io/biocontainers/eggnog-mapper:2.1.13--pyhdfd78af_0 \\\n    emapper.py \\\n    -i /input/$(basename $INPUTFILE) \\\n    -o /output/$OUTPUT_PREFIX \\\n    --data_dir /data \\\n    --dmnd_db /data/fungi.dmnd \\\n    --cpu $NCPUS \\\n    --override"
  },
  {
    "objectID": "hands-on/08-prepare-data-for-annotation.html",
    "href": "hands-on/08-prepare-data-for-annotation.html",
    "title": "Prepare Data for Annotation",
    "section": "",
    "text": "In this section, we will prepare the necessary data for the genome annotation of our assembly. This involves setting up the required tools, creating a custom protein database, and processing RNA-Seq data to be used as evidence for gene prediction."
  },
  {
    "objectID": "hands-on/08-prepare-data-for-annotation.html#setup",
    "href": "hands-on/08-prepare-data-for-annotation.html#setup",
    "title": "Prepare Data for Annotation",
    "section": "1 Setup",
    "text": "1 Setup\nWe will use apptainer to manage our software dependencies. Here are the aliases we will use:\nalias prefetch=\"apptainer run docker://ncbi/sra-tools prefetch\"\nalias fasterq-dump=\"apptainer run docker://ncbi/sra-tools fasterq-dump\"\nalias fastp=\"apptainer exec docker://staphb/fastp fastp\"\nalias STAR=\"apptainer exec docker://quay.io/biocontainers/star:2.7.11b--h5ca1c30_6 STAR\"\nalias samtools=\"apptainer exec docker://staphb/samtools samtools\""
  },
  {
    "objectID": "hands-on/08-prepare-data-for-annotation.html#variable",
    "href": "hands-on/08-prepare-data-for-annotation.html#variable",
    "title": "Prepare Data for Annotation",
    "section": "2 Variable",
    "text": "2 Variable\nexport SRR_RNA=\"SRR10123445\"\nexport SN=Fo47\nexport NCPUS=12\nexport assembly=\"$PWD/07-repeats-analysis/rm_out/Fo47.fasta.masked\""
  },
  {
    "objectID": "hands-on/08-prepare-data-for-annotation.html#create-fungi-protein-database-from-orthodb",
    "href": "hands-on/08-prepare-data-for-annotation.html#create-fungi-protein-database-from-orthodb",
    "title": "Prepare Data for Annotation",
    "section": "3 Create Fungi Protein Database from OrthoDB",
    "text": "3 Create Fungi Protein Database from OrthoDB\nWe will create a custom protein database containing only Fungi sequences from OrthoDB.\nFirst, let’s create a directory and download the necessary files.\nmkdir -p 08-data/orthodb\n\nwget https://data.orthodb.org/current/download/odb12v1_aa_fasta.gz -O 08-data/orthodb/odb12v1_aa_fasta.gz\nwget https://data.orthodb.org/current/download/odb12v1_levels.tab.gz -O 08-data/orthodb/odb12v1_levels.tab.gz\nwget https://data.orthodb.org/current/download/odb12v1_level2species.tab.gz -O 08-data/orthodb/odb12v1_level2species.tab.gz\ngunzip 08-data/orthodb/*.gz\n\nwget https://raw.githubusercontent.com/tomasbruna/orthodb-clades/master/selectClade.py -O 08-data/orthodb/selectClade.py\nchmod +x 08-data/orthodb/selectClade.py\nNow, we can run the script to select the Fungi sequences.\n./08-data/orthodb/selectClade.py \\\n  08-data/orthodb/odb12v1_aa_fasta \\\n  08-data/orthodb/odb12v1_levels.tab \\\n  08-data/orthodb/odb12v1_level2species.tab \\\n  Fungi &gt; 08-data/orthodb/Fungi.fa"
  },
  {
    "objectID": "hands-on/08-prepare-data-for-annotation.html#fetch-and-clean-rna-seq-reads",
    "href": "hands-on/08-prepare-data-for-annotation.html#fetch-and-clean-rna-seq-reads",
    "title": "Prepare Data for Annotation",
    "section": "4 Fetch and Clean RNA-Seq Reads",
    "text": "4 Fetch and Clean RNA-Seq Reads\nWe will download RNA-Seq data from the SRA and clean it to remove low-quality reads and adapters.\nmkdir -p 08-data/reads\n\nprefetch $SRR_RNA -O 08-data/reads\nfasterq-dump $SRR_RNA -O 08-data/reads\n\nfastp -i 08-data/reads/${SRR_RNA}_1.fastq -I 08-data/reads/${SRR_RNA}_2.fastq \\\n      -o 08-data/reads/${SRR_RNA}_1.clean.fastq.gz -O 08-data/reads/${SRR_RNA}_2.clean.fastq.gz \\\n      --html 08-data/reads/${SRR_RNA}.fastp.html"
  },
  {
    "objectID": "hands-on/08-prepare-data-for-annotation.html#align-rna-seq-reads-with-star",
    "href": "hands-on/08-prepare-data-for-annotation.html#align-rna-seq-reads-with-star",
    "title": "Prepare Data for Annotation",
    "section": "5 Align RNA-Seq Reads with STAR",
    "text": "5 Align RNA-Seq Reads with STAR\nFinally, we will align the cleaned RNA-Seq reads to our genome assembly from 07-repeats-analysis.\nmkdir -p 08-data/star/INDEX\n\nSTAR --runThreadN $NCPUS \\\n  --runMode genomeGenerate \\\n  --genomeDir 08-data/star/INDEX \\\n  --genomeFastaFiles $assembly\n\nSTAR --runThreadN $NCPUS \\\n     --genomeDir 08-data/star/INDEX \\\n     --readFilesIn 08-data/reads/${SRR_RNA}_1.clean.fastq.gz 08-data/reads/${SRR_RNA}_2.clean.fastq.gz \\\n     --readFilesCommand zcat \\\n     --outFileNamePrefix 08-data/star/$SN \\\n     --outSAMtype BAM SortedByCoordinate\n\nsamtools flagstats 08-data/star/${SN}.sorted.bam"
  },
  {
    "objectID": "hands-on/06-scaffolding.html",
    "href": "hands-on/06-scaffolding.html",
    "title": "Scaffolding with Ragout",
    "section": "",
    "text": "In this chapter, we will use Ragout to scaffold our best de novo assembly. Scaffolding is the process of ordering and orienting contigs from a draft assembly into larger structures, called scaffolds, using reference genomes. This step is crucial for improving the contiguity of the assembly and approaching a chromosome-level assembly.\nWe will use multiple reference genomes to guide the scaffolding process. Ragout will infer the correct order and orientation of the contigs based on synteny information from these references.\n\n\n\n\nWe’ll continue using the established environment variables:\nexport SN=tuto482\nexport NCPUS=12\n\n\n\nWe will use Ragout for this chapter.\n\n\n\nalias ragout=\"apptainer run docker://ghcr.io/nexomis/ragout:build_1.0-ragout_2.3-hal_2.3-Sibelia_3.0.7 ragout\"\nalias mash=\"apptainer run docker://staphb/mash:2.3-CBIRDv2 mash\"\n\n\n\nalias ragout=\"docker run --rm -u $UID:$GID -v $(pwd):$(pwd) -w $(pwd) ghcr.io/nexomis/ragout:build_1.0-ragout_2.3-hal_2.3-Sibelia_3.0.7 ragout\"\n\n\n\nconda create -n ragout -c bioconda ragout\nconda activate ragout\n\n\n\n\nLet’s create an organized directory structure for our analysis:\nmkdir -p 06-scaffolding/references\nmkdir -p 06-scaffolding/ragout_output\n\n\n\n\n\n\nWe will download four reference genomes to be used by Ragout.\nwget -P 06-scaffolding/references https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/013/085/055/GCF_013085055.1_ASM1308505v1/GCF_013085055.1_ASM1308505v1_genomic.fna.gz\nwget -P 06-scaffolding/references https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/048/165/035/GCA_048165035.1_ASM4816503v1/GCA_048165035.1_ASM4816503v1_genomic.fna.gz\nwget -P 06-scaffolding/references https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/048/164/945/GCA_048164945.1_ASM4816494v1/GCA_048164945.1_ASM4816494v1_genomic.fna.gz\nwget -P 06-scaffolding/references https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/030/719/095/GCA_030719095.1_ASM3071909v1/GCA_030719095.1_ASM3071909v1_genomic.fna.gz\nRagout requires the input fasta files to be uncompressed and renamed with simple names.\n# Decompress and rename reference files\ngunzip 06-scaffolding/references/*.gz\nmv 06-scaffolding/references/GCF_013085055.1_ASM1308505v1_genomic.fna 06-scaffolding/references/Fo47.fasta\nmv 06-scaffolding/references/GCA_048165035.1_ASM4816503v1_genomic.fna 06-scaffolding/references/ZUM2407.fasta\nmv 06-scaffolding/references/GCA_048164945.1_ASM4816494v1_genomic.fna 06-scaffolding/references/V032g.fasta\nmv 06-scaffolding/references/GCA_030719095.1_ASM3071909v1_genomic.fna 06-scaffolding/references/ME23.fasta\n\n# Rename fasta headers for Fo47 to have cleaner names (e.g., chr_I, chr_II)\nsed -i -E 's/&gt;.*chromosome ([IVX]+),.*/&gt;chr_\\1/' 06-scaffolding/references/Fo47.fasta\n\n# cp 04-assembly/masurca/option-A/flye*/assembly.fasta 06-scaffolding/references/${SN}.fasta\n# dowload from repo if necessary\nwget -p hands-on/06-scaffolding/references https://raw.githubusercontent.com/nexomis/tuto-euk-de-novo-genome/refs/heads/main/hands-on/06-scaffolding/references/tuto482.fasta.gz \ngunzip 06-scaffolding/references/*.gz\n\n\n\n\nIn the following steps, we might need a tree to guide multiple-genome alignment, so let’s build it.\nFor larger genomes (&gt;100Mb), the recommended approach is to use Cactus for whole-genome alignment, which requires a phylogenetic tree as input. This section covers how to build this tree.\nTree requirements for Cactus:\n\nMust be in Newick format\nMust be strictly binary (no multifurcations)\nAll leaf nodes must correspond to genome names\nBranch lengths are ignored\n\n\n\n\n\nSince the JolyTree container is missing dependencies, we’ll create a local conda environment:\n# Create local conda environment in hands-on/venv\nconda create -p venv -c bioconda -c conda-forge jolytree bc\n\n\n\nBefore creating the Cactus sequence file, we’ll use JolyTree to generate an accurate phylogenetic tree from our reference genomes and target assembly. JolyTree is a fast alignment-free tool that can infer distance-based phylogenetic trees from genome assemblies.\n# Run JolyTree to generate phylogenetic tree\nconda activate ./venv\nJolyTree.sh -i 06-scaffolding/references -b 06-scaffolding/jolytree -t $NCPUS\nconda deactivate\nJolyTree parameters explained:\n\n-i: Directory containing FASTA files (all genomes)\n-b: Basename for output files\n-t: Number of threads to use\n\nThe generated tree file 06-scaffolding/jolytree.nwk will contain the inferred phylogenetic relationships between all genomes.\nNote on Alternative Tree Construction Methods:\nJolyTree uses a sound methodology (Mash for distance estimation followed by tree construction algorithms), but its implementation lacks robustness and reproducibility. For more rigorous phylogenetic analysis, you can manually construct pairwise distance matrices based on k-mer frequencies using tools like:\n\nKAT: Can generate k-mer frequency histograms and compare k-mer profiles between genomes\nMash: Estimates genome-to-genome distances using MinHash sketches of k-mer sets\n\nThese distance matrices can then be used with phylogenetic reconstruction methods such as:\n\nUPGMA (Unweighted Pair Group Method with Arithmetic Mean)\nNeighbor-joining\n\nThis manual approach provides better control over parameters and more reproducible results, especially for closely related genomes or when branch length accuracy is important for downstream analyses.\n\n\n\nImportant: The JolyTree output may need manual editing to be compatible with Cactus requirements:\n\nRemove branch lengths: Cactus can work without branch lengths.\nConvert multifurcations to binary tree: Cactus requires strictly binary trees (each internal node has exactly 2 children). If JolyTree produces any 3-way or higher splits, these need to be resolved into binary splits.\n\nTo edit the tree 06-scaffolding/jolytree.nwk manually:\n(tuto482:0.1,Fo47:0.2,(ZUM2407:0.05,V032g:0.03,ME23:0.08):0.15);\ninto\n((tuto482,Fo47),((ZUM2407,V032g),ME23));\n\n\n\n\nHere, we provide a code snippet to build a phylogenetic tree using mash for distance calculation and a Python script for UPGMA tree construction. This approach offers more transparency and control over the tree-building process.\nFirst, run mash to compute the distance matrix and save it to a file:\nmash sketch -k 32 -s 100000 -o 06-scaffolding/references/sketch.msh 06-scaffolding/references/*fasta\nmash dist -s 100000 06-scaffolding/references/sketch.msh 06-scaffolding/references/*fasta &gt; 06-scaffolding/mash_dist.tsv\nNext, use the following Python script to build the UPGMA tree and save it in Newick format. This script requires numpy, pandas, and scipy. You may need to install them: pip install numpy pandas scipy.\n#!/usr/bin/env python\nimport pandas as pd\nimport numpy as np\nfrom scipy.cluster.hierarchy import linkage, to_tree\nfrom scipy.spatial.distance import squareform\nimport os\n\ndef get_newick(node, newick, parent_dist, leaf_names):\n    \"\"\"\n    Convert a SciPy linkage matrix to a Newick format string.\n    \"\"\"\n    if node.is_leaf():\n        return \"%s:%.4f%s\" % (leaf_names[node.id], parent_dist - node.dist, newick)\n    else:\n        if len(newick) &gt; 0:\n            newick = \")%s\" % newick\n        else:\n            newick = \");\"\n        dist = parent_dist - node.dist\n        if dist &lt; 0:\n            dist = 0\n        newick = get_newick(node.get_left(), \",%s\" % get_newick(node.get_right(), \"\", node.dist, leaf_names), node.dist, leaf_names) + newick\n        newick = \"(%s:%.4f\" % (newick, dist)\n        return newick\n\n# Read the distance matrix\ndist_df = pd.read_csv('06-scaffolding/mash_dist.tsv', sep='\\t', header=None, names=['ref1', 'ref2', 'dist', 'pvalue', 'kmers'])\n\n# Pivot to create a square distance matrix\ndist_pivot = dist_df.pivot(index='ref1', columns='ref2', values='dist').fillna(0)\nlabels = [os.path.basename(name).replace('.fasta', '') for name in dist_pivot.index]\ndist_pivot.index = labels\ndist_pivot.columns = labels\n\n# Ensure the matrix is symmetric and convert to condensed format\ndist_matrix = dist_pivot.values\ndist_matrix = (dist_matrix + dist_matrix.T) / 2\nnp.fill_diagonal(dist_matrix, 0)\ncondensed_dist = squareform(dist_matrix)\n\n# Perform UPGMA clustering\nZ = linkage(condensed_dist, 'average')\n\n# Convert to Newick format\ntree = to_tree(Z)\nnewick_tree = get_newick(tree, \"\", tree.dist, labels)\n\n# Save the tree to a file\nwith open('06-scaffolding/mash.nwk', 'w') as f:\n    f.write(newick_tree)\n\nprint(\"Newick tree saved to 06-scaffolding/mash.nwk\")\nprint(newick_tree)\n\n\n\n\nNow that we have our data prepared and, if needed, a phylogenetic tree, we can proceed with scaffolding. We present two methods.\n\n\nThe default method in Ragout uses Sibelia for synteny block decomposition. While effective for small genomes (e.g., bacterial), this can be very slow and memory-intensive for larger, more complex genomes.\n\n\nRagout uses a recipe file to specify the input genomes and parameters. We will create one for our analysis. We will use the best assembly from the previous step (04-assembly/masurca/option-A/flye*/assembly.fasta) as the target assembly.\n\n# Create the recipe file\ncat &gt; 06-scaffolding/ragout_recipe.rcp &lt;&lt; EOF\n.references = Fo47,ZUM2407,V032g,ME23\n.target = ${SN}\n.naming_ref = Fo47\n\nFo47.fasta = ${PWD}/06-scaffolding/references/Fo47.fasta\nZUM2407.fasta = ${PWD}/06-scaffolding/references/ZUM2407.fasta\nV032g.fasta = ${PWD}/06-scaffolding/references/V032g.fasta \nME23.fasta = ${PWD}/06-scaffolding/references/ME23.fasta\n${SN}.fasta = ${PWD}/06-scaffolding/references/${SN}.fasta\nEOF\n\n\n\nUse this method only for small genomes or for quick, preliminary analysis.\nragout 06-scaffolding/ragout_recipe.rcp \\\n  -o 06-scaffolding/ragout_output \\\n  --refine \\\n  --solid-scaffolds \\\n  -t $NCPUS\n\n\n\n\nFor larger genomes (&gt;100Mb) or when higher accuracy is desired, the recommended workflow is to use Cactus to create a HAL (Hierarchical Alignment) file, which is then converted to MAF format for Ragout. This approach is generally faster and more robust for complex genomes.\n\n\n\n\nalias cactus=\"apptainer run docker://quay.io/comparative-genomics-toolkit/cactus:v2.9.9 cactus\"\n\n\n\nalias cactus=\"docker run --rm -u $UID:$GID -v $(pwd):$(pwd) -w $(pwd) quay.io/comparative-genomics-toolkit/cactus:v2.9.9 cactus\"\n\n\n\n\nNow we’ll create a sequence file for Cactus using the phylogenetic tree we generated earlier:\n# Create directory for Cactus workflow\nmkdir -p 06-scaffolding/cactus\n\ncat &gt; 06-scaffolding/cactus/genomes.txt &lt;&lt; EOF\n# Phylogenetic tree (use the one from JolyTree or Mash)\n((ME23,(ZUM2407,(V032g,Fo47))),tuto482);\n\n# Genome paths (must be absolute paths)\n${SN} ${PWD}/06-scaffolding/references/${SN}.fasta\nFo47 ${PWD}/06-scaffolding/references/Fo47.fasta\nZUM2407 ${PWD}/06-scaffolding/references/ZUM2407.fasta\nV032g ${PWD}/06-scaffolding/references/V032g.fasta\nME23 ${PWD}/06-scaffolding/references/ME23.fasta\nEOF\n\n\n\n# Run Cactus to create HAL alignment\ncactus 06-scaffolding/cactus/js \\\n  06-scaffolding/cactus/genomes.txt \\\n  06-scaffolding/cactus/alignment.hal \\\n  --maxCores $NCPUS\nCactus parameters explained:\n\njs: Job store directory for intermediate files\ngenomes.txt: Sequence file with tree and genome paths\nalignment.hal: Output HAL alignment file\n--maxCores: Maximum number of CPU cores to use\n\n\n\n\nAfter generating the HAL alignment, we need to convert it to MAF format for use with Ragout:\n# Set up cactus-hal2maf alias\nalias cactus-hal2maf=\"apptainer run docker://quay.io/comparative-genomics-toolkit/cactus:v2.9.9 cactus-hal2maf\"\n\n# Convert HAL to MAF format\ncactus-hal2maf 06-scaffolding/cactus/js-hal2maf \\\n  06-scaffolding/cactus/alignment.hal \\\n  06-scaffolding/cactus/alignment.maf \\\n  --refGenome Fo47 \\\n  --filterGapCausingDupes \\\n  --noAncestors \\\n  --dupeMode single \\\n  --chunkSize 500000\ncactus-hal2maf parameters explained:\n\n--refGenome: Reference genome for MAF projection (Fo47 for consistent chromosome naming)\n--chunkSize: Size of chunks for parallel processing\n\n\n\n\nNow create a recipe file that uses the MAF alignment:\n# Create MAF-based recipe file\ncat &gt; 06-scaffolding/ragout_maf_recipe.rcp &lt;&lt; EOF\n.references = Fo47,ZUM2407,V032g,ME23\n.target = ${SN}\n.naming_ref = Fo47\n.maf = ${PWD}/06-scaffolding/cactus/alignment.maf\n\n# Individual FASTA paths are required for MAF backend\nFo47.fasta = ${PWD}/06-scaffolding/references/Fo47.fasta\nZUM2407.fasta = ${PWD}/06-scaffolding/references/ZUM2407.fasta\nV032g.fasta = ${PWD}/06-scaffolding/references/V032g.fasta\nME23.fasta = ${PWD}/06-scaffolding/references/ME23.fasta\n${SN}.fasta = ${PWD}/06-scaffolding/references/${SN}.fasta\nEOF\n\n\n\nragout 06-scaffolding/ragout_maf_recipe.rcp \\\n  -o 06-scaffolding/ragout_maf_output \\\n  --solid-scaffolds \\\n  -s maf \\\n  -t $NCPUS\nMAF method parameters:\n\n-s maf: Specifies MAF as the synteny backend\nIndividual FASTA paths must be provided alongside the MAF file\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMethod\nProcessing Time\nGenome Size Suitability\nMemory Usage\n\n\n\n\nSibelia\nSlower\n&lt;100Mb (bacterial, small eukaryotes)\nModerate\n\n\nCactus/MAF\nFaster\n&gt;100Mb (large eukaryotes)\nHigher initial, but more efficient overall\n\n\n\nRecommendations:\n\nUse Sibelia method (not recommended) for bacterial genomes or small eukaryotic genomes.\nUse Cactus/MAF method (recommended) for large eukaryotic genomes or when faster processing is needed.\n\n\n\n\n\n(This part will be seen after annotation tutorial)\n\n\n\nAfter Ragout finishes, the output directory will contain the scaffolded assembly. The main output files are:\n\n\n\n${SN}_scaffolds.fasta: The final scaffolded assembly\n${SN}_unplaced.fasta: Input contigs that could not be placed in the scaffolds\n${SN}_scaffolds.agp: The structure of the scaffolds in AGP format\n\n\n\n\n\n${SN}_scaffolds.fasta: The final scaffolded assembly\n${SN}_unplaced.fasta: Input contigs that could not be placed in the scaffolds\n${SN}_scaffolds.agp: The structure of the scaffolds in AGP format\n\n\n\n\n\nalignment.hal: Multiple genome alignment in HAL format\nalignment.maf: Multiple genome alignment in HAL format\n\n\n\n\n\nYou can then proceed to evaluate the quality of the new scaffolded assembly using QUAST and BUSCO, as described in the previous chapter. Compare the results from both methods to determine which approach works best for your specific dataset."
  },
  {
    "objectID": "hands-on/06-scaffolding.html#configuration-and-setup",
    "href": "hands-on/06-scaffolding.html#configuration-and-setup",
    "title": "Scaffolding with Ragout",
    "section": "",
    "text": "We’ll continue using the established environment variables:\nexport SN=tuto482\nexport NCPUS=12\n\n\n\nWe will use Ragout for this chapter.\n\n\n\nalias ragout=\"apptainer run docker://ghcr.io/nexomis/ragout:build_1.0-ragout_2.3-hal_2.3-Sibelia_3.0.7 ragout\"\nalias mash=\"apptainer run docker://staphb/mash:2.3-CBIRDv2 mash\"\n\n\n\nalias ragout=\"docker run --rm -u $UID:$GID -v $(pwd):$(pwd) -w $(pwd) ghcr.io/nexomis/ragout:build_1.0-ragout_2.3-hal_2.3-Sibelia_3.0.7 ragout\"\n\n\n\nconda create -n ragout -c bioconda ragout\nconda activate ragout\n\n\n\n\nLet’s create an organized directory structure for our analysis:\nmkdir -p 06-scaffolding/references\nmkdir -p 06-scaffolding/ragout_output"
  },
  {
    "objectID": "hands-on/06-scaffolding.html#data-preparation",
    "href": "hands-on/06-scaffolding.html#data-preparation",
    "title": "Scaffolding with Ragout",
    "section": "",
    "text": "We will download four reference genomes to be used by Ragout.\nwget -P 06-scaffolding/references https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/013/085/055/GCF_013085055.1_ASM1308505v1/GCF_013085055.1_ASM1308505v1_genomic.fna.gz\nwget -P 06-scaffolding/references https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/048/165/035/GCA_048165035.1_ASM4816503v1/GCA_048165035.1_ASM4816503v1_genomic.fna.gz\nwget -P 06-scaffolding/references https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/048/164/945/GCA_048164945.1_ASM4816494v1/GCA_048164945.1_ASM4816494v1_genomic.fna.gz\nwget -P 06-scaffolding/references https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/030/719/095/GCA_030719095.1_ASM3071909v1/GCA_030719095.1_ASM3071909v1_genomic.fna.gz\nRagout requires the input fasta files to be uncompressed and renamed with simple names.\n# Decompress and rename reference files\ngunzip 06-scaffolding/references/*.gz\nmv 06-scaffolding/references/GCF_013085055.1_ASM1308505v1_genomic.fna 06-scaffolding/references/Fo47.fasta\nmv 06-scaffolding/references/GCA_048165035.1_ASM4816503v1_genomic.fna 06-scaffolding/references/ZUM2407.fasta\nmv 06-scaffolding/references/GCA_048164945.1_ASM4816494v1_genomic.fna 06-scaffolding/references/V032g.fasta\nmv 06-scaffolding/references/GCA_030719095.1_ASM3071909v1_genomic.fna 06-scaffolding/references/ME23.fasta\n\n# Rename fasta headers for Fo47 to have cleaner names (e.g., chr_I, chr_II)\nsed -i -E 's/&gt;.*chromosome ([IVX]+),.*/&gt;chr_\\1/' 06-scaffolding/references/Fo47.fasta\n\n# cp 04-assembly/masurca/option-A/flye*/assembly.fasta 06-scaffolding/references/${SN}.fasta\n# dowload from repo if necessary\nwget -p hands-on/06-scaffolding/references https://raw.githubusercontent.com/nexomis/tuto-euk-de-novo-genome/refs/heads/main/hands-on/06-scaffolding/references/tuto482.fasta.gz \ngunzip 06-scaffolding/references/*.gz"
  },
  {
    "objectID": "hands-on/06-scaffolding.html#building-a-phylogenetic-tree-fast-from-genome-sequences",
    "href": "hands-on/06-scaffolding.html#building-a-phylogenetic-tree-fast-from-genome-sequences",
    "title": "Scaffolding with Ragout",
    "section": "",
    "text": "In the following steps, we might need a tree to guide multiple-genome alignment, so let’s build it.\nFor larger genomes (&gt;100Mb), the recommended approach is to use Cactus for whole-genome alignment, which requires a phylogenetic tree as input. This section covers how to build this tree.\nTree requirements for Cactus:\n\nMust be in Newick format\nMust be strictly binary (no multifurcations)\nAll leaf nodes must correspond to genome names\nBranch lengths are ignored\n\n\n\n\n\nSince the JolyTree container is missing dependencies, we’ll create a local conda environment:\n# Create local conda environment in hands-on/venv\nconda create -p venv -c bioconda -c conda-forge jolytree bc\n\n\n\nBefore creating the Cactus sequence file, we’ll use JolyTree to generate an accurate phylogenetic tree from our reference genomes and target assembly. JolyTree is a fast alignment-free tool that can infer distance-based phylogenetic trees from genome assemblies.\n# Run JolyTree to generate phylogenetic tree\nconda activate ./venv\nJolyTree.sh -i 06-scaffolding/references -b 06-scaffolding/jolytree -t $NCPUS\nconda deactivate\nJolyTree parameters explained:\n\n-i: Directory containing FASTA files (all genomes)\n-b: Basename for output files\n-t: Number of threads to use\n\nThe generated tree file 06-scaffolding/jolytree.nwk will contain the inferred phylogenetic relationships between all genomes.\nNote on Alternative Tree Construction Methods:\nJolyTree uses a sound methodology (Mash for distance estimation followed by tree construction algorithms), but its implementation lacks robustness and reproducibility. For more rigorous phylogenetic analysis, you can manually construct pairwise distance matrices based on k-mer frequencies using tools like:\n\nKAT: Can generate k-mer frequency histograms and compare k-mer profiles between genomes\nMash: Estimates genome-to-genome distances using MinHash sketches of k-mer sets\n\nThese distance matrices can then be used with phylogenetic reconstruction methods such as:\n\nUPGMA (Unweighted Pair Group Method with Arithmetic Mean)\nNeighbor-joining\n\nThis manual approach provides better control over parameters and more reproducible results, especially for closely related genomes or when branch length accuracy is important for downstream analyses.\n\n\n\nImportant: The JolyTree output may need manual editing to be compatible with Cactus requirements:\n\nRemove branch lengths: Cactus can work without branch lengths.\nConvert multifurcations to binary tree: Cactus requires strictly binary trees (each internal node has exactly 2 children). If JolyTree produces any 3-way or higher splits, these need to be resolved into binary splits.\n\nTo edit the tree 06-scaffolding/jolytree.nwk manually:\n(tuto482:0.1,Fo47:0.2,(ZUM2407:0.05,V032g:0.03,ME23:0.08):0.15);\ninto\n((tuto482,Fo47),((ZUM2407,V032g),ME23));\n\n\n\n\nHere, we provide a code snippet to build a phylogenetic tree using mash for distance calculation and a Python script for UPGMA tree construction. This approach offers more transparency and control over the tree-building process.\nFirst, run mash to compute the distance matrix and save it to a file:\nmash sketch -k 32 -s 100000 -o 06-scaffolding/references/sketch.msh 06-scaffolding/references/*fasta\nmash dist -s 100000 06-scaffolding/references/sketch.msh 06-scaffolding/references/*fasta &gt; 06-scaffolding/mash_dist.tsv\nNext, use the following Python script to build the UPGMA tree and save it in Newick format. This script requires numpy, pandas, and scipy. You may need to install them: pip install numpy pandas scipy.\n#!/usr/bin/env python\nimport pandas as pd\nimport numpy as np\nfrom scipy.cluster.hierarchy import linkage, to_tree\nfrom scipy.spatial.distance import squareform\nimport os\n\ndef get_newick(node, newick, parent_dist, leaf_names):\n    \"\"\"\n    Convert a SciPy linkage matrix to a Newick format string.\n    \"\"\"\n    if node.is_leaf():\n        return \"%s:%.4f%s\" % (leaf_names[node.id], parent_dist - node.dist, newick)\n    else:\n        if len(newick) &gt; 0:\n            newick = \")%s\" % newick\n        else:\n            newick = \");\"\n        dist = parent_dist - node.dist\n        if dist &lt; 0:\n            dist = 0\n        newick = get_newick(node.get_left(), \",%s\" % get_newick(node.get_right(), \"\", node.dist, leaf_names), node.dist, leaf_names) + newick\n        newick = \"(%s:%.4f\" % (newick, dist)\n        return newick\n\n# Read the distance matrix\ndist_df = pd.read_csv('06-scaffolding/mash_dist.tsv', sep='\\t', header=None, names=['ref1', 'ref2', 'dist', 'pvalue', 'kmers'])\n\n# Pivot to create a square distance matrix\ndist_pivot = dist_df.pivot(index='ref1', columns='ref2', values='dist').fillna(0)\nlabels = [os.path.basename(name).replace('.fasta', '') for name in dist_pivot.index]\ndist_pivot.index = labels\ndist_pivot.columns = labels\n\n# Ensure the matrix is symmetric and convert to condensed format\ndist_matrix = dist_pivot.values\ndist_matrix = (dist_matrix + dist_matrix.T) / 2\nnp.fill_diagonal(dist_matrix, 0)\ncondensed_dist = squareform(dist_matrix)\n\n# Perform UPGMA clustering\nZ = linkage(condensed_dist, 'average')\n\n# Convert to Newick format\ntree = to_tree(Z)\nnewick_tree = get_newick(tree, \"\", tree.dist, labels)\n\n# Save the tree to a file\nwith open('06-scaffolding/mash.nwk', 'w') as f:\n    f.write(newick_tree)\n\nprint(\"Newick tree saved to 06-scaffolding/mash.nwk\")\nprint(newick_tree)"
  },
  {
    "objectID": "hands-on/06-scaffolding.html#phylogenetic-scaffolding",
    "href": "hands-on/06-scaffolding.html#phylogenetic-scaffolding",
    "title": "Scaffolding with Ragout",
    "section": "",
    "text": "Now that we have our data prepared and, if needed, a phylogenetic tree, we can proceed with scaffolding. We present two methods.\n\n\nThe default method in Ragout uses Sibelia for synteny block decomposition. While effective for small genomes (e.g., bacterial), this can be very slow and memory-intensive for larger, more complex genomes.\n\n\nRagout uses a recipe file to specify the input genomes and parameters. We will create one for our analysis. We will use the best assembly from the previous step (04-assembly/masurca/option-A/flye*/assembly.fasta) as the target assembly.\n\n# Create the recipe file\ncat &gt; 06-scaffolding/ragout_recipe.rcp &lt;&lt; EOF\n.references = Fo47,ZUM2407,V032g,ME23\n.target = ${SN}\n.naming_ref = Fo47\n\nFo47.fasta = ${PWD}/06-scaffolding/references/Fo47.fasta\nZUM2407.fasta = ${PWD}/06-scaffolding/references/ZUM2407.fasta\nV032g.fasta = ${PWD}/06-scaffolding/references/V032g.fasta \nME23.fasta = ${PWD}/06-scaffolding/references/ME23.fasta\n${SN}.fasta = ${PWD}/06-scaffolding/references/${SN}.fasta\nEOF\n\n\n\nUse this method only for small genomes or for quick, preliminary analysis.\nragout 06-scaffolding/ragout_recipe.rcp \\\n  -o 06-scaffolding/ragout_output \\\n  --refine \\\n  --solid-scaffolds \\\n  -t $NCPUS\n\n\n\n\nFor larger genomes (&gt;100Mb) or when higher accuracy is desired, the recommended workflow is to use Cactus to create a HAL (Hierarchical Alignment) file, which is then converted to MAF format for Ragout. This approach is generally faster and more robust for complex genomes.\n\n\n\n\nalias cactus=\"apptainer run docker://quay.io/comparative-genomics-toolkit/cactus:v2.9.9 cactus\"\n\n\n\nalias cactus=\"docker run --rm -u $UID:$GID -v $(pwd):$(pwd) -w $(pwd) quay.io/comparative-genomics-toolkit/cactus:v2.9.9 cactus\"\n\n\n\n\nNow we’ll create a sequence file for Cactus using the phylogenetic tree we generated earlier:\n# Create directory for Cactus workflow\nmkdir -p 06-scaffolding/cactus\n\ncat &gt; 06-scaffolding/cactus/genomes.txt &lt;&lt; EOF\n# Phylogenetic tree (use the one from JolyTree or Mash)\n((ME23,(ZUM2407,(V032g,Fo47))),tuto482);\n\n# Genome paths (must be absolute paths)\n${SN} ${PWD}/06-scaffolding/references/${SN}.fasta\nFo47 ${PWD}/06-scaffolding/references/Fo47.fasta\nZUM2407 ${PWD}/06-scaffolding/references/ZUM2407.fasta\nV032g ${PWD}/06-scaffolding/references/V032g.fasta\nME23 ${PWD}/06-scaffolding/references/ME23.fasta\nEOF\n\n\n\n# Run Cactus to create HAL alignment\ncactus 06-scaffolding/cactus/js \\\n  06-scaffolding/cactus/genomes.txt \\\n  06-scaffolding/cactus/alignment.hal \\\n  --maxCores $NCPUS\nCactus parameters explained:\n\njs: Job store directory for intermediate files\ngenomes.txt: Sequence file with tree and genome paths\nalignment.hal: Output HAL alignment file\n--maxCores: Maximum number of CPU cores to use\n\n\n\n\nAfter generating the HAL alignment, we need to convert it to MAF format for use with Ragout:\n# Set up cactus-hal2maf alias\nalias cactus-hal2maf=\"apptainer run docker://quay.io/comparative-genomics-toolkit/cactus:v2.9.9 cactus-hal2maf\"\n\n# Convert HAL to MAF format\ncactus-hal2maf 06-scaffolding/cactus/js-hal2maf \\\n  06-scaffolding/cactus/alignment.hal \\\n  06-scaffolding/cactus/alignment.maf \\\n  --refGenome Fo47 \\\n  --filterGapCausingDupes \\\n  --noAncestors \\\n  --dupeMode single \\\n  --chunkSize 500000\ncactus-hal2maf parameters explained:\n\n--refGenome: Reference genome for MAF projection (Fo47 for consistent chromosome naming)\n--chunkSize: Size of chunks for parallel processing\n\n\n\n\nNow create a recipe file that uses the MAF alignment:\n# Create MAF-based recipe file\ncat &gt; 06-scaffolding/ragout_maf_recipe.rcp &lt;&lt; EOF\n.references = Fo47,ZUM2407,V032g,ME23\n.target = ${SN}\n.naming_ref = Fo47\n.maf = ${PWD}/06-scaffolding/cactus/alignment.maf\n\n# Individual FASTA paths are required for MAF backend\nFo47.fasta = ${PWD}/06-scaffolding/references/Fo47.fasta\nZUM2407.fasta = ${PWD}/06-scaffolding/references/ZUM2407.fasta\nV032g.fasta = ${PWD}/06-scaffolding/references/V032g.fasta\nME23.fasta = ${PWD}/06-scaffolding/references/ME23.fasta\n${SN}.fasta = ${PWD}/06-scaffolding/references/${SN}.fasta\nEOF\n\n\n\nragout 06-scaffolding/ragout_maf_recipe.rcp \\\n  -o 06-scaffolding/ragout_maf_output \\\n  --solid-scaffolds \\\n  -s maf \\\n  -t $NCPUS\nMAF method parameters:\n\n-s maf: Specifies MAF as the synteny backend\nIndividual FASTA paths must be provided alongside the MAF file\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMethod\nProcessing Time\nGenome Size Suitability\nMemory Usage\n\n\n\n\nSibelia\nSlower\n&lt;100Mb (bacterial, small eukaryotes)\nModerate\n\n\nCactus/MAF\nFaster\n&gt;100Mb (large eukaryotes)\nHigher initial, but more efficient overall\n\n\n\nRecommendations:\n\nUse Sibelia method (not recommended) for bacterial genomes or small eukaryotic genomes.\nUse Cactus/MAF method (recommended) for large eukaryotic genomes or when faster processing is needed."
  },
  {
    "objectID": "hands-on/06-scaffolding.html#rna-seq-based-scaffolding-with-agouti",
    "href": "hands-on/06-scaffolding.html#rna-seq-based-scaffolding-with-agouti",
    "title": "Scaffolding with Ragout",
    "section": "",
    "text": "(This part will be seen after annotation tutorial)"
  },
  {
    "objectID": "hands-on/06-scaffolding.html#expected-outcomes",
    "href": "hands-on/06-scaffolding.html#expected-outcomes",
    "title": "Scaffolding with Ragout",
    "section": "",
    "text": "After Ragout finishes, the output directory will contain the scaffolded assembly. The main output files are:\n\n\n\n${SN}_scaffolds.fasta: The final scaffolded assembly\n${SN}_unplaced.fasta: Input contigs that could not be placed in the scaffolds\n${SN}_scaffolds.agp: The structure of the scaffolds in AGP format\n\n\n\n\n\n${SN}_scaffolds.fasta: The final scaffolded assembly\n${SN}_unplaced.fasta: Input contigs that could not be placed in the scaffolds\n${SN}_scaffolds.agp: The structure of the scaffolds in AGP format\n\n\n\n\n\nalignment.hal: Multiple genome alignment in HAL format\nalignment.maf: Multiple genome alignment in HAL format"
  },
  {
    "objectID": "hands-on/06-scaffolding.html#quality",
    "href": "hands-on/06-scaffolding.html#quality",
    "title": "Scaffolding with Ragout",
    "section": "",
    "text": "You can then proceed to evaluate the quality of the new scaffolded assembly using QUAST and BUSCO, as described in the previous chapter. Compare the results from both methods to determine which approach works best for your specific dataset."
  },
  {
    "objectID": "hands-on/04-hybrid-assembly-spades-masurca.html",
    "href": "hands-on/04-hybrid-assembly-spades-masurca.html",
    "title": "Hybrid Assembly: SPAdes and MaSuRCA",
    "section": "",
    "text": "Hybrid genome assembly combines the accuracy of short reads (Illumina) with the contiguity advantages of long reads (Nanopore/PacBio) to produce high-quality genome assemblies. This approach leverages the complementary strengths of both sequencing technologies to overcome their individual limitations.\nIn this chapter, we’ll perform hybrid genome assembly using two leading assemblers: SPAdes and MaSuRCA. Each tool employs distinct algorithmic approaches, making them suitable for different assembly scenarios and data characteristics.\n\n\n\n\nSPAdes (St. Petersburg genome assembler) uses a multi-k-mer de Bruijn graph approach combined with sophisticated graph simplification algorithms. For hybrid assembly, SPAdes:\n\nError corrects both short and long reads independently\nConstructs assembly graphs using multiple k-mer sizes\nIntegrates long reads to resolve repeats and improve contiguity\nGenerates scaffolds using paired-end and long-read information\n\nKey strengths: Excellent for bacterial genomes, robust error correction, handles complex genomic regions well.\n\n\n\nMaSuRCA (Maryland Super Read Cabog Assembler) combines de Bruijn graph and Overlap-Layout-Consensus approaches using:\n\nSuper-reads: Error-corrected short reads extended using k-mer graphs\nMega-reads: Long reads corrected using Illumina data for hybrid assembly\nCABOG/Flye integration: Final assembly using corrected reads\n\nKey strengths: Excellent performance on larger eukaryotic genomes, superior handling of highly repetitive regions, mature hybrid assembly pipeline.\n\n\n\n\n\n\n\n\n\n\n\nFeature\nSPAdes\nMaSuRCA\n\n\n\n\nOptimal for\nBacterial, small eukaryotes\nLarge eukaryotes, complex genomes\n\n\nMemory usage\nModerate\nHigh (genome-size dependent)\n\n\nRuntime\nFast-moderate\nModerate-slow\n\n\nRepeat handling\nGood\nExcellent\n\n\nConfiguration\nSimple\nAdvanced (config file)\n\n\n\n\n\n\n\n\n\nWe’ll continue using the established environment variables from previous chapters:\nexport SN=tuto482\nexport NCPUS=12\n\n\n\nBoth assemblers are available through multiple installation methods. We provide three approaches consistent with the tutorial framework:\n\n\nalias spades=\"apptainer run docker://staphb/spades spades.py\"\nalias masurca_apptainer=\"apptainer run docker://staphb/masurca\"\n\n\n\nalias spades=\"docker run --rm -u $UID:$GID -v \\$(pwd):/data -w /data staphb/spades spades.py\"\nalias masurca_docker=\"docker run --rm -u $UID:$GID -v \\$(pwd):/data -w /data staphb/masurca masurca\"\n\n\n\n# Create environment with assembly tools\nconda create -n assembly-tools -c bioconda spades masurca\n\n# Activate the environment\nconda activate assembly-tools\n\n\n\n\n\nCreate an organized directory structure for assembly results:\n# Create main assembly directory\nmkdir -p 04-assembly/{spades,masurca}\n\n# Create subdirectories for different preprocessing options\nmkdir -p 04-assembly/spades/{option-A,option-B}\nmkdir -p 04-assembly/masurca/{option-A,option-B}\nDirectory structure:\n\nspades/: SPAdes assembly results for both preprocessing options\nmasurca/: MaSuRCA assembly results and configuration files (configs stored in each option directory)\n\n\n\n\nBased on the k-mer analysis from Chapter 3, we’ll use optimal k-mer sizes for assembly. For this tutorial, we’ll use a representative k-mer range that typically works well for fungal genomes:\n# Define optimal k-mer sizes based on analysis (MAXIMUM=127)\nexport KMERS=\"25,33,37,47,59,71,91,111,127\"\n\nNote: In practice, you should select k-mer sizes based on your Chapter 3 analysis results, choosing sizes where genome size estimates were stable and consistent.\n\n\n\n\nSPAdes hybrid assembly integrates Illumina paired-end reads, merged reads, and Nanopore long reads to produce high-quality assemblies. We’ll run SPAdes on both preprocessing options to compare results.\n\n\nWe’ll demonstrate the assembly using Option A data (moderate filtering). The same approach can be applied to Option B data by changing the opt variable.\n# Set preprocessing option\nexport opt=A\n\n# Run SPAdes hybrid assembly\nspades \\\n  -1 02-primary/merged/${SN}-${opt}-illumina_R1.fastq.gz \\\n  -2 02-primary/merged/${SN}-${opt}-illumina_R2.fastq.gz \\\n  -s 02-primary/merged/${SN}-${opt}-illumina.fastq.gz \\\n  --nanopore 02-primary/trimmed/${SN}-nanopore.fastq.gz \\\n  --isolate \\\n  -t $NCPUS \\\n  -k $KMERS \\\n  -o 04-assembly/spades/option-${opt}\nSPAdes parameters explained:\n\n-1/-2: Paired-end read files (R1/R2)\n-s: Single-end reads (merged paired-end reads)\n--nanopore: Oxford Nanopore long reads\n--isolate: Optimizes for single bacterial/small eukaryotic genomes\n-t: Number of CPU threads\n-k: K-mer sizes for assembly (comma-separated)\n-o: Output directory\n\n\n\n\nTo compare preprocessing strategies, you can also run SPAdes with the more stringent Option B data:\n# Assembly with Option B (stringent filtering)\nexport opt=B\nspades \\\n  -1 02-primary/merged/${SN}-${opt}-illumina_R1.fastq.gz \\\n  -2 02-primary/merged/${SN}-${opt}-illumina_R2.fastq.gz \\\n  -s 02-primary/merged/${SN}-${opt}-illumina.fastq.gz \\\n  --nanopore 02-primary/trimmed/${SN}-nanopore.fastq.gz \\\n  --isolate \\\n  -t $NCPUS \\\n  -k $KMERS \\\n  -o 04-assembly/spades/option-${opt}\n\nParameter optimization note: Different k-mer combinations, memory settings, and coverage thresholds should be tested for optimal results. SPAdes also supports --careful mode for enhanced small variant detection.\n\n\n\n\n\nMaSuRCA requires a configuration file that specifies data locations and assembly parameters. We’ll create configurations for Option A data and demonstrate the complete assembly workflow.\n\n\nCreate a MaSuRCA configuration file for Option A data:\n# Create MaSuRCA configuration for Option A\nexport opt=A\ncat &gt; 04-assembly/masurca/option-${opt}/masurca_config.txt &lt;&lt; EOF\n# MaSuRCA configuration for hybrid assembly\n\nDATA\n# Illumina paired-end reads\nPE = pe 300 50 $PWD/02-primary/merged/${SN}-${opt}-illumina_R1.fastq $PWD/02-primary/merged/${SN}-${opt}-illumina_R2.fastq\nPE = sr 300 50 $PWD/02-primary/merged/${SN}-${opt}-illumina.fastq\n# Nanopore long reads\nNANOPORE=$PWD/02-primary/trimmed/${SN}-nanopore.fastq\nEND\n\nPARAMETERS\n# Essential parameters for hybrid assembly\nGRAPH_KMER_SIZE = auto\nUSE_LINKING_MATES = 0\nUSE_GRID = 0\nLHE_COVERAGE = 25\nMEGA_READS_ONE_PASS = 0\nLIMIT_JUMP_COVERAGE = 300\nCA_PARAMETERS = cgwErrorRate=0.15\nCLOSE_GAPS = 1\nNUM_THREADS = $NCPUS\nJF_SIZE = 2000000000\nSOAP_ASSEMBLY = 0\nFLYE_ASSEMBLY = 1\nEND\nEOF\nKey MaSuRCA parameters explained:\n\nPE: Paired-end library specification (prefix, mean insert size, stdev, forward reads, reverse reads)\nNANOPORE: Long reads file path\nGRAPH_KMER_SIZE = auto: Automatically determine optimal k-mer size\nUSE_LINKING_MATES = 0: Recommended for hybrid assemblies with long reads\nLHE_COVERAGE = 25: Maximum long read coverage to use\nFLYE_ASSEMBLY = 1: Use Flye for final assembly (faster and often better quality)\nJF_SIZE: Jellyfish hash size (set to ~20x genome size)\nNUM_THREADS: Uses the $NCPUS variable for consistent thread usage\n\n\nImportant: File paths in MaSuRCA configuration files must be absolute paths. We use $PWD to convert relative paths to absolute paths, ensuring MaSuRCA can locate input files regardless of the working directory during assembly execution.\n\n\n\n\nCompressed fastq are not supported so we need to decompress them:\nexport opt=A\npigz -d -p $NCPUS 02-primary/trimmed/${SN}-nanopore.fastq.gz 02-primary/merged/${SN}-${opt}-illumina.fastq.gz 02-primary/merged/${SN}-${opt}-illumina_R1.fastq.gz 02-primary/merged/${SN}-${opt}-illumina_R2.fastq.gz\n\n\n\nUse the MaSuRCA configuration to generate the assembly script:\n# Generate MaSuRCA assembly script for Option A\nexport opt=A\nmasurca_apptainer bash -c \"cd 04-assembly/masurca/option-${opt} ; masurca masurca_config.txt\"\n\n\n\nRun the generated assembly script:\n# Execute MaSuRCA assembly for Option A\nexport opt=A\nmasurca_apptainer bash -c \"cd 04-assembly/masurca/option-${opt} ; ./assemble.sh\"\n\n\n\nFor comparison, create and run MaSuRCA assembly with Option B data.\n\nParameter optimization note: Different LHE_COVERAGE values (15-35), MEGA_READS_ONE_PASS settings, and error rates should be tested. For larger genomes, consider increasing JF_SIZE and adjusting CA_PARAMETERS.\n\n\n\n\nKey metrics to examine:\nContiguity metrics:\n\nNumber of contigs: Fewer contigs generally indicate better assembly\nN50: Length of the contig at which 50% of the assembly is contained in contigs of this size or larger\nLargest contig: Indicates ability to resolve long genomic regions\n\nCompleteness indicators:\n\nTotal assembly size: Should approximate expected genome size from k-mer analysis\nCoverage representation: All major genomic regions should be represented\n\n\nNote: Comprehensive assembly evaluation using QUAST, BUSCO, and other tools will be covered in Chapter 5.\n\n\n\n\n\nAfter completing the hybrid assembly workflow, you should have the following directory structure:\n04-assembly/\n├── spades/\n│   ├── option-A/\n│   │   ├── scaffolds.fasta          # Final scaffolds\n│   │   ├── contigs.fasta            # Final contigs\n│   │   ├── assembly_graph.fastg     # Assembly graph\n│   │   ├── spades.log               # Assembly log\n│   │   └── ... (other SPAdes files)\n│   └── option-B/\n│       └── ... (similar structure)\n└── masurca/\n    ├── option-A/\n    │   ├── masurca_config.txt       # MaSurCA config\n    │   ├── assemble.sh              # Generated assembly script\n    │   ├── CA/                      # CABOG assembly results\n    │   ├── flye.mr.*/               # Flye assembly results (if used)\n    │   ├── work1/                   # Intermediate files\n    │   └── ... (other MaSuRCA files)\n    └── option-B/\n        └── ... (similar structure)"
  },
  {
    "objectID": "hands-on/04-hybrid-assembly-spades-masurca.html#theoretical-background",
    "href": "hands-on/04-hybrid-assembly-spades-masurca.html#theoretical-background",
    "title": "Hybrid Assembly: SPAdes and MaSuRCA",
    "section": "",
    "text": "SPAdes (St. Petersburg genome assembler) uses a multi-k-mer de Bruijn graph approach combined with sophisticated graph simplification algorithms. For hybrid assembly, SPAdes:\n\nError corrects both short and long reads independently\nConstructs assembly graphs using multiple k-mer sizes\nIntegrates long reads to resolve repeats and improve contiguity\nGenerates scaffolds using paired-end and long-read information\n\nKey strengths: Excellent for bacterial genomes, robust error correction, handles complex genomic regions well.\n\n\n\nMaSuRCA (Maryland Super Read Cabog Assembler) combines de Bruijn graph and Overlap-Layout-Consensus approaches using:\n\nSuper-reads: Error-corrected short reads extended using k-mer graphs\nMega-reads: Long reads corrected using Illumina data for hybrid assembly\nCABOG/Flye integration: Final assembly using corrected reads\n\nKey strengths: Excellent performance on larger eukaryotic genomes, superior handling of highly repetitive regions, mature hybrid assembly pipeline.\n\n\n\n\n\n\n\n\n\n\n\nFeature\nSPAdes\nMaSuRCA\n\n\n\n\nOptimal for\nBacterial, small eukaryotes\nLarge eukaryotes, complex genomes\n\n\nMemory usage\nModerate\nHigh (genome-size dependent)\n\n\nRuntime\nFast-moderate\nModerate-slow\n\n\nRepeat handling\nGood\nExcellent\n\n\nConfiguration\nSimple\nAdvanced (config file)"
  },
  {
    "objectID": "hands-on/04-hybrid-assembly-spades-masurca.html#configuration-and-setup",
    "href": "hands-on/04-hybrid-assembly-spades-masurca.html#configuration-and-setup",
    "title": "Hybrid Assembly: SPAdes and MaSuRCA",
    "section": "",
    "text": "We’ll continue using the established environment variables from previous chapters:\nexport SN=tuto482\nexport NCPUS=12\n\n\n\nBoth assemblers are available through multiple installation methods. We provide three approaches consistent with the tutorial framework:\n\n\nalias spades=\"apptainer run docker://staphb/spades spades.py\"\nalias masurca_apptainer=\"apptainer run docker://staphb/masurca\"\n\n\n\nalias spades=\"docker run --rm -u $UID:$GID -v \\$(pwd):/data -w /data staphb/spades spades.py\"\nalias masurca_docker=\"docker run --rm -u $UID:$GID -v \\$(pwd):/data -w /data staphb/masurca masurca\"\n\n\n\n# Create environment with assembly tools\nconda create -n assembly-tools -c bioconda spades masurca\n\n# Activate the environment\nconda activate assembly-tools"
  },
  {
    "objectID": "hands-on/04-hybrid-assembly-spades-masurca.html#directory-structure-setup",
    "href": "hands-on/04-hybrid-assembly-spades-masurca.html#directory-structure-setup",
    "title": "Hybrid Assembly: SPAdes and MaSuRCA",
    "section": "",
    "text": "Create an organized directory structure for assembly results:\n# Create main assembly directory\nmkdir -p 04-assembly/{spades,masurca}\n\n# Create subdirectories for different preprocessing options\nmkdir -p 04-assembly/spades/{option-A,option-B}\nmkdir -p 04-assembly/masurca/{option-A,option-B}\nDirectory structure:\n\nspades/: SPAdes assembly results for both preprocessing options\nmasurca/: MaSuRCA assembly results and configuration files (configs stored in each option directory)"
  },
  {
    "objectID": "hands-on/04-hybrid-assembly-spades-masurca.html#k-mer-size-selection",
    "href": "hands-on/04-hybrid-assembly-spades-masurca.html#k-mer-size-selection",
    "title": "Hybrid Assembly: SPAdes and MaSuRCA",
    "section": "",
    "text": "Based on the k-mer analysis from Chapter 3, we’ll use optimal k-mer sizes for assembly. For this tutorial, we’ll use a representative k-mer range that typically works well for fungal genomes:\n# Define optimal k-mer sizes based on analysis (MAXIMUM=127)\nexport KMERS=\"25,33,37,47,59,71,91,111,127\"\n\nNote: In practice, you should select k-mer sizes based on your Chapter 3 analysis results, choosing sizes where genome size estimates were stable and consistent."
  },
  {
    "objectID": "hands-on/04-hybrid-assembly-spades-masurca.html#spades-hybrid-assembly-1",
    "href": "hands-on/04-hybrid-assembly-spades-masurca.html#spades-hybrid-assembly-1",
    "title": "Hybrid Assembly: SPAdes and MaSuRCA",
    "section": "",
    "text": "SPAdes hybrid assembly integrates Illumina paired-end reads, merged reads, and Nanopore long reads to produce high-quality assemblies. We’ll run SPAdes on both preprocessing options to compare results.\n\n\nWe’ll demonstrate the assembly using Option A data (moderate filtering). The same approach can be applied to Option B data by changing the opt variable.\n# Set preprocessing option\nexport opt=A\n\n# Run SPAdes hybrid assembly\nspades \\\n  -1 02-primary/merged/${SN}-${opt}-illumina_R1.fastq.gz \\\n  -2 02-primary/merged/${SN}-${opt}-illumina_R2.fastq.gz \\\n  -s 02-primary/merged/${SN}-${opt}-illumina.fastq.gz \\\n  --nanopore 02-primary/trimmed/${SN}-nanopore.fastq.gz \\\n  --isolate \\\n  -t $NCPUS \\\n  -k $KMERS \\\n  -o 04-assembly/spades/option-${opt}\nSPAdes parameters explained:\n\n-1/-2: Paired-end read files (R1/R2)\n-s: Single-end reads (merged paired-end reads)\n--nanopore: Oxford Nanopore long reads\n--isolate: Optimizes for single bacterial/small eukaryotic genomes\n-t: Number of CPU threads\n-k: K-mer sizes for assembly (comma-separated)\n-o: Output directory\n\n\n\n\nTo compare preprocessing strategies, you can also run SPAdes with the more stringent Option B data:\n# Assembly with Option B (stringent filtering)\nexport opt=B\nspades \\\n  -1 02-primary/merged/${SN}-${opt}-illumina_R1.fastq.gz \\\n  -2 02-primary/merged/${SN}-${opt}-illumina_R2.fastq.gz \\\n  -s 02-primary/merged/${SN}-${opt}-illumina.fastq.gz \\\n  --nanopore 02-primary/trimmed/${SN}-nanopore.fastq.gz \\\n  --isolate \\\n  -t $NCPUS \\\n  -k $KMERS \\\n  -o 04-assembly/spades/option-${opt}\n\nParameter optimization note: Different k-mer combinations, memory settings, and coverage thresholds should be tested for optimal results. SPAdes also supports --careful mode for enhanced small variant detection."
  },
  {
    "objectID": "hands-on/04-hybrid-assembly-spades-masurca.html#masurca-hybrid-assembly-1",
    "href": "hands-on/04-hybrid-assembly-spades-masurca.html#masurca-hybrid-assembly-1",
    "title": "Hybrid Assembly: SPAdes and MaSuRCA",
    "section": "",
    "text": "MaSuRCA requires a configuration file that specifies data locations and assembly parameters. We’ll create configurations for Option A data and demonstrate the complete assembly workflow.\n\n\nCreate a MaSuRCA configuration file for Option A data:\n# Create MaSuRCA configuration for Option A\nexport opt=A\ncat &gt; 04-assembly/masurca/option-${opt}/masurca_config.txt &lt;&lt; EOF\n# MaSuRCA configuration for hybrid assembly\n\nDATA\n# Illumina paired-end reads\nPE = pe 300 50 $PWD/02-primary/merged/${SN}-${opt}-illumina_R1.fastq $PWD/02-primary/merged/${SN}-${opt}-illumina_R2.fastq\nPE = sr 300 50 $PWD/02-primary/merged/${SN}-${opt}-illumina.fastq\n# Nanopore long reads\nNANOPORE=$PWD/02-primary/trimmed/${SN}-nanopore.fastq\nEND\n\nPARAMETERS\n# Essential parameters for hybrid assembly\nGRAPH_KMER_SIZE = auto\nUSE_LINKING_MATES = 0\nUSE_GRID = 0\nLHE_COVERAGE = 25\nMEGA_READS_ONE_PASS = 0\nLIMIT_JUMP_COVERAGE = 300\nCA_PARAMETERS = cgwErrorRate=0.15\nCLOSE_GAPS = 1\nNUM_THREADS = $NCPUS\nJF_SIZE = 2000000000\nSOAP_ASSEMBLY = 0\nFLYE_ASSEMBLY = 1\nEND\nEOF\nKey MaSuRCA parameters explained:\n\nPE: Paired-end library specification (prefix, mean insert size, stdev, forward reads, reverse reads)\nNANOPORE: Long reads file path\nGRAPH_KMER_SIZE = auto: Automatically determine optimal k-mer size\nUSE_LINKING_MATES = 0: Recommended for hybrid assemblies with long reads\nLHE_COVERAGE = 25: Maximum long read coverage to use\nFLYE_ASSEMBLY = 1: Use Flye for final assembly (faster and often better quality)\nJF_SIZE: Jellyfish hash size (set to ~20x genome size)\nNUM_THREADS: Uses the $NCPUS variable for consistent thread usage\n\n\nImportant: File paths in MaSuRCA configuration files must be absolute paths. We use $PWD to convert relative paths to absolute paths, ensuring MaSuRCA can locate input files regardless of the working directory during assembly execution.\n\n\n\n\nCompressed fastq are not supported so we need to decompress them:\nexport opt=A\npigz -d -p $NCPUS 02-primary/trimmed/${SN}-nanopore.fastq.gz 02-primary/merged/${SN}-${opt}-illumina.fastq.gz 02-primary/merged/${SN}-${opt}-illumina_R1.fastq.gz 02-primary/merged/${SN}-${opt}-illumina_R2.fastq.gz\n\n\n\nUse the MaSuRCA configuration to generate the assembly script:\n# Generate MaSuRCA assembly script for Option A\nexport opt=A\nmasurca_apptainer bash -c \"cd 04-assembly/masurca/option-${opt} ; masurca masurca_config.txt\"\n\n\n\nRun the generated assembly script:\n# Execute MaSuRCA assembly for Option A\nexport opt=A\nmasurca_apptainer bash -c \"cd 04-assembly/masurca/option-${opt} ; ./assemble.sh\"\n\n\n\nFor comparison, create and run MaSuRCA assembly with Option B data.\n\nParameter optimization note: Different LHE_COVERAGE values (15-35), MEGA_READS_ONE_PASS settings, and error rates should be tested. For larger genomes, consider increasing JF_SIZE and adjusting CA_PARAMETERS.\n\n\n\n\nKey metrics to examine:\nContiguity metrics:\n\nNumber of contigs: Fewer contigs generally indicate better assembly\nN50: Length of the contig at which 50% of the assembly is contained in contigs of this size or larger\nLargest contig: Indicates ability to resolve long genomic regions\n\nCompleteness indicators:\n\nTotal assembly size: Should approximate expected genome size from k-mer analysis\nCoverage representation: All major genomic regions should be represented\n\n\nNote: Comprehensive assembly evaluation using QUAST, BUSCO, and other tools will be covered in Chapter 5."
  },
  {
    "objectID": "hands-on/04-hybrid-assembly-spades-masurca.html#expected-outcomes",
    "href": "hands-on/04-hybrid-assembly-spades-masurca.html#expected-outcomes",
    "title": "Hybrid Assembly: SPAdes and MaSuRCA",
    "section": "",
    "text": "After completing the hybrid assembly workflow, you should have the following directory structure:\n04-assembly/\n├── spades/\n│   ├── option-A/\n│   │   ├── scaffolds.fasta          # Final scaffolds\n│   │   ├── contigs.fasta            # Final contigs\n│   │   ├── assembly_graph.fastg     # Assembly graph\n│   │   ├── spades.log               # Assembly log\n│   │   └── ... (other SPAdes files)\n│   └── option-B/\n│       └── ... (similar structure)\n└── masurca/\n    ├── option-A/\n    │   ├── masurca_config.txt       # MaSurCA config\n    │   ├── assemble.sh              # Generated assembly script\n    │   ├── CA/                      # CABOG assembly results\n    │   ├── flye.mr.*/               # Flye assembly results (if used)\n    │   ├── work1/                   # Intermediate files\n    │   └── ... (other MaSuRCA files)\n    └── option-B/\n        └── ... (similar structure)"
  },
  {
    "objectID": "hands-on/02-primary-analysis.html",
    "href": "hands-on/02-primary-analysis.html",
    "title": "Primary Analysis: Quality Control and Preprocessing",
    "section": "",
    "text": "After downloading our genomic datasets, the next critical step is to assess data quality and perform necessary preprocessing. This chapter covers quality control (QC) assessment, read filtering, trimming, and merging operations for both Illumina short-read and Oxford Nanopore long-read data.\nQuality control is essential because: - Raw sequencing data often contains low-quality bases, especially at read ends - Adapter sequences may remain attached to reads - Some reads may be too short to be useful for assembly - Understanding data characteristics helps optimize downstream analysis parameters\nWe’ll use a comprehensive workflow that includes:\n\nInitial QC assessment to understand raw data characteristics\nPreprocessing (filtering, trimming, merging) to improve data quality\nPost-processing QC to verify improvements\nComprehensive reporting with MultiQC\n\n\n\n\n\nWe’ll continue using the environment variables from the previous chapter. If starting fresh, set them again:\nexport SN=tuto482\nexport NCPUS=12\n\n\n\nThis workflow uses several bioinformatics tools, each optimized for specific data types and tasks:\n\nFastQC: Quality assessment for Illumina short reads\nNanoPlot: Quality assessment specialized for long reads (Nanopore/PacBio)\nfastp: Ultra-fast preprocessing tool for both short and long reads\nMultiQC: Aggregates QC reports from multiple tools\n\n\n\nalias fastqc=\"apptainer run docker://staphb/fastqc fastqc\"\nalias NanoPlot=\"apptainer run docker://staphb/nanoplot NanoPlot\"\nalias fastp=\"apptainer run docker://staphb/fastp fastp\"\nalias multiqc=\"apptainer run docker://staphb/multiqc multiqc\"\nalias seqtk=\"apptainer run docker://staphb/seqtk seqtk\"\nalias kraken2=\"apptainer run docker://staphb/kraken2 kraken2\"\n\n\n\nalias fastqc=\"docker run --rm -u $UID:$GID -v \\$(pwd):/data -w /data staphb/fastqc fastqc\"\nalias nanoplot=\"docker run --rm -u $UID:$GID -v \\$(pwd):/data -w /data staphb/nanoplot NanoPlot\"\nalias fastp=\"docker run --rm -u $UID:$GID -v \\$(pwd):/data -w /data staphb/fastp fastp\"\nalias multiqc=\"docker run --rm -u $UID:$GID -v \\$(pwd):/data -w /data staphb/multiqc multiqc\"\nalias seqtk=\"docker run --rm -u $UID:$GID -v \\$(pwd):/data -w /data staphb/seqtk seqtk\"\nalias kraken2=\"docker run --rm -u $UID:$GID -v \\$(pwd):/data -w /data staphb/kraken2 kraken2\"\n\n\n\n# Create environment with all required tools\nconda create -n qc-tools -c bioconda fastqc nanoplot fastp kraken2 seqtk multiqc\n\n# Activate the environment\nconda activate qc-tools\n\n\n\n\n\nLet’s create an organized directory structure for our analysis:\n# Navigate to working directory and create structure\nmkdir -p 02-primary/{trimmed,merged,reports,kraken2}\nmkdir -p 02-primary/{fastQC,nanoplot}/{raw,processed}\nmkdir -p 02-primary/trimmed/sub\nDirectory structure:\n\nraw/: Raw sequencing data and initial QC reports\ntrimmed/: Filtered and trimmed data with QC reports\ntrimmed/sub/: Subsampled datasets for analysis\nmerged/: Merged Illumina reads (when applicable) with QC\nkraken2/: Kraken2 database and taxonomic classification results\nreports/: Comprehensive MultiQC reports\n\n\n\n\nBefore any preprocessing, we need to understand the characteristics of our raw data. This baseline assessment helps us:\n\nIdentify potential issues (adapter contamination, quality drops, etc.)\nSet appropriate filtering parameters\nEvaluate the success of preprocessing steps\n\n\n\nFastQC provides comprehensive quality metrics for short-read data, including per-base quality scores, GC content, adapter content, and sequence duplication levels.\n# Run FastQC on raw Illumina data\nfastqc --threads $NCPUS --memory 8000 \\\n  --extract --outdir 02-primary/fastQC/raw/ \\\n  01-rawdata/${SN}-illumina_R1.fastq.gz 01-rawdata/${SN}-illumina_R2.fastq.gz\nKey FastQC metrics to examine:\n\nPer base sequence quality: Should be &gt;20 (preferably &gt;30) across most of the read\nPer sequence quality scores: Distribution should be skewed toward high quality\nAdapter content: Should be minimal in good data\nSequence duplication levels: High duplication may indicate PCR bias\n\n\n\n\nNanoPlot is specifically designed for long-read technologies and provides metrics relevant to Nanopore data characteristics.\n# Run NanoPlot on raw Nanopore data\nmkdir -p 02-primary/nanoplot/\nNanoPlot --fastq 01-rawdata/${SN}-nanopore.fastq.gz \\\n  --outdir 02-primary/nanoplot/raw/ --info_in_report \\\n  --loglength --threads $NCPUS\nKey NanoPlot metrics to examine:\n\nRead length distribution: Nanopore typically produces reads from 1kb to &gt;100kb\nQuality score distribution: Lower than Illumina but should show reasonable distribution\nYield over time: Helps assess sequencing run performance\n\n\n\n\n\nBased on the initial QC results, we’ll now filter and trim our data to improve quality while retaining maximum useful information.\n\n\nfastp is an ultra-fast tool that combines multiple preprocessing steps: adapter trimming, quality filtering, and read merging. We provide two filtering options with different stringency levels:\n\nOption A: Moderate FilteringOption B: Stringent Filtering\n\n\nParameters: Average quality ≥28, minimum length 101bp\nThis option provides a balance between data retention and quality improvement, suitable for most genome assembly projects.\n# Run fastp for Illumina paired-end data (Option A)\nexport opt=A\nfastp \\\n  -i 01-rawdata/${SN}-illumina_R1.fastq.gz -I 01-rawdata/${SN}-illumina_R2.fastq.gz \\\n  -o 02-primary/trimmed/${SN}-${opt}-illumina_R1.fastq.gz \\\n  -O 02-primary/trimmed/${SN}-${opt}-illumina_R2.fastq.gz \\\n  --detect_adapter_for_pe --trim_poly_g --trim_poly_x \\\n  --average_qual 28 --length_required 101 \\\n  --html 02-primary/trimmed/${SN}-${opt}-illumina_fastp.html \\\n  --cut_right_window_size 4 --cut_right_mean_quality 20 --cut_right \\\n  --cut_tail_window_size 4 --cut_tail_mean_quality 25 --cut_tail \\\n  --thread $NCPUS\n\n\nParameters: Average quality ≥30, minimum length 125bp\nThis option provides higher quality thresholds for projects requiring maximum data quality, though with potentially higher data loss.\n# Run fastp for Illumina paired-end data (Option B)\nexport opt=B\nfastp \\\n  -i 01-rawdata/${SN}-illumina_R1.fastq.gz -I 01-rawdata/${SN}-illumina_R2.fastq.gz \\\n  -o 02-primary/trimmed/${SN}-${opt}-illumina_R1.fastq.gz \\\n  -O 02-primary/trimmed/${SN}-${opt}-illumina_R2.fastq.gz \\\n  --detect_adapter_for_pe --trim_poly_g --trim_poly_x \\\n  --average_qual 30 --length_required 125 \\\n  --html 02-primary/trimmed/${SN}-${opt}-illumina_fastp.html \\\n  --cut_right_window_size 4 --cut_right_mean_quality 25 --cut_right \\\n  --cut_tail_window_size 4 --cut_tail_mean_quality 28 --cut_tail \\\n  --thread $NCPUS\n\n\n\nfastp parameters explained:\n\n--detect_adapter_for_pe: Automatically detect and remove adapters\n--trim_poly_g/x: Remove poly-G/X tails (common in NextSeq/NovaSeq)\n--average_qual: Minimum average quality score threshold\n--length_required: Minimum read length after trimming\n--cut_right/tail: Quality-based trimming from 3’ end\n\nChoosing between options: - Option A is recommended for most projects, providing good quality while preserving data - Option B is suitable when maximum quality is required and sufficient coverage depth is available\n\n\n\nFor Nanopore data, we focus on length and quality filtering rather than adapter trimming:\n# Run fastp for Nanopore data (quality and length filtering)\nfastp \\\n  -i 01-rawdata/${SN}-nanopore.fastq.gz -o 02-primary/trimmed/${SN}-nanopore.fastq.gz \\\n  --disable_adapter_trimming \\\n  --average_qual 16 --length_required 1000 \\\n  --html 02-primary/trimmed/${SN}-nanopore_fastp.html \\\n  --thread $NCPUS\nNanopore-specific parameters:\n\n--disable_adapter_trimming: Adapters typically removed during basecalling\n--average_qual 16: More lenient quality threshold for long reads\n--length_required 1000: Remove very short reads that are less useful for assembly\n\n\nNote that basecalling and important preliminary steps to clean nanopore reads are not seen here.\n\n\n\n\n\nFor Illumina paired-end data, overlapping read pairs can be merged to create longer, higher-quality reads. This is particularly beneficial for genome assembly.\n\nOption A: Moderate FilteringOption B: Stringent Filtering\n\n\n# Merge overlapping Illumina read pairs (Option A)\nexport opt=A\nfastp \\\n  --merge --correction --merged_out 02-primary/merged/${SN}-${opt}-illumina.fastq.gz \\\n  --disable_quality_filtering \\\n  -i 02-primary/trimmed/${SN}-${opt}-illumina_R1.fastq.gz \\\n  -I 02-primary/trimmed/${SN}-${opt}-illumina_R2.fastq.gz \\\n  -o 02-primary/merged/${SN}-${opt}-illumina_R1.fastq.gz \\\n  -O 02-primary/merged/${SN}-${opt}-illumina_R2.fastq.gz \\\n  --detect_adapter_for_pe --trim_poly_g --trim_poly_x \\\n  --html merged/${SN}-${opt}-illumina_merge_fastp.html \\\n  --thread $NCPUS\n\n\n# Merge overlapping Illumina read pairs (Option B)\nexport opt=B\nfastp \\\n  --merge --correction --merged_out 02-primary/merged/${SN}-${opt}-illumina.fastq.gz \\\n  --disable_quality_filtering \\\n  -i 02-primary/trimmed/${SN}-${opt}-illumina_R1.fastq.gz \\\n  -I 02-primary/trimmed/${SN}-${opt}-illumina_R2.fastq.gz \\\n  -o 02-primary/merged/${SN}-${opt}-illumina_R1.fastq.gz \\\n  -O 02-primary/merged/${SN}-${opt}-illumina_R2.fastq.gz \\\n  --detect_adapter_for_pe --trim_poly_g --trim_poly_x \\\n  --html merged/${SN}-${opt}-illumina_merge_fastp.html \\\n  --thread $NCPUS\n\n\n\nMerging benefits:\n\nCreates longer reads from overlapping pairs\nImproves base quality through consensus\nReduces data complexity for assembly algorithms\nMaintains unmerged pairs for additional coverage\n\n\n\n\nAfter preprocessing, we need to verify that our filtering and trimming steps improved data quality without excessive data loss.\n\n\nfastqc --threads $NCPUS --memory 8000 \\\n  --extract --outdir 02-primary/fastQC/processed \\\n  02-primary/merged/*illumina*.fastq.gz\n\n\n\nNanoPlot --fastq 02-primary/trimmed/${SN}-nanopore.fastq.gz \\\n  --outdir 02-primary/nanoplot/processed --info_in_report \\\n  --loglength --threads $NCPUS\n\n\n\n\nTaxonomic classification helps identify potential contamination and provides insights into the composition of your sequencing data. We’ll use Kraken2, a fast and accurate taxonomic classifier, on both full datasets and subsampled data to demonstrate scalable analysis approaches.\n\n\nFirst, we need to download and set up the Kraken2 database. We’ll use the PlusPF database which includes bacteria, archaea, viruses, fungi, and protozoa:\n# Download and extract Kraken2 database\ncd 02-primary/kraken2\nmkdir -p k2_pluspf_08gb_20250402\ncd k2_pluspf_08gb_20250402\nwget https://genome-idx.s3.amazonaws.com/kraken/k2_pluspf_08gb_20250402.tar.gz\ntar -xzf k2_pluspf_08gb_20250402.tar.gz\nrm k2_pluspf_08gb_20250402.tar.gz\ncd ../../..\n\nNote that we are using a capped database limited to 8Gb (in place of ~70). Therefore sensitivity will not be the best. In real world scenario it’s recommended to use the complete database.\n\n\n\n\nFor demonstration purposes and faster analysis, we’ll create subsampled datasets with 100,000 reads each. This approach is useful for:\n\nQuick preliminary analysis\nTesting workflows before full-scale analysis\nReducing computational requirements for large datasets\n\n# Create subsamples of trimmed Illumina data (100k reads each)\n# Set seed for reproducible subsampling\nexport SEED=42\nexport opt=A\n# Subsample Option A datasets\nseqtk sample -s $SEED 02-primary/trimmed/${SN}-${opt}-illumina_R1.fastq.gz 100000 | \\\n  gzip &gt; 02-primary/trimmed/sub/${SN}-${opt}-illumina_R1_sub100k.fastq.gz\n\nseqtk sample -s $SEED 02-primary/trimmed/${SN}-${opt}-illumina_R2.fastq.gz 100000 | \\\n  gzip &gt; 02-primary/trimmed/sub/${SN}-${opt}-illumina_R2_sub100k.fastq.gz\n\nNote that only 1 dataset is enough.\n\nseqtk parameters explained:\n\nsample -s SEED: Random sampling with fixed seed for reproducibility\n100000: Number of reads to sample\nOutput is compressed with gzip for space efficiency\n\n\n\n\nNow we’ll run Kraken2 on subsampled dataset to demonstrate the workflow scalability:\n\n\n# Run Kraken2 on subsampled datasets (faster analysis)\nexport opt=A\nkraken2 --db 02-primary/kraken2/k2_pluspf_08gb_20250402 \\\n  --threads $NCPUS --paired \\\n  --output 02-primary/kraken2/${SN}-${opt}-illumina_sub100k.kraken \\\n  --report 02-primary/kraken2/${SN}-${opt}-illumina_sub100k.report \\\n  02-primary/trimmed/sub/${SN}-${opt}-illumina_R1_sub100k.fastq.gz \\\n  02-primary/trimmed/sub/${SN}-${opt}-illumina_R2_sub100k.fastq.gz\nKraken2 parameters explained:\n\n--db: Path to Kraken2 database\n--threads: Number of CPU threads to use\n--paired: Indicates paired-end reads\n--output: Detailed classification output (optional, can be large)\n--report: Summary report with taxonomic composition\n\n\n\n\n\nThe Kraken2 report provides taxonomic composition with the following columns:\n\nPercentage of reads: Fraction of reads assigned to this taxon\nNumber of reads: Absolute count of reads assigned\nNumber of reads at this level: Reads assigned specifically to this taxon\nTaxonomic rank: D (domain), K (kingdom), P (phylum), C (class), O (order), F (family), G (genus), S (species)\nNCBI taxonomy ID: Unique identifier for the taxon\nScientific name: Taxonomic name\n\nKey metrics to examine:\n\nUnclassified reads: High percentages may indicate novel organisms or poor database coverage\nDominant taxa: Should align with expected organism(s)\nContamination indicators: Unexpected taxa may suggest contamination\n\n\n\n\n\nMultiQC aggregates results from multiple QC tools into a single, interactive HTML report, making it easy to compare before/after preprocessing results.\n# Generate comprehensive MultiQC report\nmultiqc 02-primary/ --outdir 02-primary/reports/ --dirs\nMultiQC benefits:\n\nCombines FastQC, NanoPlot, kraken2, and fastp reports\nProvides side-by-side comparisons\nInteractive plots for detailed exploration\nSummary statistics across all samples\n\n\n\n\nAfter completing this primary analysis workflow, you should observe:\n02-primary\n├── fastQC\n│   ├── processed\n│   │   ├── tuto482-A-illumina_fastqc/\n│   │   ├── tuto482-A-illumina_fastqc.html\n│   │   ├── tuto482-A-illumina_fastqc.zip\n│   │   ├── tuto482-A-illumina_R1_fastqc/\n│   │   ├── tuto482-A-illumina_R1_fastqc.html\n│   │   ├── tuto482-A-illumina_R1_fastqc.zip\n│   │   ├── tuto482-A-illumina_R2_fastqc/\n│   │   ├── tuto482-A-illumina_R2_fastqc.html\n│   │   ├── tuto482-A-illumina_R2_fastqc.zip\n│   │   ├── tuto482-B-illumina_fastqc/\n│   │   ├── tuto482-B-illumina_fastqc.html\n│   │   ├── tuto482-B-illumina_fastqc.zip\n│   │   ├── tuto482-B-illumina_R1_fastqc/\n│   │   ├── tuto482-B-illumina_R1_fastqc.html\n│   │   ├── tuto482-B-illumina_R1_fastqc.zip\n│   │   ├── tuto482-B-illumina_R2_fastqc/\n│   │   ├── tuto482-B-illumina_R2_fastqc.html\n│   │   └── tuto482-B-illumina_R2_fastqc.zip\n│   └── raw\n│       ├── tuto482-illumina_R1_fastqc\n│       ├── tuto482-illumina_R1_fastqc.html\n│       ├── tuto482-illumina_R1_fastqc.zip\n│       ├── tuto482-illumina_R2_fastqc/\n│       ├── tuto482-illumina_R2_fastqc.html\n│       └── tuto482-illumina_R2_fastqc.zip\n├── merged\n│   ├── tuto482-A-illumina.fastq.gz\n│   ├── tuto482-A-illumina_R1.fastq.gz\n│   ├── tuto482-A-illumina_R2.fastq.gz\n│   ├── tuto482-B-illumina.fastq.gz\n│   ├── tuto482-B-illumina_R1.fastq.gz\n│   └── tuto482-B-illumina_R2.fastq.gz\n├── nanoplot\n│   ├── processed\n│   │   ├── ...\n│   │   ├── NanoPlot-report.html\n│   │   ├── NanoStats.txt\n│   │   ├── ...\n│   └── raw\n│       ├── ...\n│       ├── NanoPlot-report.html\n│       ├── NanoStats.txt\n│       ├── ...\n├── reports\n│   ├── multiqc_data/\n│   └── multiqc_report.html\n└── trimmed\n    ├── sub/\n    ├── tuto482-A-illumina_fastp.html\n    ├── tuto482-A-illumina_R1.fastq.gz\n    ├── tuto482-A-illumina_R2.fastq.gz\n    ├── tuto482-B-illumina_fastp.html\n    ├── tuto482-B-illumina_R1.fastq.gz\n    ├── tuto482-B-illumina_R2.fastq.gz\n    ├── tuto482-nanopore_fastp.html\n    └── tuto482-nanopore.fastq.gz\n\n\n\nIncreased average quality scores across read positions\nReduced adapter contamination to near-zero levels\n\n\n\n\n\nRemoval of very short reads that don’t contribute to assembly\nImproved average quality while maintaining read length advantages\n\n\n\n\n\nWith high-quality, preprocessed sequencing data, we’re now ready to proceed to:\n\nK-mer analysis to estimate genome characteristics\nDe novo genome assembly using hybrid approaches\nAssembly quality assessment and validation\n\nThe cleaned datasets will serve as input for downstream assembly algorithms, where the improved data quality will directly translate to better assembly contiguity and accuracy.\n\n\n\nIn de novo genome assembly, it’s considered best practice to generate multiple processed datasets using different preprocessing parameters as input for assembly. This approach leverages the fundamental trade-off between data quality and sequencing depth.\n\n\nDifferent preprocessing stringency levels create datasets with distinct characteristics:\n\nLess stringent filtering (Option A): Higher sequencing depth but potentially lower average quality\nMore stringent filtering (Option B): Higher average quality but reduced sequencing depth\n\nThe optimal balance depends on several genome-specific factors.\n\n\n\n\nWhat is the expected depth for Illumina data (A/B)?\n\nCalculate: (Total bases sequenced) / (Estimated genome size)\nRecommended minimum: 30-50x for short reads (100x is ideal); 10x-30x for long read"
  },
  {
    "objectID": "hands-on/02-primary-analysis.html#configuration-and-setup",
    "href": "hands-on/02-primary-analysis.html#configuration-and-setup",
    "title": "Primary Analysis: Quality Control and Preprocessing",
    "section": "",
    "text": "We’ll continue using the environment variables from the previous chapter. If starting fresh, set them again:\nexport SN=tuto482\nexport NCPUS=12\n\n\n\nThis workflow uses several bioinformatics tools, each optimized for specific data types and tasks:\n\nFastQC: Quality assessment for Illumina short reads\nNanoPlot: Quality assessment specialized for long reads (Nanopore/PacBio)\nfastp: Ultra-fast preprocessing tool for both short and long reads\nMultiQC: Aggregates QC reports from multiple tools\n\n\n\nalias fastqc=\"apptainer run docker://staphb/fastqc fastqc\"\nalias NanoPlot=\"apptainer run docker://staphb/nanoplot NanoPlot\"\nalias fastp=\"apptainer run docker://staphb/fastp fastp\"\nalias multiqc=\"apptainer run docker://staphb/multiqc multiqc\"\nalias seqtk=\"apptainer run docker://staphb/seqtk seqtk\"\nalias kraken2=\"apptainer run docker://staphb/kraken2 kraken2\"\n\n\n\nalias fastqc=\"docker run --rm -u $UID:$GID -v \\$(pwd):/data -w /data staphb/fastqc fastqc\"\nalias nanoplot=\"docker run --rm -u $UID:$GID -v \\$(pwd):/data -w /data staphb/nanoplot NanoPlot\"\nalias fastp=\"docker run --rm -u $UID:$GID -v \\$(pwd):/data -w /data staphb/fastp fastp\"\nalias multiqc=\"docker run --rm -u $UID:$GID -v \\$(pwd):/data -w /data staphb/multiqc multiqc\"\nalias seqtk=\"docker run --rm -u $UID:$GID -v \\$(pwd):/data -w /data staphb/seqtk seqtk\"\nalias kraken2=\"docker run --rm -u $UID:$GID -v \\$(pwd):/data -w /data staphb/kraken2 kraken2\"\n\n\n\n# Create environment with all required tools\nconda create -n qc-tools -c bioconda fastqc nanoplot fastp kraken2 seqtk multiqc\n\n# Activate the environment\nconda activate qc-tools"
  },
  {
    "objectID": "hands-on/02-primary-analysis.html#directory-structure-setup",
    "href": "hands-on/02-primary-analysis.html#directory-structure-setup",
    "title": "Primary Analysis: Quality Control and Preprocessing",
    "section": "",
    "text": "Let’s create an organized directory structure for our analysis:\n# Navigate to working directory and create structure\nmkdir -p 02-primary/{trimmed,merged,reports,kraken2}\nmkdir -p 02-primary/{fastQC,nanoplot}/{raw,processed}\nmkdir -p 02-primary/trimmed/sub\nDirectory structure:\n\nraw/: Raw sequencing data and initial QC reports\ntrimmed/: Filtered and trimmed data with QC reports\ntrimmed/sub/: Subsampled datasets for analysis\nmerged/: Merged Illumina reads (when applicable) with QC\nkraken2/: Kraken2 database and taxonomic classification results\nreports/: Comprehensive MultiQC reports"
  },
  {
    "objectID": "hands-on/02-primary-analysis.html#initial-quality-control-assessment",
    "href": "hands-on/02-primary-analysis.html#initial-quality-control-assessment",
    "title": "Primary Analysis: Quality Control and Preprocessing",
    "section": "",
    "text": "Before any preprocessing, we need to understand the characteristics of our raw data. This baseline assessment helps us:\n\nIdentify potential issues (adapter contamination, quality drops, etc.)\nSet appropriate filtering parameters\nEvaluate the success of preprocessing steps\n\n\n\nFastQC provides comprehensive quality metrics for short-read data, including per-base quality scores, GC content, adapter content, and sequence duplication levels.\n# Run FastQC on raw Illumina data\nfastqc --threads $NCPUS --memory 8000 \\\n  --extract --outdir 02-primary/fastQC/raw/ \\\n  01-rawdata/${SN}-illumina_R1.fastq.gz 01-rawdata/${SN}-illumina_R2.fastq.gz\nKey FastQC metrics to examine:\n\nPer base sequence quality: Should be &gt;20 (preferably &gt;30) across most of the read\nPer sequence quality scores: Distribution should be skewed toward high quality\nAdapter content: Should be minimal in good data\nSequence duplication levels: High duplication may indicate PCR bias\n\n\n\n\nNanoPlot is specifically designed for long-read technologies and provides metrics relevant to Nanopore data characteristics.\n# Run NanoPlot on raw Nanopore data\nmkdir -p 02-primary/nanoplot/\nNanoPlot --fastq 01-rawdata/${SN}-nanopore.fastq.gz \\\n  --outdir 02-primary/nanoplot/raw/ --info_in_report \\\n  --loglength --threads $NCPUS\nKey NanoPlot metrics to examine:\n\nRead length distribution: Nanopore typically produces reads from 1kb to &gt;100kb\nQuality score distribution: Lower than Illumina but should show reasonable distribution\nYield over time: Helps assess sequencing run performance"
  },
  {
    "objectID": "hands-on/02-primary-analysis.html#preprocessing-filtering-and-trimming",
    "href": "hands-on/02-primary-analysis.html#preprocessing-filtering-and-trimming",
    "title": "Primary Analysis: Quality Control and Preprocessing",
    "section": "",
    "text": "Based on the initial QC results, we’ll now filter and trim our data to improve quality while retaining maximum useful information.\n\n\nfastp is an ultra-fast tool that combines multiple preprocessing steps: adapter trimming, quality filtering, and read merging. We provide two filtering options with different stringency levels:\n\nOption A: Moderate FilteringOption B: Stringent Filtering\n\n\nParameters: Average quality ≥28, minimum length 101bp\nThis option provides a balance between data retention and quality improvement, suitable for most genome assembly projects.\n# Run fastp for Illumina paired-end data (Option A)\nexport opt=A\nfastp \\\n  -i 01-rawdata/${SN}-illumina_R1.fastq.gz -I 01-rawdata/${SN}-illumina_R2.fastq.gz \\\n  -o 02-primary/trimmed/${SN}-${opt}-illumina_R1.fastq.gz \\\n  -O 02-primary/trimmed/${SN}-${opt}-illumina_R2.fastq.gz \\\n  --detect_adapter_for_pe --trim_poly_g --trim_poly_x \\\n  --average_qual 28 --length_required 101 \\\n  --html 02-primary/trimmed/${SN}-${opt}-illumina_fastp.html \\\n  --cut_right_window_size 4 --cut_right_mean_quality 20 --cut_right \\\n  --cut_tail_window_size 4 --cut_tail_mean_quality 25 --cut_tail \\\n  --thread $NCPUS\n\n\nParameters: Average quality ≥30, minimum length 125bp\nThis option provides higher quality thresholds for projects requiring maximum data quality, though with potentially higher data loss.\n# Run fastp for Illumina paired-end data (Option B)\nexport opt=B\nfastp \\\n  -i 01-rawdata/${SN}-illumina_R1.fastq.gz -I 01-rawdata/${SN}-illumina_R2.fastq.gz \\\n  -o 02-primary/trimmed/${SN}-${opt}-illumina_R1.fastq.gz \\\n  -O 02-primary/trimmed/${SN}-${opt}-illumina_R2.fastq.gz \\\n  --detect_adapter_for_pe --trim_poly_g --trim_poly_x \\\n  --average_qual 30 --length_required 125 \\\n  --html 02-primary/trimmed/${SN}-${opt}-illumina_fastp.html \\\n  --cut_right_window_size 4 --cut_right_mean_quality 25 --cut_right \\\n  --cut_tail_window_size 4 --cut_tail_mean_quality 28 --cut_tail \\\n  --thread $NCPUS\n\n\n\nfastp parameters explained:\n\n--detect_adapter_for_pe: Automatically detect and remove adapters\n--trim_poly_g/x: Remove poly-G/X tails (common in NextSeq/NovaSeq)\n--average_qual: Minimum average quality score threshold\n--length_required: Minimum read length after trimming\n--cut_right/tail: Quality-based trimming from 3’ end\n\nChoosing between options: - Option A is recommended for most projects, providing good quality while preserving data - Option B is suitable when maximum quality is required and sufficient coverage depth is available\n\n\n\nFor Nanopore data, we focus on length and quality filtering rather than adapter trimming:\n# Run fastp for Nanopore data (quality and length filtering)\nfastp \\\n  -i 01-rawdata/${SN}-nanopore.fastq.gz -o 02-primary/trimmed/${SN}-nanopore.fastq.gz \\\n  --disable_adapter_trimming \\\n  --average_qual 16 --length_required 1000 \\\n  --html 02-primary/trimmed/${SN}-nanopore_fastp.html \\\n  --thread $NCPUS\nNanopore-specific parameters:\n\n--disable_adapter_trimming: Adapters typically removed during basecalling\n--average_qual 16: More lenient quality threshold for long reads\n--length_required 1000: Remove very short reads that are less useful for assembly\n\n\nNote that basecalling and important preliminary steps to clean nanopore reads are not seen here."
  },
  {
    "objectID": "hands-on/02-primary-analysis.html#read-merging-illumina-only",
    "href": "hands-on/02-primary-analysis.html#read-merging-illumina-only",
    "title": "Primary Analysis: Quality Control and Preprocessing",
    "section": "",
    "text": "For Illumina paired-end data, overlapping read pairs can be merged to create longer, higher-quality reads. This is particularly beneficial for genome assembly.\n\nOption A: Moderate FilteringOption B: Stringent Filtering\n\n\n# Merge overlapping Illumina read pairs (Option A)\nexport opt=A\nfastp \\\n  --merge --correction --merged_out 02-primary/merged/${SN}-${opt}-illumina.fastq.gz \\\n  --disable_quality_filtering \\\n  -i 02-primary/trimmed/${SN}-${opt}-illumina_R1.fastq.gz \\\n  -I 02-primary/trimmed/${SN}-${opt}-illumina_R2.fastq.gz \\\n  -o 02-primary/merged/${SN}-${opt}-illumina_R1.fastq.gz \\\n  -O 02-primary/merged/${SN}-${opt}-illumina_R2.fastq.gz \\\n  --detect_adapter_for_pe --trim_poly_g --trim_poly_x \\\n  --html merged/${SN}-${opt}-illumina_merge_fastp.html \\\n  --thread $NCPUS\n\n\n# Merge overlapping Illumina read pairs (Option B)\nexport opt=B\nfastp \\\n  --merge --correction --merged_out 02-primary/merged/${SN}-${opt}-illumina.fastq.gz \\\n  --disable_quality_filtering \\\n  -i 02-primary/trimmed/${SN}-${opt}-illumina_R1.fastq.gz \\\n  -I 02-primary/trimmed/${SN}-${opt}-illumina_R2.fastq.gz \\\n  -o 02-primary/merged/${SN}-${opt}-illumina_R1.fastq.gz \\\n  -O 02-primary/merged/${SN}-${opt}-illumina_R2.fastq.gz \\\n  --detect_adapter_for_pe --trim_poly_g --trim_poly_x \\\n  --html merged/${SN}-${opt}-illumina_merge_fastp.html \\\n  --thread $NCPUS\n\n\n\nMerging benefits:\n\nCreates longer reads from overlapping pairs\nImproves base quality through consensus\nReduces data complexity for assembly algorithms\nMaintains unmerged pairs for additional coverage"
  },
  {
    "objectID": "hands-on/02-primary-analysis.html#post-processing-quality-control",
    "href": "hands-on/02-primary-analysis.html#post-processing-quality-control",
    "title": "Primary Analysis: Quality Control and Preprocessing",
    "section": "",
    "text": "After preprocessing, we need to verify that our filtering and trimming steps improved data quality without excessive data loss.\n\n\nfastqc --threads $NCPUS --memory 8000 \\\n  --extract --outdir 02-primary/fastQC/processed \\\n  02-primary/merged/*illumina*.fastq.gz\n\n\n\nNanoPlot --fastq 02-primary/trimmed/${SN}-nanopore.fastq.gz \\\n  --outdir 02-primary/nanoplot/processed --info_in_report \\\n  --loglength --threads $NCPUS"
  },
  {
    "objectID": "hands-on/02-primary-analysis.html#taxonomic-classification-with-kraken2",
    "href": "hands-on/02-primary-analysis.html#taxonomic-classification-with-kraken2",
    "title": "Primary Analysis: Quality Control and Preprocessing",
    "section": "",
    "text": "Taxonomic classification helps identify potential contamination and provides insights into the composition of your sequencing data. We’ll use Kraken2, a fast and accurate taxonomic classifier, on both full datasets and subsampled data to demonstrate scalable analysis approaches.\n\n\nFirst, we need to download and set up the Kraken2 database. We’ll use the PlusPF database which includes bacteria, archaea, viruses, fungi, and protozoa:\n# Download and extract Kraken2 database\ncd 02-primary/kraken2\nmkdir -p k2_pluspf_08gb_20250402\ncd k2_pluspf_08gb_20250402\nwget https://genome-idx.s3.amazonaws.com/kraken/k2_pluspf_08gb_20250402.tar.gz\ntar -xzf k2_pluspf_08gb_20250402.tar.gz\nrm k2_pluspf_08gb_20250402.tar.gz\ncd ../../..\n\nNote that we are using a capped database limited to 8Gb (in place of ~70). Therefore sensitivity will not be the best. In real world scenario it’s recommended to use the complete database.\n\n\n\n\nFor demonstration purposes and faster analysis, we’ll create subsampled datasets with 100,000 reads each. This approach is useful for:\n\nQuick preliminary analysis\nTesting workflows before full-scale analysis\nReducing computational requirements for large datasets\n\n# Create subsamples of trimmed Illumina data (100k reads each)\n# Set seed for reproducible subsampling\nexport SEED=42\nexport opt=A\n# Subsample Option A datasets\nseqtk sample -s $SEED 02-primary/trimmed/${SN}-${opt}-illumina_R1.fastq.gz 100000 | \\\n  gzip &gt; 02-primary/trimmed/sub/${SN}-${opt}-illumina_R1_sub100k.fastq.gz\n\nseqtk sample -s $SEED 02-primary/trimmed/${SN}-${opt}-illumina_R2.fastq.gz 100000 | \\\n  gzip &gt; 02-primary/trimmed/sub/${SN}-${opt}-illumina_R2_sub100k.fastq.gz\n\nNote that only 1 dataset is enough.\n\nseqtk parameters explained:\n\nsample -s SEED: Random sampling with fixed seed for reproducibility\n100000: Number of reads to sample\nOutput is compressed with gzip for space efficiency\n\n\n\n\nNow we’ll run Kraken2 on subsampled dataset to demonstrate the workflow scalability:\n\n\n# Run Kraken2 on subsampled datasets (faster analysis)\nexport opt=A\nkraken2 --db 02-primary/kraken2/k2_pluspf_08gb_20250402 \\\n  --threads $NCPUS --paired \\\n  --output 02-primary/kraken2/${SN}-${opt}-illumina_sub100k.kraken \\\n  --report 02-primary/kraken2/${SN}-${opt}-illumina_sub100k.report \\\n  02-primary/trimmed/sub/${SN}-${opt}-illumina_R1_sub100k.fastq.gz \\\n  02-primary/trimmed/sub/${SN}-${opt}-illumina_R2_sub100k.fastq.gz\nKraken2 parameters explained:\n\n--db: Path to Kraken2 database\n--threads: Number of CPU threads to use\n--paired: Indicates paired-end reads\n--output: Detailed classification output (optional, can be large)\n--report: Summary report with taxonomic composition\n\n\n\n\n\nThe Kraken2 report provides taxonomic composition with the following columns:\n\nPercentage of reads: Fraction of reads assigned to this taxon\nNumber of reads: Absolute count of reads assigned\nNumber of reads at this level: Reads assigned specifically to this taxon\nTaxonomic rank: D (domain), K (kingdom), P (phylum), C (class), O (order), F (family), G (genus), S (species)\nNCBI taxonomy ID: Unique identifier for the taxon\nScientific name: Taxonomic name\n\nKey metrics to examine:\n\nUnclassified reads: High percentages may indicate novel organisms or poor database coverage\nDominant taxa: Should align with expected organism(s)\nContamination indicators: Unexpected taxa may suggest contamination"
  },
  {
    "objectID": "hands-on/02-primary-analysis.html#comprehensive-reporting-with-multiqc",
    "href": "hands-on/02-primary-analysis.html#comprehensive-reporting-with-multiqc",
    "title": "Primary Analysis: Quality Control and Preprocessing",
    "section": "",
    "text": "MultiQC aggregates results from multiple QC tools into a single, interactive HTML report, making it easy to compare before/after preprocessing results.\n# Generate comprehensive MultiQC report\nmultiqc 02-primary/ --outdir 02-primary/reports/ --dirs\nMultiQC benefits:\n\nCombines FastQC, NanoPlot, kraken2, and fastp reports\nProvides side-by-side comparisons\nInteractive plots for detailed exploration\nSummary statistics across all samples"
  },
  {
    "objectID": "hands-on/02-primary-analysis.html#expected-outcomes",
    "href": "hands-on/02-primary-analysis.html#expected-outcomes",
    "title": "Primary Analysis: Quality Control and Preprocessing",
    "section": "",
    "text": "After completing this primary analysis workflow, you should observe:\n02-primary\n├── fastQC\n│   ├── processed\n│   │   ├── tuto482-A-illumina_fastqc/\n│   │   ├── tuto482-A-illumina_fastqc.html\n│   │   ├── tuto482-A-illumina_fastqc.zip\n│   │   ├── tuto482-A-illumina_R1_fastqc/\n│   │   ├── tuto482-A-illumina_R1_fastqc.html\n│   │   ├── tuto482-A-illumina_R1_fastqc.zip\n│   │   ├── tuto482-A-illumina_R2_fastqc/\n│   │   ├── tuto482-A-illumina_R2_fastqc.html\n│   │   ├── tuto482-A-illumina_R2_fastqc.zip\n│   │   ├── tuto482-B-illumina_fastqc/\n│   │   ├── tuto482-B-illumina_fastqc.html\n│   │   ├── tuto482-B-illumina_fastqc.zip\n│   │   ├── tuto482-B-illumina_R1_fastqc/\n│   │   ├── tuto482-B-illumina_R1_fastqc.html\n│   │   ├── tuto482-B-illumina_R1_fastqc.zip\n│   │   ├── tuto482-B-illumina_R2_fastqc/\n│   │   ├── tuto482-B-illumina_R2_fastqc.html\n│   │   └── tuto482-B-illumina_R2_fastqc.zip\n│   └── raw\n│       ├── tuto482-illumina_R1_fastqc\n│       ├── tuto482-illumina_R1_fastqc.html\n│       ├── tuto482-illumina_R1_fastqc.zip\n│       ├── tuto482-illumina_R2_fastqc/\n│       ├── tuto482-illumina_R2_fastqc.html\n│       └── tuto482-illumina_R2_fastqc.zip\n├── merged\n│   ├── tuto482-A-illumina.fastq.gz\n│   ├── tuto482-A-illumina_R1.fastq.gz\n│   ├── tuto482-A-illumina_R2.fastq.gz\n│   ├── tuto482-B-illumina.fastq.gz\n│   ├── tuto482-B-illumina_R1.fastq.gz\n│   └── tuto482-B-illumina_R2.fastq.gz\n├── nanoplot\n│   ├── processed\n│   │   ├── ...\n│   │   ├── NanoPlot-report.html\n│   │   ├── NanoStats.txt\n│   │   ├── ...\n│   └── raw\n│       ├── ...\n│       ├── NanoPlot-report.html\n│       ├── NanoStats.txt\n│       ├── ...\n├── reports\n│   ├── multiqc_data/\n│   └── multiqc_report.html\n└── trimmed\n    ├── sub/\n    ├── tuto482-A-illumina_fastp.html\n    ├── tuto482-A-illumina_R1.fastq.gz\n    ├── tuto482-A-illumina_R2.fastq.gz\n    ├── tuto482-B-illumina_fastp.html\n    ├── tuto482-B-illumina_R1.fastq.gz\n    ├── tuto482-B-illumina_R2.fastq.gz\n    ├── tuto482-nanopore_fastp.html\n    └── tuto482-nanopore.fastq.gz\n\n\n\nIncreased average quality scores across read positions\nReduced adapter contamination to near-zero levels\n\n\n\n\n\nRemoval of very short reads that don’t contribute to assembly\nImproved average quality while maintaining read length advantages"
  },
  {
    "objectID": "hands-on/02-primary-analysis.html#next-steps",
    "href": "hands-on/02-primary-analysis.html#next-steps",
    "title": "Primary Analysis: Quality Control and Preprocessing",
    "section": "",
    "text": "With high-quality, preprocessed sequencing data, we’re now ready to proceed to:\n\nK-mer analysis to estimate genome characteristics\nDe novo genome assembly using hybrid approaches\nAssembly quality assessment and validation\n\nThe cleaned datasets will serve as input for downstream assembly algorithms, where the improved data quality will directly translate to better assembly contiguity and accuracy."
  },
  {
    "objectID": "hands-on/02-primary-analysis.html#multiple-datasets-for-assembly",
    "href": "hands-on/02-primary-analysis.html#multiple-datasets-for-assembly",
    "title": "Primary Analysis: Quality Control and Preprocessing",
    "section": "",
    "text": "In de novo genome assembly, it’s considered best practice to generate multiple processed datasets using different preprocessing parameters as input for assembly. This approach leverages the fundamental trade-off between data quality and sequencing depth.\n\n\nDifferent preprocessing stringency levels create datasets with distinct characteristics:\n\nLess stringent filtering (Option A): Higher sequencing depth but potentially lower average quality\nMore stringent filtering (Option B): Higher average quality but reduced sequencing depth\n\nThe optimal balance depends on several genome-specific factors.\n\n\n\n\nWhat is the expected depth for Illumina data (A/B)?\n\nCalculate: (Total bases sequenced) / (Estimated genome size)\nRecommended minimum: 30-50x for short reads (100x is ideal); 10x-30x for long read"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "1 About This Tutorial\nThe tutorial was elaborated by Julien FOURET for the NGS4ECOPROD Workshop in Tunisia 2024 Funded by the European Commision. Reworked after."
  },
  {
    "objectID": "hands-on/01-fetch-public-datasets.html",
    "href": "hands-on/01-fetch-public-datasets.html",
    "title": "Fetch Public Datasets",
    "section": "",
    "text": "In this tutorial, we will download publicly available genomic datasets from the NCBI Sequence Read Archive (SRA). This is a common first step in genomic analysis workflows, allowing researchers to access high-quality sequencing data for comparative studies or method development.\nWe have selected PRJNA721899 in which 6 differently virulent strains of use QUAST for eval.\nwere sequenced using both Illumina (short-read) and Oxford Nanopore (long-read) technologies. This dual-platform approach provides complementary data types ideal for hybrid genome assembly.\nWe will focus on sample SAMN13621855 which corresponds to strain 482, a pathogenic isolate that will serve as our model organism for this de novo genome assembly tutorial.\nThis work was published in PMC8248858, providing valuable context about the biological significance of these strains.\nDataset overview:\n\nIllumina paired-end data: SRR14237206 (high accuracy, shorter reads)\nOxford Nanopore data: SRR14237202 (longer reads, higher error rate)\n\n\n\n\n\nFirst, we’ll set up environment variables to store our SRA accession numbers. This makes our scripts more readable and easier to modify:\nexport SRR_ILLUMINA=SRR14237206\nexport SRR_NANOPORE=SRR14237202\n#set a sample name (SN)\nexport SN=tuto482\nThese variables will be used throughout the data download process to reference our specific datasets. The sample name (SN) provides a consistent identifier that will be used across all files, making it easier to track and organize data from multiple samples in larger projects.\n\n\n\nTo download data from the SRA, we need the SRA Tools package, which includes prefetch and fasterq-dump utilities. Additionally, we’ll use pigz for parallel compression of FASTQ files.\nBelow are three different approaches to install and run these tools:\nWe will use alias then for the rest of the tutorial to be agnostic from the.\nThe use of sra-tools to fetch raw fastq is documented here.\n\n\nalias prefetch=\"apptainer run docker://ncbi/sra-tools prefetch\"\nalias fasterq-dump=\"apptainer run docker://ncbi/sra-tools fasterq-dump\"\n\n\n\nalias prefetch=\"docker run --rm -u $UID:$GID -v \\$(pwd):/data -w /data ncbi/sra-tools prefetch\"\nalias fasterq-dump=\"docker run --rm -u $UID:$GID -v \\$(pwd):/data -w /data ncbi/sra-tools fasterq-dump\"\nNote: The -v $(pwd):/data -w /data flags mount your current directory to /data inside the container and set it as the working directory, ensuring downloaded files are accessible on your host system.\n\n\n\nThe SRA Tools package is available through the Bioconda channel. Create a dedicated conda environment for this tutorial:\n# Create a new conda environment with sra-tools\nconda create -n sra-tools -c bioconda sra-tools\n\n# Activate the environment\nconda activate sra-tools\nOnce installed, the prefetch and fasterq-dump commands will be directly available in your PATH without needing aliases.\n\n\n\n\n\n## Prefetch the SRA file\nprefetch $SRR_ILLUMINA\n\n## Extract Fastq from SRA\nfasterq-dump $SRR_ILLUMINA --split-files --progress\n\n## Compress files\npigz -p 8 ${SRR_ILLUMINA}_* \n## Clean\nrm -rf $SRR_ILLUMINA\n## Rename\nmkdir -p 01-rawdata/\nmv ${SRR_ILLUMINA}_1.fastq.gz 01-rawdata/$SN-illumina_R1.fastq.gz\nmv ${SRR_ILLUMINA}_2.fastq.gz 01-rawdata/$SN-illumina_R2.fastq.gz\n\nNote that this time, as the next, can be long to run; especially with a slow bandwidth.\n\n\n\n\nThe Nanopore data download follows a similar workflow, but note that long-read data typically produces a single FASTQ file (unlike paired-end Illumina data):\n## Download the Nanopore SRA file\nprefetch $SRR_NANOPORE\n\n## Extract FASTQ from SRA (single file for long reads)\nfasterq-dump $SRR_NANOPORE --split-files --progress\n\n## Compress the FASTQ file\npigz -p 8 ${SRR_NANOPORE}.fastq \n\n## Clean up intermediate files\nrm -rf $SRR_NANOPORE\n\n## Move to organized location with descriptive name\nmv ${SRR_NANOPORE}.fastq.gz 01-rawdata/$SN-nanopore.fastq.gz\nExpected output: A single compressed FASTQ file containing Oxford Nanopore long reads, stored in the 01-rawdata/ directory.\n\n\n\nls -lh 01-rawdata\ntotal 3,4G\n-rw-rw-r-- 1 jfouret jfouret 1,3G juil.  3 09:27 tuto482-illumina_R1.fastq.gz\n-rw-rw-r-- 1 jfouret jfouret 1,4G juil.  3 09:27 tuto482-illumina_R2.fastq.gz\n-rw-rw-r-- 1 jfouret jfouret 724M juil.  3 10:08 tuto482-nanopore.fastq.gz"
  },
  {
    "objectID": "hands-on/01-fetch-public-datasets.html#configuration-and-setup",
    "href": "hands-on/01-fetch-public-datasets.html#configuration-and-setup",
    "title": "Fetch Public Datasets",
    "section": "",
    "text": "First, we’ll set up environment variables to store our SRA accession numbers. This makes our scripts more readable and easier to modify:\nexport SRR_ILLUMINA=SRR14237206\nexport SRR_NANOPORE=SRR14237202\n#set a sample name (SN)\nexport SN=tuto482\nThese variables will be used throughout the data download process to reference our specific datasets. The sample name (SN) provides a consistent identifier that will be used across all files, making it easier to track and organize data from multiple samples in larger projects.\n\n\n\nTo download data from the SRA, we need the SRA Tools package, which includes prefetch and fasterq-dump utilities. Additionally, we’ll use pigz for parallel compression of FASTQ files.\nBelow are three different approaches to install and run these tools:\nWe will use alias then for the rest of the tutorial to be agnostic from the.\nThe use of sra-tools to fetch raw fastq is documented here.\n\n\nalias prefetch=\"apptainer run docker://ncbi/sra-tools prefetch\"\nalias fasterq-dump=\"apptainer run docker://ncbi/sra-tools fasterq-dump\"\n\n\n\nalias prefetch=\"docker run --rm -u $UID:$GID -v \\$(pwd):/data -w /data ncbi/sra-tools prefetch\"\nalias fasterq-dump=\"docker run --rm -u $UID:$GID -v \\$(pwd):/data -w /data ncbi/sra-tools fasterq-dump\"\nNote: The -v $(pwd):/data -w /data flags mount your current directory to /data inside the container and set it as the working directory, ensuring downloaded files are accessible on your host system.\n\n\n\nThe SRA Tools package is available through the Bioconda channel. Create a dedicated conda environment for this tutorial:\n# Create a new conda environment with sra-tools\nconda create -n sra-tools -c bioconda sra-tools\n\n# Activate the environment\nconda activate sra-tools\nOnce installed, the prefetch and fasterq-dump commands will be directly available in your PATH without needing aliases."
  },
  {
    "objectID": "hands-on/01-fetch-public-datasets.html#fetch-illumina-data",
    "href": "hands-on/01-fetch-public-datasets.html#fetch-illumina-data",
    "title": "Fetch Public Datasets",
    "section": "",
    "text": "## Prefetch the SRA file\nprefetch $SRR_ILLUMINA\n\n## Extract Fastq from SRA\nfasterq-dump $SRR_ILLUMINA --split-files --progress\n\n## Compress files\npigz -p 8 ${SRR_ILLUMINA}_* \n## Clean\nrm -rf $SRR_ILLUMINA\n## Rename\nmkdir -p 01-rawdata/\nmv ${SRR_ILLUMINA}_1.fastq.gz 01-rawdata/$SN-illumina_R1.fastq.gz\nmv ${SRR_ILLUMINA}_2.fastq.gz 01-rawdata/$SN-illumina_R2.fastq.gz\n\nNote that this time, as the next, can be long to run; especially with a slow bandwidth."
  },
  {
    "objectID": "hands-on/01-fetch-public-datasets.html#fetch-nanopore-data",
    "href": "hands-on/01-fetch-public-datasets.html#fetch-nanopore-data",
    "title": "Fetch Public Datasets",
    "section": "",
    "text": "The Nanopore data download follows a similar workflow, but note that long-read data typically produces a single FASTQ file (unlike paired-end Illumina data):\n## Download the Nanopore SRA file\nprefetch $SRR_NANOPORE\n\n## Extract FASTQ from SRA (single file for long reads)\nfasterq-dump $SRR_NANOPORE --split-files --progress\n\n## Compress the FASTQ file\npigz -p 8 ${SRR_NANOPORE}.fastq \n\n## Clean up intermediate files\nrm -rf $SRR_NANOPORE\n\n## Move to organized location with descriptive name\nmv ${SRR_NANOPORE}.fastq.gz 01-rawdata/$SN-nanopore.fastq.gz\nExpected output: A single compressed FASTQ file containing Oxford Nanopore long reads, stored in the 01-rawdata/ directory."
  },
  {
    "objectID": "hands-on/01-fetch-public-datasets.html#expected-outputs",
    "href": "hands-on/01-fetch-public-datasets.html#expected-outputs",
    "title": "Fetch Public Datasets",
    "section": "",
    "text": "ls -lh 01-rawdata\ntotal 3,4G\n-rw-rw-r-- 1 jfouret jfouret 1,3G juil.  3 09:27 tuto482-illumina_R1.fastq.gz\n-rw-rw-r-- 1 jfouret jfouret 1,4G juil.  3 09:27 tuto482-illumina_R2.fastq.gz\n-rw-rw-r-- 1 jfouret jfouret 724M juil.  3 10:08 tuto482-nanopore.fastq.gz"
  },
  {
    "objectID": "hands-on/03-kmer-analysis.html",
    "href": "hands-on/03-kmer-analysis.html",
    "title": "K-mer Analysis: Genome Characteristics Estimation",
    "section": "",
    "text": "K-mer analysis is a fundamental step in genome assembly workflows that provides crucial insights into genome characteristics before attempting assembly. By analyzing the frequency distribution of k-mers (DNA sequences of length k) in sequencing data, we can estimate important genome properties such as genome size, heterozygosity levels, and repetitive content.\nThis analysis helps us:\n\nEstimate genome size independently of assembly quality\nAssess genome complexity (heterozygosity, repeats)\nChoose optimal k-mer sizes for assembly algorithms\nCompare preprocessing strategies (Option A vs Option B)\n\nWe’ll use KAT (K-mer Analysis Toolkit), a comprehensive suite designed specifically for k-mer frequency analysis in genomic data. KAT generates detailed histograms and statistics that reveal the underlying structure of our sequencing datasets.\n\n\n\n\nIn an ideal diploid genome with uniform coverage, k-mer frequency distributions typically show:\n\nPeak at 1x coverage: Heterozygous regions (single-copy k-mers)\nPeak at 2x coverage: Homozygous regions (double-copy k-mers)\nHigher frequency peaks: Repetitive elements\n\nThe position and shape of these peaks provide quantitative estimates of genome characteristics.\n\n\n\nDifferent k-mer sizes reveal different aspects of genome structure:\n\nSmall k-mers (5-15): Sensitive to sequencing errors, good for error detection\nMedium k-mers (21-31): Optimal balance for most genomes\nLarge k-mers (35-81): Better specificity, useful for complex genomes\n\n\n\n\n\n\n\nWe’ll continue using the established environment variables from previous chapters:\nexport SN=tuto482\nexport NCPUS=12\n\n\n\nKAT requires specific installation due to its specialized k-mer analysis algorithms. We provide three installation approaches consistent with the tutorial framework:\n\n\nalias kat=\"apptainer run docker://quay.io/biocontainers/kat:2.4.2--py36hc902310_3 kat\"\n\n\n\nalias kat=\"docker run --rm -u $UID:$GID -v \\$(pwd):/data -w /data quay.io/biocontainers/kat:2.4.2--py36hc902310_3 kat\"\n\n\n\n# Create environment with KAT\nconda create -n kat-analysis -c bioconda kat\n\n# Activate the environment\nconda activate kat-analysis\n\n\n\n\nCreate an organized directory structure for k-mer analysis results:\n# Create main k-mer analysis directory\nmkdir -p 03-kmer-analysis/{kat,plots,results}\n\n# Create subdirectories for different analysis types\nmkdir -p 03-kmer-analysis/kat/{option-A,option-B}\nDirectory structure:\n\nkat/: KAT histogram outputs and statistics\nplots/: Generated visualizations and plots\nresults/: Summary tables and analysis results\n\n\n\n\n\nWe’ll perform comprehensive k-mer analysis on both preprocessing options (A and B) using all three file types (R1, R2, merged) simultaneously. KAT can process multiple input files in a single command, which is more efficient and provides a combined analysis.\n\n\nWe’ll test a comprehensive range of k-mer sizes: seq 21 4 39; seq 41 6 71 ; seq 71 10 101\nThis range covers:\n\nSmall k-mers for error assessment\nStandard assembly k-mer sizes\nLarge k-mers for complex genome regions\n\n\n\n\nWe’ll analyze all three file types (R1, R2, and merged) together:\nfor opt in A B; do\n    for k in $(seq 21 4 39; seq 41 6 71 ; seq 71 10 101); do\n        echo \"Processing Option $opt (all files) with k-mer size: $k\"\n        kat hist \\\n            -o 03-kmer-analysis/kat/option-${opt}/hist-k${k} \\\n            -H 250000000 \\\n            -t $NCPUS \\\n            -m $k \\\n            -p pdf \\\n            02-primary/merged/${SN}-${opt}-illumina*.fastq.gz \\\n            &gt; 03-kmer-analysis/kat/option-${opt}/hist-k${k}.out 2&gt;&1\n    done\ndone\nKAT parameters explained:\n\n-o: Output file prefix\n-H: Maximum hash table size (250M entries)\n-t: Number of threads\n-m: K-mer size\n-p pdf: Generate PDF plots of histograms\n\n\n\n\n\nAfter running all k-mer analyses, we need to extract genome size estimates from the output files and organize them for visualization.\n\n\n# Create results summary file\necho \"Option,KmerSize,GenomeSize\" &gt; 03-kmer-analysis/results/genome_size_estimates.csv\n\n# Extract genome size estimates from Option A results\nfor opt in A B; do\n    for k in $(seq 21 4 39; seq 41 6 71 ; seq 71 10 101); do\n        if [ -f \"03-kmer-analysis/kat/option-${opt}/hist-k${k}.out\" ]; then\n            genome_size=$(grep \"Estimated genome size\" 03-kmer-analysis/kat/option-${opt}/hist-k${k}.out | \\\n                        awk '{print $4}' | sed 's/,//g')\n            if [ ! -z \"$genome_size\" ]; then\n                echo \"${opt},${k},${genome_size}\" &gt;&gt; 03-kmer-analysis/results/genome_size_estimates.csv\n            fi\n        fi\n    done\ndone\n\n\n\n\nNow we’ll create comprehensive visualizations to compare genome size estimates across different k-mer sizes and preprocessing options.\n\n\nCreate an R script for generating publication-quality plots:\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(scales)\nlibrary(viridis)\n\n# Set working directory and load data\ndata &lt;- read_csv(\"03-kmer-analysis/results/genome_size_estimates.csv\")\n\n# Convert GenomeSize to numeric (handle potential formatting issues)\ndata$GenomeSize &lt;- as.numeric(gsub(\",\", \"\", data$GenomeSize))\n\n# Create factor levels for proper ordering\ndata$Option &lt;- factor(data$Option, levels = c(\"A\", \"B\"))\n\n# Remove abberant values:\ndata &lt;- filter(data, .data$GenomeSize &lt; 80)\n\n# Create a detailed comparison plot\np1 &lt;- ggplot(data, aes(x = KmerSize, y = GenomeSize, color = Option)) +\n  geom_point(size = 2.5, alpha = 0.8) +\n  geom_line(aes(group = Option), size = 1, alpha = 0.7) +\n  geom_smooth(aes(group = Option), method = \"loess\", se = TRUE, alpha = 0.2) +\n  scale_color_viridis_d(name = \"Preprocessing\\nOption\") +\n  scale_x_continuous(breaks = c(5, 15, 25, 35, 61, 81, 91, 101, 111, 121, 131)) +\n  scale_y_continuous(labels = comma_format()) +\n  labs(title = \"Genome Size Estimates with Trend Lines\",\n       subtitle = \"Smoothed trends showing convergence patterns\",\n       x = \"K-mer Size\",\n       y = \"Estimated Genome Size (Mb)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        strip.text = element_text(face = \"bold\"))\n\n# Save trend plot\nggsave(\"03-kmer-analysis/plots/genome_size_trends.png\", p1, width = 12, height = 6, dpi = 300)\nggsave(\"03-kmer-analysis/plots/genome_size_trends.pdf\", p1, width = 12, height = 6)\n\n\n\n\n\n\nThe generated KAT histograms reveal important genome characteristics:\n\nError Peak (low frequency): K-mers appearing once, often due to sequencing errors\nHeterozygous Peak: K-mers from heterozygous regions (single-copy)\nHomozygous Peak: K-mers from homozygous regions (double-copy)\nRepetitive Peaks: Higher frequency peaks indicating repetitive elements\n\n\n\n\nGenome size estimates should:\n\nConverge across different k-mer sizes (21-31 range)\nShow consistency between preprocessing options\nReflect the combined information from all read types\n\n\n\n\nFor downstream assembly, consider:\n\nK-mer sizes 21-31: Generally optimal for most genomes\nStable estimates: Choose k-mer sizes with consistent genome size estimates\nAssembly algorithm requirements: Some assemblers have preferred k-mer ranges\n\n\n\n\nCompare Option A vs Option B:\n\nData retention: Option A typically retains more data\nQuality improvement: Option B provides higher quality reads\nGenome size consistency: Both should give similar estimates if preprocessing is effective\n\n\n\n\n\nAfter completing the k-mer analysis, you should have:\n03-kmer-analysis/\n├── kat/\n│   ├── option-A/\n│   │   ├── hist-k5.out, hist-k5.dist, hist-k5.dist.pdf\n│   │   ├── hist-k9.out, hist-k9.dist, hist-k9.dist.pdf\n│   │   └── ... (for all k-mer sizes)\n│   └── option-B/\n│       ├── hist-k5.out, hist-k5.dist, hist-k5.dist.pdf\n│       ├── hist-k9.out, hist-k9.dist, hist-k9.dist.pdf\n│       └── ... (for all k-mer sizes)\n├── plots/\n│   ├── visualize_kmer_analysis.R\n│   ├── genome_size_vs_kmer.png\n│   ├── genome_size_vs_kmer.pdf\n│   ├── genome_size_trends.png\n│   └── genome_size_trends.pdf\n└── results/\n    ├── genome_size_estimates.csv\n    ├── summary_statistics.csv\n    ├── optimal_kmer_recommendations.csv\n    └── option_comparison.csv\n\n\n\nGenome size estimates: Should be consistent across k-mer sizes 21-31\nPreprocessing effects: Compare Option A vs B impact on estimates\nConvergence patterns: Look for stable estimates in the optimal k-mer range\nOptimal k-mer selection: Identify stable k-mer sizes for assembly\n\n\n\n\n\nWith comprehensive k-mer analysis complete, you’re ready to proceed to:\n\nHybrid genome assembly using optimal k-mer sizes identified\nAssembly parameter optimization based on genome characteristics\nQuality assessment comparing assembly results with k-mer predictions\n\nThe k-mer analysis provides crucial baseline metrics that will help evaluate assembly success and guide parameter selection for downstream analysis steps.\n\n\n\n\n\nMemory errors: Reduce hash table size (-H parameter) or use smaller k-mer sizes first Missing genome size estimates: Check KAT output files for errors or warnings Inconsistent estimates: May indicate contamination or highly heterozygous regions R visualization errors: Ensure all required packages are installed:\nR -e \"install.packages(c('ggplot2', 'dplyr', 'readr', 'scales', 'viridis'))\""
  },
  {
    "objectID": "hands-on/03-kmer-analysis.html#theoretical-background",
    "href": "hands-on/03-kmer-analysis.html#theoretical-background",
    "title": "K-mer Analysis: Genome Characteristics Estimation",
    "section": "",
    "text": "In an ideal diploid genome with uniform coverage, k-mer frequency distributions typically show:\n\nPeak at 1x coverage: Heterozygous regions (single-copy k-mers)\nPeak at 2x coverage: Homozygous regions (double-copy k-mers)\nHigher frequency peaks: Repetitive elements\n\nThe position and shape of these peaks provide quantitative estimates of genome characteristics.\n\n\n\nDifferent k-mer sizes reveal different aspects of genome structure:\n\nSmall k-mers (5-15): Sensitive to sequencing errors, good for error detection\nMedium k-mers (21-31): Optimal balance for most genomes\nLarge k-mers (35-81): Better specificity, useful for complex genomes"
  },
  {
    "objectID": "hands-on/03-kmer-analysis.html#configuration-and-setup",
    "href": "hands-on/03-kmer-analysis.html#configuration-and-setup",
    "title": "K-mer Analysis: Genome Characteristics Estimation",
    "section": "",
    "text": "We’ll continue using the established environment variables from previous chapters:\nexport SN=tuto482\nexport NCPUS=12\n\n\n\nKAT requires specific installation due to its specialized k-mer analysis algorithms. We provide three installation approaches consistent with the tutorial framework:\n\n\nalias kat=\"apptainer run docker://quay.io/biocontainers/kat:2.4.2--py36hc902310_3 kat\"\n\n\n\nalias kat=\"docker run --rm -u $UID:$GID -v \\$(pwd):/data -w /data quay.io/biocontainers/kat:2.4.2--py36hc902310_3 kat\"\n\n\n\n# Create environment with KAT\nconda create -n kat-analysis -c bioconda kat\n\n# Activate the environment\nconda activate kat-analysis\n\n\n\n\nCreate an organized directory structure for k-mer analysis results:\n# Create main k-mer analysis directory\nmkdir -p 03-kmer-analysis/{kat,plots,results}\n\n# Create subdirectories for different analysis types\nmkdir -p 03-kmer-analysis/kat/{option-A,option-B}\nDirectory structure:\n\nkat/: KAT histogram outputs and statistics\nplots/: Generated visualizations and plots\nresults/: Summary tables and analysis results"
  },
  {
    "objectID": "hands-on/03-kmer-analysis.html#k-mer-frequency-analysis",
    "href": "hands-on/03-kmer-analysis.html#k-mer-frequency-analysis",
    "title": "K-mer Analysis: Genome Characteristics Estimation",
    "section": "",
    "text": "We’ll perform comprehensive k-mer analysis on both preprocessing options (A and B) using all three file types (R1, R2, merged) simultaneously. KAT can process multiple input files in a single command, which is more efficient and provides a combined analysis.\n\n\nWe’ll test a comprehensive range of k-mer sizes: seq 21 4 39; seq 41 6 71 ; seq 71 10 101\nThis range covers:\n\nSmall k-mers for error assessment\nStandard assembly k-mer sizes\nLarge k-mers for complex genome regions\n\n\n\n\nWe’ll analyze all three file types (R1, R2, and merged) together:\nfor opt in A B; do\n    for k in $(seq 21 4 39; seq 41 6 71 ; seq 71 10 101); do\n        echo \"Processing Option $opt (all files) with k-mer size: $k\"\n        kat hist \\\n            -o 03-kmer-analysis/kat/option-${opt}/hist-k${k} \\\n            -H 250000000 \\\n            -t $NCPUS \\\n            -m $k \\\n            -p pdf \\\n            02-primary/merged/${SN}-${opt}-illumina*.fastq.gz \\\n            &gt; 03-kmer-analysis/kat/option-${opt}/hist-k${k}.out 2&gt;&1\n    done\ndone\nKAT parameters explained:\n\n-o: Output file prefix\n-H: Maximum hash table size (250M entries)\n-t: Number of threads\n-m: K-mer size\n-p pdf: Generate PDF plots of histograms"
  },
  {
    "objectID": "hands-on/03-kmer-analysis.html#results-extraction-and-summary",
    "href": "hands-on/03-kmer-analysis.html#results-extraction-and-summary",
    "title": "K-mer Analysis: Genome Characteristics Estimation",
    "section": "",
    "text": "After running all k-mer analyses, we need to extract genome size estimates from the output files and organize them for visualization.\n\n\n# Create results summary file\necho \"Option,KmerSize,GenomeSize\" &gt; 03-kmer-analysis/results/genome_size_estimates.csv\n\n# Extract genome size estimates from Option A results\nfor opt in A B; do\n    for k in $(seq 21 4 39; seq 41 6 71 ; seq 71 10 101); do\n        if [ -f \"03-kmer-analysis/kat/option-${opt}/hist-k${k}.out\" ]; then\n            genome_size=$(grep \"Estimated genome size\" 03-kmer-analysis/kat/option-${opt}/hist-k${k}.out | \\\n                        awk '{print $4}' | sed 's/,//g')\n            if [ ! -z \"$genome_size\" ]; then\n                echo \"${opt},${k},${genome_size}\" &gt;&gt; 03-kmer-analysis/results/genome_size_estimates.csv\n            fi\n        fi\n    done\ndone"
  },
  {
    "objectID": "hands-on/03-kmer-analysis.html#data-visualization-with-r-and-ggplot2",
    "href": "hands-on/03-kmer-analysis.html#data-visualization-with-r-and-ggplot2",
    "title": "K-mer Analysis: Genome Characteristics Estimation",
    "section": "",
    "text": "Now we’ll create comprehensive visualizations to compare genome size estimates across different k-mer sizes and preprocessing options.\n\n\nCreate an R script for generating publication-quality plots:\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(scales)\nlibrary(viridis)\n\n# Set working directory and load data\ndata &lt;- read_csv(\"03-kmer-analysis/results/genome_size_estimates.csv\")\n\n# Convert GenomeSize to numeric (handle potential formatting issues)\ndata$GenomeSize &lt;- as.numeric(gsub(\",\", \"\", data$GenomeSize))\n\n# Create factor levels for proper ordering\ndata$Option &lt;- factor(data$Option, levels = c(\"A\", \"B\"))\n\n# Remove abberant values:\ndata &lt;- filter(data, .data$GenomeSize &lt; 80)\n\n# Create a detailed comparison plot\np1 &lt;- ggplot(data, aes(x = KmerSize, y = GenomeSize, color = Option)) +\n  geom_point(size = 2.5, alpha = 0.8) +\n  geom_line(aes(group = Option), size = 1, alpha = 0.7) +\n  geom_smooth(aes(group = Option), method = \"loess\", se = TRUE, alpha = 0.2) +\n  scale_color_viridis_d(name = \"Preprocessing\\nOption\") +\n  scale_x_continuous(breaks = c(5, 15, 25, 35, 61, 81, 91, 101, 111, 121, 131)) +\n  scale_y_continuous(labels = comma_format()) +\n  labs(title = \"Genome Size Estimates with Trend Lines\",\n       subtitle = \"Smoothed trends showing convergence patterns\",\n       x = \"K-mer Size\",\n       y = \"Estimated Genome Size (Mb)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        strip.text = element_text(face = \"bold\"))\n\n# Save trend plot\nggsave(\"03-kmer-analysis/plots/genome_size_trends.png\", p1, width = 12, height = 6, dpi = 300)\nggsave(\"03-kmer-analysis/plots/genome_size_trends.pdf\", p1, width = 12, height = 6)"
  },
  {
    "objectID": "hands-on/03-kmer-analysis.html#results-interpretation",
    "href": "hands-on/03-kmer-analysis.html#results-interpretation",
    "title": "K-mer Analysis: Genome Characteristics Estimation",
    "section": "",
    "text": "The generated KAT histograms reveal important genome characteristics:\n\nError Peak (low frequency): K-mers appearing once, often due to sequencing errors\nHeterozygous Peak: K-mers from heterozygous regions (single-copy)\nHomozygous Peak: K-mers from homozygous regions (double-copy)\nRepetitive Peaks: Higher frequency peaks indicating repetitive elements\n\n\n\n\nGenome size estimates should:\n\nConverge across different k-mer sizes (21-31 range)\nShow consistency between preprocessing options\nReflect the combined information from all read types\n\n\n\n\nFor downstream assembly, consider:\n\nK-mer sizes 21-31: Generally optimal for most genomes\nStable estimates: Choose k-mer sizes with consistent genome size estimates\nAssembly algorithm requirements: Some assemblers have preferred k-mer ranges\n\n\n\n\nCompare Option A vs Option B:\n\nData retention: Option A typically retains more data\nQuality improvement: Option B provides higher quality reads\nGenome size consistency: Both should give similar estimates if preprocessing is effective"
  },
  {
    "objectID": "hands-on/03-kmer-analysis.html#expected-outcomes",
    "href": "hands-on/03-kmer-analysis.html#expected-outcomes",
    "title": "K-mer Analysis: Genome Characteristics Estimation",
    "section": "",
    "text": "After completing the k-mer analysis, you should have:\n03-kmer-analysis/\n├── kat/\n│   ├── option-A/\n│   │   ├── hist-k5.out, hist-k5.dist, hist-k5.dist.pdf\n│   │   ├── hist-k9.out, hist-k9.dist, hist-k9.dist.pdf\n│   │   └── ... (for all k-mer sizes)\n│   └── option-B/\n│       ├── hist-k5.out, hist-k5.dist, hist-k5.dist.pdf\n│       ├── hist-k9.out, hist-k9.dist, hist-k9.dist.pdf\n│       └── ... (for all k-mer sizes)\n├── plots/\n│   ├── visualize_kmer_analysis.R\n│   ├── genome_size_vs_kmer.png\n│   ├── genome_size_vs_kmer.pdf\n│   ├── genome_size_trends.png\n│   └── genome_size_trends.pdf\n└── results/\n    ├── genome_size_estimates.csv\n    ├── summary_statistics.csv\n    ├── optimal_kmer_recommendations.csv\n    └── option_comparison.csv\n\n\n\nGenome size estimates: Should be consistent across k-mer sizes 21-31\nPreprocessing effects: Compare Option A vs B impact on estimates\nConvergence patterns: Look for stable estimates in the optimal k-mer range\nOptimal k-mer selection: Identify stable k-mer sizes for assembly"
  },
  {
    "objectID": "hands-on/03-kmer-analysis.html#next-steps",
    "href": "hands-on/03-kmer-analysis.html#next-steps",
    "title": "K-mer Analysis: Genome Characteristics Estimation",
    "section": "",
    "text": "With comprehensive k-mer analysis complete, you’re ready to proceed to:\n\nHybrid genome assembly using optimal k-mer sizes identified\nAssembly parameter optimization based on genome characteristics\nQuality assessment comparing assembly results with k-mer predictions\n\nThe k-mer analysis provides crucial baseline metrics that will help evaluate assembly success and guide parameter selection for downstream analysis steps."
  },
  {
    "objectID": "hands-on/03-kmer-analysis.html#troubleshooting",
    "href": "hands-on/03-kmer-analysis.html#troubleshooting",
    "title": "K-mer Analysis: Genome Characteristics Estimation",
    "section": "",
    "text": "Memory errors: Reduce hash table size (-H parameter) or use smaller k-mer sizes first Missing genome size estimates: Check KAT output files for errors or warnings Inconsistent estimates: May indicate contamination or highly heterozygous regions R visualization errors: Ensure all required packages are installed:\nR -e \"install.packages(c('ggplot2', 'dplyr', 'readr', 'scales', 'viridis'))\""
  },
  {
    "objectID": "hands-on/05-assembly-evaluation.html",
    "href": "hands-on/05-assembly-evaluation.html",
    "title": "Assembly evaluation with QUAST and BUSCO",
    "section": "",
    "text": "In this chapter, we will evaluate the quality of the assemblies generated by SPAdes and MaSuRCA using QUAST (Quality Assessment Tool for Genome Assemblies) and BUSCO (Benchmarking Universal Single-Copy Orthologs). QUAST computes a range of metrics to assess assembly contiguity, completeness, and correctness against a reference genome, while BUSCO provides a measure of gene content completeness.\n\n\n\n\nWe’ll continue using the established environment variables:\nexport SN=tuto482\nexport NCPUS=12\n\n\n\nWe will use QUAST and BUSCO through Apptainer containers.\nalias quast.py=\"apptainer run docker://staphb/quast quast.py\"\n\n\n\nBUSCO requires write access to the container’s internal storage for downloading databases and creating temporary files. Since Apptainer’s default SIF format is read-only, we need to create a writable sandbox directory from the Docker image.\napptainer build --sandbox busco_sandbox docker://ezlabgva/busco:v6.0.0_cv1\nalias busco=\"apptainer exec --writable busco_sandbox busco\"\n\n\n\n\nCreate an organized directory structure for the evaluation results:\nmkdir -p 05-quast-evaluation\nmkdir -p 05-busco-evaluation\n\n\n\n\n\nFirst, we download the reference genome for our target organism.\nwget -P 05-quast-evaluation https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/013/085/055/GCF_013085055.1_ASM1308505v1/GCF_013085055.1_ASM1308505v1_genomic.fna.gz\n\n\n\nNow, we will run QUAST, providing the Illumina and Nanopore reads, all generated assemblies, and the reference genome. We will also enable fungus-specific settings.\nWe will use the reads from preprocessing Option A for this evaluation.\nquast.py \\\n  --fungus \\\n  -r 05-quast-evaluation/GCF_013085055.1_ASM1308505v1_genomic.fna.gz \\\n  -1 02-primary/merged/${SN}-A-illumina_R1.fastq \\\n  -2 02-primary/merged/${SN}-A-illumina_R2.fastq \\\n  --nanopore 02-primary/trimmed/${SN}-nanopore.fastq \\\n  -o 05-quast-evaluation \\\n  -t $NCPUS \\\n  --labels \"spades_A, spades_B, masurca_A, masurca_B\" \\\n  04-assembly/spades/option-A/scaffolds.fasta \\\n  04-assembly/spades/option-B/scaffolds.fasta \\\n  04-assembly/masurca/option-A/flye*/assembly.fasta \\\n  04-assembly/masurca/option-B/flye*/assembly.fasta\nQUAST parameters explained:\n\n--fungus: Enables fungus-specific settings for GeneMark.\n-r: Path to the reference genome.\n-1/-2: Paired-end Illumina reads.\n--nanopore: Nanopore long reads.\n-o: Output directory for the results.\n-t: Number of CPU threads.\n--labels: Human-readable names for the assemblies in the report.\nPositional arguments: Paths to the assembly files to be evaluated.\n\n\n\n\n\n\n\nWe will use the fungi_odb10 database for our analysis.\nbusco --download fungi_odb10 --download_path 05-busco-evaluation/datasets\n\n\n\nWe will now run BUSCO on each of our four assemblies.\nfor assembly in 04-assembly/spades/option-A/scaffolds.fasta 04-assembly/spades/option-B/scaffolds.fasta 04-assembly/masurca/option-A/flye*/assembly.fasta 04-assembly/masurca/option-B/flye*/assembly.fasta; do\n  label=$(basename $(dirname $assembly))_$(basename $(dirname $(dirname $assembly)))\n  busco -i $assembly -o 05-busco-evaluation/${label} --download_path 05-busco-evaluation/datasets -l fungi_odb10 -m genome -c $NCPUS --force\ndone\nBUSCO parameters explained:\n\n-i: Input assembly file.\n-o: Output directory for the results.\n-l: Lineage dataset to use.\n-m: Mode of analysis (genome).\n-c: Number of CPU threads.\n--force: Allows overwriting of existing results.\n\n\n\n\n\nAfter QUAST and BUSCO finish, the 05-quast-evaluation and 05-busco-evaluation directories will contain comprehensive reports with various metrics and plots comparing the quality of the different assemblies."
  },
  {
    "objectID": "hands-on/05-assembly-evaluation.html#configuration-and-setup",
    "href": "hands-on/05-assembly-evaluation.html#configuration-and-setup",
    "title": "Assembly evaluation with QUAST and BUSCO",
    "section": "",
    "text": "We’ll continue using the established environment variables:\nexport SN=tuto482\nexport NCPUS=12\n\n\n\nWe will use QUAST and BUSCO through Apptainer containers.\nalias quast.py=\"apptainer run docker://staphb/quast quast.py\"\n\n\n\nBUSCO requires write access to the container’s internal storage for downloading databases and creating temporary files. Since Apptainer’s default SIF format is read-only, we need to create a writable sandbox directory from the Docker image.\napptainer build --sandbox busco_sandbox docker://ezlabgva/busco:v6.0.0_cv1\nalias busco=\"apptainer exec --writable busco_sandbox busco\""
  },
  {
    "objectID": "hands-on/05-assembly-evaluation.html#directory-structure-setup",
    "href": "hands-on/05-assembly-evaluation.html#directory-structure-setup",
    "title": "Assembly evaluation with QUAST and BUSCO",
    "section": "",
    "text": "Create an organized directory structure for the evaluation results:\nmkdir -p 05-quast-evaluation\nmkdir -p 05-busco-evaluation"
  },
  {
    "objectID": "hands-on/05-assembly-evaluation.html#quast-evaluation",
    "href": "hands-on/05-assembly-evaluation.html#quast-evaluation",
    "title": "Assembly evaluation with QUAST and BUSCO",
    "section": "",
    "text": "First, we download the reference genome for our target organism.\nwget -P 05-quast-evaluation https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/013/085/055/GCF_013085055.1_ASM1308505v1/GCF_013085055.1_ASM1308505v1_genomic.fna.gz\n\n\n\nNow, we will run QUAST, providing the Illumina and Nanopore reads, all generated assemblies, and the reference genome. We will also enable fungus-specific settings.\nWe will use the reads from preprocessing Option A for this evaluation.\nquast.py \\\n  --fungus \\\n  -r 05-quast-evaluation/GCF_013085055.1_ASM1308505v1_genomic.fna.gz \\\n  -1 02-primary/merged/${SN}-A-illumina_R1.fastq \\\n  -2 02-primary/merged/${SN}-A-illumina_R2.fastq \\\n  --nanopore 02-primary/trimmed/${SN}-nanopore.fastq \\\n  -o 05-quast-evaluation \\\n  -t $NCPUS \\\n  --labels \"spades_A, spades_B, masurca_A, masurca_B\" \\\n  04-assembly/spades/option-A/scaffolds.fasta \\\n  04-assembly/spades/option-B/scaffolds.fasta \\\n  04-assembly/masurca/option-A/flye*/assembly.fasta \\\n  04-assembly/masurca/option-B/flye*/assembly.fasta\nQUAST parameters explained:\n\n--fungus: Enables fungus-specific settings for GeneMark.\n-r: Path to the reference genome.\n-1/-2: Paired-end Illumina reads.\n--nanopore: Nanopore long reads.\n-o: Output directory for the results.\n-t: Number of CPU threads.\n--labels: Human-readable names for the assemblies in the report.\nPositional arguments: Paths to the assembly files to be evaluated."
  },
  {
    "objectID": "hands-on/05-assembly-evaluation.html#busco-evaluation",
    "href": "hands-on/05-assembly-evaluation.html#busco-evaluation",
    "title": "Assembly evaluation with QUAST and BUSCO",
    "section": "",
    "text": "We will use the fungi_odb10 database for our analysis.\nbusco --download fungi_odb10 --download_path 05-busco-evaluation/datasets\n\n\n\nWe will now run BUSCO on each of our four assemblies.\nfor assembly in 04-assembly/spades/option-A/scaffolds.fasta 04-assembly/spades/option-B/scaffolds.fasta 04-assembly/masurca/option-A/flye*/assembly.fasta 04-assembly/masurca/option-B/flye*/assembly.fasta; do\n  label=$(basename $(dirname $assembly))_$(basename $(dirname $(dirname $assembly)))\n  busco -i $assembly -o 05-busco-evaluation/${label} --download_path 05-busco-evaluation/datasets -l fungi_odb10 -m genome -c $NCPUS --force\ndone\nBUSCO parameters explained:\n\n-i: Input assembly file.\n-o: Output directory for the results.\n-l: Lineage dataset to use.\n-m: Mode of analysis (genome).\n-c: Number of CPU threads.\n--force: Allows overwriting of existing results."
  },
  {
    "objectID": "hands-on/05-assembly-evaluation.html#expected-outcomes",
    "href": "hands-on/05-assembly-evaluation.html#expected-outcomes",
    "title": "Assembly evaluation with QUAST and BUSCO",
    "section": "",
    "text": "After QUAST and BUSCO finish, the 05-quast-evaluation and 05-busco-evaluation directories will contain comprehensive reports with various metrics and plots comparing the quality of the different assemblies."
  },
  {
    "objectID": "hands-on/07-repeats-analysis.html",
    "href": "hands-on/07-repeats-analysis.html",
    "title": "Repeats Analysis with RepeatMasker",
    "section": "",
    "text": "In this chapter, we will perform repeat analysis on our assembled genome. Repetitive elements are a major component of eukaryotic genomes and their identification is a crucial step before gene annotation. We will use RepeatMasker, a widely used tool for identifying and classifying repetitive elements.\n\n\nRepeatMasker is a program that screens DNA sequences for interspersed repeats and low complexity DNA sequences. The output of the program is a detailed annotation of the repeats that are present in the query sequence as well as a modified version of the query sequence in which all the annotated repeats have been masked.\nRepeatMasker relies on a library of repeat sequences. We will use Dfam, a database of transposable element profile HMM models and consensus sequences.\n\n\n\nThe primary objective of this chapter is to mask repetitive elements in our assembled genome to prepare it for downstream gene annotation. Repeat masking is essential because:\n\nGene prediction accuracy: Unmasked repeats can lead to false gene predictions, as gene-finding algorithms may incorrectly identify repetitive sequences as exons or coding regions.\nAnnotation quality: Proper repeat masking ensures that gene annotation tools focus on genuine gene sequences rather than being confused by repetitive elements.\nComparative analysis: Masked genomes provide cleaner datasets for comparative genomics and phylogenetic analyses.\n\nImportant note: While we will use RepeatModeler to identify de novo repeat families, our primary goal is comprehensive genome masking rather than the discovery of novel transposable elements. The RepeatModeler step ensures we capture species-specific repeats that may not be present in curated databases like Dfam, thereby improving the completeness of our repeat masking.\n\n\n\n\n\nWe’ll continue using the established environment variables:\nexport NCPUS=12\nexport NPA=$(echo $NCPUS/4 | bc)\nexport SN=Fo47\n\n\n\nInstalling RepeatMasker and all its dependencies can be complex. To simplify this process, we will use a pre-built Apptainer/Docker container from the Dfam consortium, which includes RepeatMasker, RepeatModeler, and all necessary dependencies.\nWe will use the dfam/tetools container.\n\n\nWe will use an alias for the Apptainer command to simplify its usage.\nalias tetools=\"apptainer run docker://dfam/tetools:1.93\"\n\n\n\n\n\n\n\nWe will use a Fusarium assembly from NCBI as the input for our repeat analysis.\nmkdir -p 07-repeats-analysis/assembly\nwget -P 07-repeats-analysis/assembly https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/013/085/055/GCF_013085055.1_ASM1308505v1/GCF_013085055.1_ASM1308505v1_genomic.fna.gz\ngunzip 07-repeats-analysis/assembly/GCF_013085055.1_ASM1308505v1_genomic.fna.gz\nNow we’ll process the assembly file to ensure all sequence lines are in uppercase and rename it appropriately:\n#!/usr/bin/env python3\nimport os\n\n# Process the assembly file: convert sequences to uppercase and rename\ninput_file = \"07-repeats-analysis/assembly/GCF_013085055.1_ASM1308505v1_genomic.fna\"\noutput_file = f\"07-repeats-analysis/assembly/{os.getenv('SN')}.fasta\"\n\nwith open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n    for line in infile:\n        if line.startswith('&gt;'):\n            outfile.write(line)\n        else:\n            outfile.write(line.upper())\n\n# Remove the original file\nos.remove(input_file)\n\n\n\nRepeatMasker requires a library of repeat sequences. We will use the Dfam database. Since we are analyzing a fungal genome, we will download the Dfam partition for fungi.\nAccording to the Dfam README, partition 16 contains Fungi.\nmkdir -p 07-repeats-analysis/libraries/famdb\nwget -P 07-repeats-analysis/libraries/famdb https://www.dfam.org/releases/current/families/FamDB/dfam39_full.0.h5.gz\nwget -P 07-repeats-analysis/libraries/famdb https://www.dfam.org/releases/current/families/FamDB/dfam39_full.16.h5.gz\ngunzip 07-repeats-analysis/libraries/famdb/*.gz\nThe tetools container comes with a minimal library. To use our custom library, we need to create a local Libraries directory and bind-mount it into the container. This approach avoids the NFS mounting issues that can occur with writable sandboxes.\nFirst, let’s copy the original libraries from the container to our host system to have a proper structure:\nmkdir -p 07-repeats-analysis/RepeatMasker\ntetools bash -c \"cp -r /opt/RepeatMasker/Libraries 07-repeats-analysis/RepeatMasker/\"\nNow, move our downloaded Dfam partitions into the local famdb directory:\nln -f 07-repeats-analysis/libraries/famdb/*.h5 07-repeats-analysis/RepeatMasker/Libraries/famdb/\n\n\n\n\nBefore running RepeatMasker with the curated Dfam library, we should first identify species-specific repeats using RepeatModeler. RepeatModeler builds a custom repeat library from our assembly using de novo repeat identification methods. This is particularly important for organisms that may have novel repeat families not present in existing databases.\n\n\nFirst, we need to create a database for RepeatModeler from our assembly:\nmkdir -p 07-repeats-analysis/repeatmodeler\ntetools BuildDatabase -name ${SN} 07-repeats-analysis/assembly/${SN}.fasta\nBuildDatabase parameters explained:\n\n-name: Database name (we use our sample identifier)\nPositional argument: Input assembly file\n\n\n\n\nNow we can run RepeatModeler to identify de novo repeat families. This process can take several hours to days depending on genome size and complexity:\ntetools RepeatModeler \\\n  -database ${SN} \\\n  -threads $NCPUS \\\n  -LTRStruct \\\n  &gt; 07-repeats-analysis/repeatmodeler/${SN}_repeatmodeler.log 2&gt;&1\nRepeatModeler parameters explained:\n\n-database: Name of the database created with BuildDatabase\n-threads: Number of parallel processes\n-LTRStruct: Enable LTR structural analysis for better LTR identification\n\n\n\n\nRepeatModeler creates several output files in a timestamped directory (e.g., RM_YYYYMMDD-HHMMSS). The key output files include:\n\n${SN}-families.fa: Main output - FASTA file containing consensus sequences of identified repeat families\n${SN}-families.stk: Stockholm format alignment file\n${SN}-rmod.log: Detailed log of the RepeatModeler run\n\nMove the results to our organized directory structure:\n# Move RepeatModeler results to our analysis directory\nmv ${SN}-families.fa 07-repeats-analysis/repeatmodeler/\nmv ${SN}-families.stk 07-repeats-analysis/repeatmodeler/\nmv ${SN}-rmod.log 07-repeats-analysis/repeatmodeler/\nmv RM_* 07-repeats-analysis/repeatmodeler/\nrm ${SN}.*\n\n\n\nYou can examine the identified repeat families:\n# Count the number of repeat families identified\ngrep -c \"&gt;\" 07-repeats-analysis/repeatmodeler/${SN}-families.fa\n\n# Look at the first few families\nhead -20 07-repeats-analysis/repeatmodeler/${SN}-families.fa\nThe headers contain classification information when available (e.g., &gt;rnd-1_family-174#LINE/L1) or #Unknown for unclassified elements.\n\n\n\nFirst, we need to reconfigure RepeatMasker to recognize our new Dfam partitions. We’ll use a bind mount to provide our custom libraries to the container and run the update script:\nBIND=\"--bind 07-repeats-analysis/RepeatMasker/Libraries:/opt/RepeatMasker/Libraries\"\nalias tetools_lib=\"apptainer run $BIND docker://dfam/tetools:1.93\"\ntetools_lib bash -c \"rm -f /opt/RepeatMasker/Libraries/famdb/rmlib.config && cd /opt/RepeatMasker && ./tetoolsDfamUpdate.pl\"\nThis command:\n\nBinds our local Libraries directory to /opt/RepeatMasker/Libraries in the container\nRemoves any existing configuration file\nRuns the update script to generate a new configuration based on our custom libraries\n\n\n\n\nSince the -lib and -species options are not compatible, we need to create a combined library that includes both the Dfam fungi sequences and our RepeatModeler-generated families. We’ll use the famdb.py tool to extract fungi sequences from our downloaded famdb files.\nFirst, extract fungi sequences from the Dfam database using famdb.py:\n# Extract fungi sequences from Dfam using famdb.py\ntetools_lib famdb.py -i /opt/RepeatMasker/Libraries/famdb \\\n  families --format fasta_name --ancestors --descendants 'Fungi' \\\n  --include-class-in-name &gt; 07-repeats-analysis/RepeatMasker/Libraries/dfam_fungi.fa\nNow combine the Dfam fungi sequences with our RepeatModeler families:\n# Create combined library\ncat 07-repeats-analysis/RepeatMasker/Libraries/dfam_fungi.fa \\\n    07-repeats-analysis/repeatmodeler/${SN}-families.fa \\\n    &gt; 07-repeats-analysis/RepeatMasker/Libraries/${SN}_combined.fa\nThis combined approach provides:\n\nCurated fungi repeats from the Dfam database\nNovel, species-specific repeats identified by RepeatModeler\nFull compatibility with RepeatMasker’s -lib option\n\n\n\n\n\nBefore using the combined library, let’s first run RepeatMasker using only the Dfam database to see how much repetitive content can be identified with curated repeats alone. This will help us understand the contribution of the de novo RepeatModeler families.\n\n\nRun RepeatMasker using only the species-specific Dfam database:\ntetools_lib RepeatMasker \\\n  -a \\\n  -pa $NPA \\\n  -species fungi \\\n  -dir 07-repeats-analysis/rm_out_dfam_only \\\n  -gff \\\n  07-repeats-analysis/assembly/${SN}.fasta &gt; 07-repeats-analysis/rm_out_dfam_only.log\n\n\n\nWith the Dfam database alone, you should expect to see:\n\nRepeat content (~1% or less) for this Fusarium assembly\nLimited identification of known repeat families\nGaps in repeat annotation due to species-specific repeats not present in Dfam\n\nThis demonstrates why de novo repeat identification with RepeatModeler is crucial for comprehensive repeat annotation, especially for organisms that may have novel or poorly characterized repeat families.\n\n\n\n\nTo run RepeatMasker with our custom libraries, we will use bind mounts. This approach mounts our local Libraries directory over the container’s /opt/RepeatMasker/Libraries directory, making our custom Dfam files available to RepeatMasker without requiring a writable container.\n\n\nNow we can run RepeatMasker on our assembly, again using the bind mount to provide our custom libraries:\ntetools_lib RepeatMasker \\\n  -a \\\n  -pa $NPA \\\n  -lib /opt/RepeatMasker/Libraries/${SN}_combined.fa \\\n  -dir 07-repeats-analysis/rm_out \\\n  -gff \\\n  07-repeats-analysis/assembly/${SN}.fasta &gt; 07-repeats-analysis/rm_out.log\n# alternatively but longerto run, we can use -lib /opt/RepeatMasker/Libraries/${SN}_combined.fa\nRepeatMasker parameters explained:\n\n-a: Show alignments\n-pa: Number of parallel processes (threads = pa * 4).\n-species: Specifies the species or group for which to search repeats. This helps in selecting the appropriate repeat library from Dfam. (not compatible with -lib)\n-lib: Custom repeat library file (our RepeatModeler-generated families).\n-dir: Output directory for the results.\n-gff: Output a GFF file in addition to the standard output.\nPositional argument: The input assembly file.\n\nThis ensures comprehensive repeat annotation using both existing knowledge and de novo discoveries.\n\n\n\n\nRepeatMasker provides several built-in utilities for visualizing repeat content, and the output can also be used with various genome browsers and custom analysis tools.\n\n\nRepeatMasker includes utilities for creating repeat landscape plots that show the evolutionary age and abundance of different repeat families.\n\n\nThe repeat landscape shows the relationship between repeat divergence (evolutionary age) and abundance:\n# Generate divergence data\ntetools_lib calcDivergenceFromAlign.pl \\\n  -s 07-repeats-analysis/rm_out/${SN}.fasta.divsum \\\n  07-repeats-analysis/rm_out/${SN}.fasta.align\n\n# Create repeat landscape plot\ntetools_lib createRepeatLandscape.pl \\\n  -g 50400000 \\\n  -div 07-repeats-analysis/rm_out/${SN}.fasta.divsum \\\n  &gt; 07-repeats-analysis/rm_out/${SN}_repeat_landscape.html\nThis creates an HTML file with an interactive repeat landscape plot.\n\n\n\n\nThe GFF output can be directly loaded into genome browsers for visual inspection:\n\n\n\nLoad your assembly (${SN}.fasta) as the reference genome\nLoad the RepeatMasker GFF file (${SN}.fasta.out.gff) as an annotation track\nNavigate through the genome to examine repeat distribution\n\n\n\n\n\nThe .tbl file provides a comprehensive summary of repeat content:\n# View the repeat summary table\ncat 07-repeats-analysis/rm_out/${SN}.fasta.tbl\nThis table shows:\n\nTotal repeat content percentage\nBreakdown by major repeat classes (LTR, LINE, SINE, DNA transposons, etc.)\nInterspersed repeats vs. small RNA\nLow complexity and simple repeat content\n\n\n\n\n\n\n\nThe Kimura substitution level (also called Kimura distance or K2P distance) is a measure of evolutionary divergence between the identified repeat and its consensus sequence in the database. It represents the estimated percentage of nucleotide substitutions that have occurred since the repeat was originally inserted into the genome.\nKey points about Kimura substitution levels:\n\nLow values (0-5%): Recent insertions, relatively unchanged from the original sequence\nMedium values (5-20%): Moderately aged repeats with accumulated mutations\nHigh values (&gt;20%): Ancient repeats that have significantly diverged from their original sequence\n\nThe Kimura distance accounts for:\n\nTransition mutations (A↔︎G, C↔︎T) occurring more frequently than transversions\nMultiple substitutions at the same position (back-mutations)\nSaturation effects in highly diverged sequences\n\n\n\n\nIn RepeatMasker output tables, you may notice that the total percentage of repeats can sometimes exceed 100% of the genome size. This occurs due to several factors:\n1. Overlapping Annotations:\n\nDifferent repeat families may overlap in the same genomic region\nNested transposable elements (one inserted within another)\nRepeatMasker reports each identified repeat separately, even if they overlap\n\n2. Fragmented Repeat Detection:\n\nLarge repeats broken by mutations may be detected as multiple smaller fragments\nEach fragment contributes to the total count\nThe sum of fragments may exceed the actual genomic space occupied\n\n3. Reference vs. Assembly Differences: - Percentages are calculated relative to the assembly size, not the “true” genome size - If the assembly is incomplete or fragmented, percentages may be inflated - Conversely, if repeats are under-assembled (collapsed), percentages may be deflated\n4. Calculation Method: RepeatMasker calculates percentages as:\nPercentage = (Total bp of identified repeats / Assembly size) × 100\nThe “Total bp of identified repeats” includes all overlapping regions, which can exceed the actual physical space occupied.\nInterpreting the Results:\n\nFocus on relative proportions between repeat classes rather than absolute percentages\nUse the masked assembly to see the actual genomic regions covered by repeats\nConsider the non-redundant repeat content for more accurate genome composition estimates\n\n\n\n\n\nRepeatMasker offers different strategies for accessing repeat libraries, each with distinct advantages and coverage levels:\n\n\nThe -species fungi option uses RepeatMasker’s built-in species-specific filtering:\n\nCurated selection: RepeatMasker selects only well-characterized repeats specific to the taxonomic group\nConservative approach: Focuses on high-confidence, validated repeat families\nLimited coverage: May miss divergent or poorly characterized repeats\nExpected result: ~1% repeat content for Fusarium (minimal coverage)\n\n\n\n\nUsing famdb.py to extract repeats allows more comprehensive library construction:\n\nTaxonomic breadth: Includes repeats from ancestral and descendant taxonomic groups\nComprehensive coverage: Captures more divergent repeat families that may be present\nExpected result: Higher repeat detection rate (not tested in this tutorial, but likely &gt;5%)\n\n\n\n\nThe most comprehensive strategy combines curated and de novo identified repeats:\n\nDfam coverage: Curated, well-characterized repeat families via famdb.py\nSpecies-specific coverage: Novel repeats identified by RepeatModeler\nMaximum sensitivity: Captures both known and novel repetitive elements\nExpected result: ~11% repeat content (most comprehensive masking)\n\n\n\n\n\nBased on different RepeatMasker approaches, expected repeat content for this Fusarium assembly:\nTested approaches:\n\nUsing Dfam database alone (narrow with -species fungi option): ~1% repeat content (limited coverage)\nUsing de novo identified repeats only (RepeatModeler families): ~6% repeat content\nUsing combined approach (Dfam via famdb.py with ancestors/descendants + RepeatModeler): ~11% repeat content (comprehensive masking)\n\nNot tested but expected:\n\nUsing Dfam only (broad via famdb.py with ancestors/descendants): Likely 3-5% repeat content (moderate coverage)\n\nThe progression from 1% → 6% → 11% demonstrates the importance of combining curated databases with de novo repeat identification for comprehensive genome masking before gene annotation.\n\n\n\nAfter RepeatMasker finishes, the 07-repeats-analysis/rm_out directory will contain the results of the repeat analysis, including:\n\n*.masked: The input sequence with repeats masked (replaced by ’N’s).\n*.out: A detailed annotation of the repeats found.\n*.tbl: A summary table of the repeat content.\n*.gff: The repeat annotations in GFF format.\n*.align: Alignment details for repeat matches (if generated).\n*.divsum: Divergence summary (after running calcDivergenceFromAlign.pl).\n*_repeat_landscape.html: Interactive repeat landscape plot (if generated)."
  },
  {
    "objectID": "hands-on/07-repeats-analysis.html#introduction-to-repeatmasker",
    "href": "hands-on/07-repeats-analysis.html#introduction-to-repeatmasker",
    "title": "Repeats Analysis with RepeatMasker",
    "section": "",
    "text": "RepeatMasker is a program that screens DNA sequences for interspersed repeats and low complexity DNA sequences. The output of the program is a detailed annotation of the repeats that are present in the query sequence as well as a modified version of the query sequence in which all the annotated repeats have been masked.\nRepeatMasker relies on a library of repeat sequences. We will use Dfam, a database of transposable element profile HMM models and consensus sequences."
  },
  {
    "objectID": "hands-on/07-repeats-analysis.html#objectives",
    "href": "hands-on/07-repeats-analysis.html#objectives",
    "title": "Repeats Analysis with RepeatMasker",
    "section": "",
    "text": "The primary objective of this chapter is to mask repetitive elements in our assembled genome to prepare it for downstream gene annotation. Repeat masking is essential because:\n\nGene prediction accuracy: Unmasked repeats can lead to false gene predictions, as gene-finding algorithms may incorrectly identify repetitive sequences as exons or coding regions.\nAnnotation quality: Proper repeat masking ensures that gene annotation tools focus on genuine gene sequences rather than being confused by repetitive elements.\nComparative analysis: Masked genomes provide cleaner datasets for comparative genomics and phylogenetic analyses.\n\nImportant note: While we will use RepeatModeler to identify de novo repeat families, our primary goal is comprehensive genome masking rather than the discovery of novel transposable elements. The RepeatModeler step ensures we capture species-specific repeats that may not be present in curated databases like Dfam, thereby improving the completeness of our repeat masking."
  },
  {
    "objectID": "hands-on/07-repeats-analysis.html#configuration-and-setup",
    "href": "hands-on/07-repeats-analysis.html#configuration-and-setup",
    "title": "Repeats Analysis with RepeatMasker",
    "section": "",
    "text": "We’ll continue using the established environment variables:\nexport NCPUS=12\nexport NPA=$(echo $NCPUS/4 | bc)\nexport SN=Fo47\n\n\n\nInstalling RepeatMasker and all its dependencies can be complex. To simplify this process, we will use a pre-built Apptainer/Docker container from the Dfam consortium, which includes RepeatMasker, RepeatModeler, and all necessary dependencies.\nWe will use the dfam/tetools container.\n\n\nWe will use an alias for the Apptainer command to simplify its usage.\nalias tetools=\"apptainer run docker://dfam/tetools:1.93\""
  },
  {
    "objectID": "hands-on/07-repeats-analysis.html#data-preparation",
    "href": "hands-on/07-repeats-analysis.html#data-preparation",
    "title": "Repeats Analysis with RepeatMasker",
    "section": "",
    "text": "We will use a Fusarium assembly from NCBI as the input for our repeat analysis.\nmkdir -p 07-repeats-analysis/assembly\nwget -P 07-repeats-analysis/assembly https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/013/085/055/GCF_013085055.1_ASM1308505v1/GCF_013085055.1_ASM1308505v1_genomic.fna.gz\ngunzip 07-repeats-analysis/assembly/GCF_013085055.1_ASM1308505v1_genomic.fna.gz\nNow we’ll process the assembly file to ensure all sequence lines are in uppercase and rename it appropriately:\n#!/usr/bin/env python3\nimport os\n\n# Process the assembly file: convert sequences to uppercase and rename\ninput_file = \"07-repeats-analysis/assembly/GCF_013085055.1_ASM1308505v1_genomic.fna\"\noutput_file = f\"07-repeats-analysis/assembly/{os.getenv('SN')}.fasta\"\n\nwith open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n    for line in infile:\n        if line.startswith('&gt;'):\n            outfile.write(line)\n        else:\n            outfile.write(line.upper())\n\n# Remove the original file\nos.remove(input_file)\n\n\n\nRepeatMasker requires a library of repeat sequences. We will use the Dfam database. Since we are analyzing a fungal genome, we will download the Dfam partition for fungi.\nAccording to the Dfam README, partition 16 contains Fungi.\nmkdir -p 07-repeats-analysis/libraries/famdb\nwget -P 07-repeats-analysis/libraries/famdb https://www.dfam.org/releases/current/families/FamDB/dfam39_full.0.h5.gz\nwget -P 07-repeats-analysis/libraries/famdb https://www.dfam.org/releases/current/families/FamDB/dfam39_full.16.h5.gz\ngunzip 07-repeats-analysis/libraries/famdb/*.gz\nThe tetools container comes with a minimal library. To use our custom library, we need to create a local Libraries directory and bind-mount it into the container. This approach avoids the NFS mounting issues that can occur with writable sandboxes.\nFirst, let’s copy the original libraries from the container to our host system to have a proper structure:\nmkdir -p 07-repeats-analysis/RepeatMasker\ntetools bash -c \"cp -r /opt/RepeatMasker/Libraries 07-repeats-analysis/RepeatMasker/\"\nNow, move our downloaded Dfam partitions into the local famdb directory:\nln -f 07-repeats-analysis/libraries/famdb/*.h5 07-repeats-analysis/RepeatMasker/Libraries/famdb/"
  },
  {
    "objectID": "hands-on/07-repeats-analysis.html#de-novo-repeat-identification-with-repeatmodeler",
    "href": "hands-on/07-repeats-analysis.html#de-novo-repeat-identification-with-repeatmodeler",
    "title": "Repeats Analysis with RepeatMasker",
    "section": "",
    "text": "Before running RepeatMasker with the curated Dfam library, we should first identify species-specific repeats using RepeatModeler. RepeatModeler builds a custom repeat library from our assembly using de novo repeat identification methods. This is particularly important for organisms that may have novel repeat families not present in existing databases.\n\n\nFirst, we need to create a database for RepeatModeler from our assembly:\nmkdir -p 07-repeats-analysis/repeatmodeler\ntetools BuildDatabase -name ${SN} 07-repeats-analysis/assembly/${SN}.fasta\nBuildDatabase parameters explained:\n\n-name: Database name (we use our sample identifier)\nPositional argument: Input assembly file\n\n\n\n\nNow we can run RepeatModeler to identify de novo repeat families. This process can take several hours to days depending on genome size and complexity:\ntetools RepeatModeler \\\n  -database ${SN} \\\n  -threads $NCPUS \\\n  -LTRStruct \\\n  &gt; 07-repeats-analysis/repeatmodeler/${SN}_repeatmodeler.log 2&gt;&1\nRepeatModeler parameters explained:\n\n-database: Name of the database created with BuildDatabase\n-threads: Number of parallel processes\n-LTRStruct: Enable LTR structural analysis for better LTR identification\n\n\n\n\nRepeatModeler creates several output files in a timestamped directory (e.g., RM_YYYYMMDD-HHMMSS). The key output files include:\n\n${SN}-families.fa: Main output - FASTA file containing consensus sequences of identified repeat families\n${SN}-families.stk: Stockholm format alignment file\n${SN}-rmod.log: Detailed log of the RepeatModeler run\n\nMove the results to our organized directory structure:\n# Move RepeatModeler results to our analysis directory\nmv ${SN}-families.fa 07-repeats-analysis/repeatmodeler/\nmv ${SN}-families.stk 07-repeats-analysis/repeatmodeler/\nmv ${SN}-rmod.log 07-repeats-analysis/repeatmodeler/\nmv RM_* 07-repeats-analysis/repeatmodeler/\nrm ${SN}.*\n\n\n\nYou can examine the identified repeat families:\n# Count the number of repeat families identified\ngrep -c \"&gt;\" 07-repeats-analysis/repeatmodeler/${SN}-families.fa\n\n# Look at the first few families\nhead -20 07-repeats-analysis/repeatmodeler/${SN}-families.fa\nThe headers contain classification information when available (e.g., &gt;rnd-1_family-174#LINE/L1) or #Unknown for unclassified elements.\n\n\n\nFirst, we need to reconfigure RepeatMasker to recognize our new Dfam partitions. We’ll use a bind mount to provide our custom libraries to the container and run the update script:\nBIND=\"--bind 07-repeats-analysis/RepeatMasker/Libraries:/opt/RepeatMasker/Libraries\"\nalias tetools_lib=\"apptainer run $BIND docker://dfam/tetools:1.93\"\ntetools_lib bash -c \"rm -f /opt/RepeatMasker/Libraries/famdb/rmlib.config && cd /opt/RepeatMasker && ./tetoolsDfamUpdate.pl\"\nThis command:\n\nBinds our local Libraries directory to /opt/RepeatMasker/Libraries in the container\nRemoves any existing configuration file\nRuns the update script to generate a new configuration based on our custom libraries\n\n\n\n\nSince the -lib and -species options are not compatible, we need to create a combined library that includes both the Dfam fungi sequences and our RepeatModeler-generated families. We’ll use the famdb.py tool to extract fungi sequences from our downloaded famdb files.\nFirst, extract fungi sequences from the Dfam database using famdb.py:\n# Extract fungi sequences from Dfam using famdb.py\ntetools_lib famdb.py -i /opt/RepeatMasker/Libraries/famdb \\\n  families --format fasta_name --ancestors --descendants 'Fungi' \\\n  --include-class-in-name &gt; 07-repeats-analysis/RepeatMasker/Libraries/dfam_fungi.fa\nNow combine the Dfam fungi sequences with our RepeatModeler families:\n# Create combined library\ncat 07-repeats-analysis/RepeatMasker/Libraries/dfam_fungi.fa \\\n    07-repeats-analysis/repeatmodeler/${SN}-families.fa \\\n    &gt; 07-repeats-analysis/RepeatMasker/Libraries/${SN}_combined.fa\nThis combined approach provides:\n\nCurated fungi repeats from the Dfam database\nNovel, species-specific repeats identified by RepeatModeler\nFull compatibility with RepeatMasker’s -lib option"
  },
  {
    "objectID": "hands-on/07-repeats-analysis.html#running-repeatmasker-with-database-only",
    "href": "hands-on/07-repeats-analysis.html#running-repeatmasker-with-database-only",
    "title": "Repeats Analysis with RepeatMasker",
    "section": "",
    "text": "Before using the combined library, let’s first run RepeatMasker using only the Dfam database to see how much repetitive content can be identified with curated repeats alone. This will help us understand the contribution of the de novo RepeatModeler families.\n\n\nRun RepeatMasker using only the species-specific Dfam database:\ntetools_lib RepeatMasker \\\n  -a \\\n  -pa $NPA \\\n  -species fungi \\\n  -dir 07-repeats-analysis/rm_out_dfam_only \\\n  -gff \\\n  07-repeats-analysis/assembly/${SN}.fasta &gt; 07-repeats-analysis/rm_out_dfam_only.log\n\n\n\nWith the Dfam database alone, you should expect to see:\n\nRepeat content (~1% or less) for this Fusarium assembly\nLimited identification of known repeat families\nGaps in repeat annotation due to species-specific repeats not present in Dfam\n\nThis demonstrates why de novo repeat identification with RepeatModeler is crucial for comprehensive repeat annotation, especially for organisms that may have novel or poorly characterized repeat families."
  },
  {
    "objectID": "hands-on/07-repeats-analysis.html#running-repeatmasker-with-combined-libraries",
    "href": "hands-on/07-repeats-analysis.html#running-repeatmasker-with-combined-libraries",
    "title": "Repeats Analysis with RepeatMasker",
    "section": "",
    "text": "To run RepeatMasker with our custom libraries, we will use bind mounts. This approach mounts our local Libraries directory over the container’s /opt/RepeatMasker/Libraries directory, making our custom Dfam files available to RepeatMasker without requiring a writable container.\n\n\nNow we can run RepeatMasker on our assembly, again using the bind mount to provide our custom libraries:\ntetools_lib RepeatMasker \\\n  -a \\\n  -pa $NPA \\\n  -lib /opt/RepeatMasker/Libraries/${SN}_combined.fa \\\n  -dir 07-repeats-analysis/rm_out \\\n  -gff \\\n  07-repeats-analysis/assembly/${SN}.fasta &gt; 07-repeats-analysis/rm_out.log\n# alternatively but longerto run, we can use -lib /opt/RepeatMasker/Libraries/${SN}_combined.fa\nRepeatMasker parameters explained:\n\n-a: Show alignments\n-pa: Number of parallel processes (threads = pa * 4).\n-species: Specifies the species or group for which to search repeats. This helps in selecting the appropriate repeat library from Dfam. (not compatible with -lib)\n-lib: Custom repeat library file (our RepeatModeler-generated families).\n-dir: Output directory for the results.\n-gff: Output a GFF file in addition to the standard output.\nPositional argument: The input assembly file.\n\nThis ensures comprehensive repeat annotation using both existing knowledge and de novo discoveries."
  },
  {
    "objectID": "hands-on/07-repeats-analysis.html#visualizing-repeatmasker-results",
    "href": "hands-on/07-repeats-analysis.html#visualizing-repeatmasker-results",
    "title": "Repeats Analysis with RepeatMasker",
    "section": "",
    "text": "RepeatMasker provides several built-in utilities for visualizing repeat content, and the output can also be used with various genome browsers and custom analysis tools.\n\n\nRepeatMasker includes utilities for creating repeat landscape plots that show the evolutionary age and abundance of different repeat families.\n\n\nThe repeat landscape shows the relationship between repeat divergence (evolutionary age) and abundance:\n# Generate divergence data\ntetools_lib calcDivergenceFromAlign.pl \\\n  -s 07-repeats-analysis/rm_out/${SN}.fasta.divsum \\\n  07-repeats-analysis/rm_out/${SN}.fasta.align\n\n# Create repeat landscape plot\ntetools_lib createRepeatLandscape.pl \\\n  -g 50400000 \\\n  -div 07-repeats-analysis/rm_out/${SN}.fasta.divsum \\\n  &gt; 07-repeats-analysis/rm_out/${SN}_repeat_landscape.html\nThis creates an HTML file with an interactive repeat landscape plot.\n\n\n\n\nThe GFF output can be directly loaded into genome browsers for visual inspection:\n\n\n\nLoad your assembly (${SN}.fasta) as the reference genome\nLoad the RepeatMasker GFF file (${SN}.fasta.out.gff) as an annotation track\nNavigate through the genome to examine repeat distribution\n\n\n\n\n\nThe .tbl file provides a comprehensive summary of repeat content:\n# View the repeat summary table\ncat 07-repeats-analysis/rm_out/${SN}.fasta.tbl\nThis table shows:\n\nTotal repeat content percentage\nBreakdown by major repeat classes (LTR, LINE, SINE, DNA transposons, etc.)\nInterspersed repeats vs. small RNA\nLow complexity and simple repeat content"
  },
  {
    "objectID": "hands-on/07-repeats-analysis.html#understanding-repeatmasker-output-metrics",
    "href": "hands-on/07-repeats-analysis.html#understanding-repeatmasker-output-metrics",
    "title": "Repeats Analysis with RepeatMasker",
    "section": "",
    "text": "The Kimura substitution level (also called Kimura distance or K2P distance) is a measure of evolutionary divergence between the identified repeat and its consensus sequence in the database. It represents the estimated percentage of nucleotide substitutions that have occurred since the repeat was originally inserted into the genome.\nKey points about Kimura substitution levels:\n\nLow values (0-5%): Recent insertions, relatively unchanged from the original sequence\nMedium values (5-20%): Moderately aged repeats with accumulated mutations\nHigh values (&gt;20%): Ancient repeats that have significantly diverged from their original sequence\n\nThe Kimura distance accounts for:\n\nTransition mutations (A↔︎G, C↔︎T) occurring more frequently than transversions\nMultiple substitutions at the same position (back-mutations)\nSaturation effects in highly diverged sequences\n\n\n\n\nIn RepeatMasker output tables, you may notice that the total percentage of repeats can sometimes exceed 100% of the genome size. This occurs due to several factors:\n1. Overlapping Annotations:\n\nDifferent repeat families may overlap in the same genomic region\nNested transposable elements (one inserted within another)\nRepeatMasker reports each identified repeat separately, even if they overlap\n\n2. Fragmented Repeat Detection:\n\nLarge repeats broken by mutations may be detected as multiple smaller fragments\nEach fragment contributes to the total count\nThe sum of fragments may exceed the actual genomic space occupied\n\n3. Reference vs. Assembly Differences: - Percentages are calculated relative to the assembly size, not the “true” genome size - If the assembly is incomplete or fragmented, percentages may be inflated - Conversely, if repeats are under-assembled (collapsed), percentages may be deflated\n4. Calculation Method: RepeatMasker calculates percentages as:\nPercentage = (Total bp of identified repeats / Assembly size) × 100\nThe “Total bp of identified repeats” includes all overlapping regions, which can exceed the actual physical space occupied.\nInterpreting the Results:\n\nFocus on relative proportions between repeat classes rather than absolute percentages\nUse the masked assembly to see the actual genomic regions covered by repeats\nConsider the non-redundant repeat content for more accurate genome composition estimates"
  },
  {
    "objectID": "hands-on/07-repeats-analysis.html#repeatmasker-approaches--species-vs.-famdb.py-lib",
    "href": "hands-on/07-repeats-analysis.html#repeatmasker-approaches--species-vs.-famdb.py-lib",
    "title": "Repeats Analysis with RepeatMasker",
    "section": "",
    "text": "RepeatMasker offers different strategies for accessing repeat libraries, each with distinct advantages and coverage levels:\n\n\nThe -species fungi option uses RepeatMasker’s built-in species-specific filtering:\n\nCurated selection: RepeatMasker selects only well-characterized repeats specific to the taxonomic group\nConservative approach: Focuses on high-confidence, validated repeat families\nLimited coverage: May miss divergent or poorly characterized repeats\nExpected result: ~1% repeat content for Fusarium (minimal coverage)\n\n\n\n\nUsing famdb.py to extract repeats allows more comprehensive library construction:\n\nTaxonomic breadth: Includes repeats from ancestral and descendant taxonomic groups\nComprehensive coverage: Captures more divergent repeat families that may be present\nExpected result: Higher repeat detection rate (not tested in this tutorial, but likely &gt;5%)\n\n\n\n\nThe most comprehensive strategy combines curated and de novo identified repeats:\n\nDfam coverage: Curated, well-characterized repeat families via famdb.py\nSpecies-specific coverage: Novel repeats identified by RepeatModeler\nMaximum sensitivity: Captures both known and novel repetitive elements\nExpected result: ~11% repeat content (most comprehensive masking)"
  },
  {
    "objectID": "hands-on/07-repeats-analysis.html#expected-repeat-content-results",
    "href": "hands-on/07-repeats-analysis.html#expected-repeat-content-results",
    "title": "Repeats Analysis with RepeatMasker",
    "section": "",
    "text": "Based on different RepeatMasker approaches, expected repeat content for this Fusarium assembly:\nTested approaches:\n\nUsing Dfam database alone (narrow with -species fungi option): ~1% repeat content (limited coverage)\nUsing de novo identified repeats only (RepeatModeler families): ~6% repeat content\nUsing combined approach (Dfam via famdb.py with ancestors/descendants + RepeatModeler): ~11% repeat content (comprehensive masking)\n\nNot tested but expected:\n\nUsing Dfam only (broad via famdb.py with ancestors/descendants): Likely 3-5% repeat content (moderate coverage)\n\nThe progression from 1% → 6% → 11% demonstrates the importance of combining curated databases with de novo repeat identification for comprehensive genome masking before gene annotation."
  },
  {
    "objectID": "hands-on/07-repeats-analysis.html#file-outputs-and-organization",
    "href": "hands-on/07-repeats-analysis.html#file-outputs-and-organization",
    "title": "Repeats Analysis with RepeatMasker",
    "section": "",
    "text": "After RepeatMasker finishes, the 07-repeats-analysis/rm_out directory will contain the results of the repeat analysis, including:\n\n*.masked: The input sequence with repeats masked (replaced by ’N’s).\n*.out: A detailed annotation of the repeats found.\n*.tbl: A summary table of the repeat content.\n*.gff: The repeat annotations in GFF format.\n*.align: Alignment details for repeat matches (if generated).\n*.divsum: Divergence summary (after running calcDivergenceFromAlign.pl).\n*_repeat_landscape.html: Interactive repeat landscape plot (if generated)."
  },
  {
    "objectID": "hands-on/09-annot-with-braker.html",
    "href": "hands-on/09-annot-with-braker.html",
    "title": "Genome Annotation with BRAKER",
    "section": "",
    "text": "In this chapter, we will perform genome annotation using BRAKER, a powerful pipeline for fully automated gene prediction in eukaryotic genomes. We will use the assembly of Fusarium oxysporum f. sp. lycopersici strain Fo47 that we have been working with.\nBRAKER combines GeneMark-ETP and AUGUSTUS, two highly accurate gene prediction tools, and can leverage evidence from RNA-Seq data and protein homology to improve annotation quality.\n\n\n\n\nWe will set up environment variables for our annotation workflow.\nexport SN=Fo47\nexport NCPUS=12\nexport assembly=\"$PWD/07-repeats-analysis/rm_out/Fo47.fasta.masked\"\nexport proteins=\"$PWD/08-data/orthodb/Fungi.fa\"\nexport rna_bam=\"$PWD/08-data/star/${SN}Aligned.sortedByCoord.out.bam\"\n\n\n\nWe will use the official BRAKER3 container.\n\n\nalias braker.pl=\"apptainer run --writable-tmpfs docker://teambraker/braker3:latest braker.pl\"\n\n\n\n\n\nIn this strategy, we will run BRAKER using only protein evidence from our curated Fungi OrthoDB database. This approach is useful when RNA-Seq data is not available or of poor quality.\n\nA note on using existing species parameters: AUGUSTUS comes with pre-trained parameters for many species, including Fusarium. While it is possible to use these as a starting point for training, the standard BRAKER workflow is to perform de novo training to create a new, highly optimized species model based on the provided evidence. In this tutorial, we will follow the standard workflow. The --species parameter in our braker.pl command gives a unique name to the new model that BRAKER will create.\n\nmkdir -p 09-annot/prot_only\nsed -E 's/^&gt;(\\S+).*$/&gt;\\1/' $assembly &gt; 09-annot/renamed_assembly.fa\nbraker.pl \\\n    --genome=09-annot/renamed_assembly.fa \\\n    --prot_seq=$proteins \\\n    --species=${SN}_prot \\\n    --workingdir=09-annot/prot_only \\\n    --threads=$NCPUS \\\n    --fungus\nBRAKER parameters explained:\n\n--genome: Path to the repeat-masked genome assembly.\n--prot_seq: Path to the protein database (OrthoDB Fungi).\n--species: A unique name for the species model being trained.\n--workingdir: Directory to store the output files.\n--threads: Number of CPU threads to use.\n--fungus: Specifies to use the fungal branch point model, which is appropriate for our organism.\n\n\n\n\nNow, we will run BRAKER using both protein and RNA-Seq evidence. This is the recommended approach as it combines homology-based evidence with transcriptomic evidence, leading to more accurate gene predictions.\nmkdir -p 09-annot/prot_rna\n# Make sure to enforce simple name without space (better if integrated at the mapping step) \nsed -E 's/^&gt;(\\S+).*$/&gt;\\1/' $assembly &gt; 09-annot/renamed_assembly.fa\n\nbraker.pl \\\n    --genome=09-annot/renamed_assembly.fa \\\n    --prot_seq=$proteins \\\n    --bam=$rna_bam \\\n    --species=${SN}_prot_rna \\\n    --workingdir=09-annot/prot_rna \\\n    --threads=$NCPUS \\\n    --fungus\nAdditional BRAKER parameter:\n\n--bam: Path to the sorted BAM file containing aligned RNA-Seq reads.\n\n\n\n\nAfter running BRAKER, each working directory (09-annot/prot_only and 09-annot/prot_rna) will contain several output files. The most important ones are:\n\nbraker.gtf: The final gene predictions in GTF format.\nbraker.aa: The predicted protein sequences in FASTA format.\nbraker.codingseq: The predicted coding sequences in FASTA format.\nGeneMark-EP/: Directory containing GeneMark-EP output (for protein-only mode).\nGeneMark-ETP/: Directory containing GeneMark-ETP output (for protein + RNA-Seq mode).\nAugustus/: Directory containing AUGUSTUS output and trained models.\n\nYou can compare the results from both strategies to see the impact of adding RNA-Seq evidence on the final gene annotations."
  },
  {
    "objectID": "hands-on/09-annot-with-braker.html#configuration-and-setup",
    "href": "hands-on/09-annot-with-braker.html#configuration-and-setup",
    "title": "Genome Annotation with BRAKER",
    "section": "",
    "text": "We will set up environment variables for our annotation workflow.\nexport SN=Fo47\nexport NCPUS=12\nexport assembly=\"$PWD/07-repeats-analysis/rm_out/Fo47.fasta.masked\"\nexport proteins=\"$PWD/08-data/orthodb/Fungi.fa\"\nexport rna_bam=\"$PWD/08-data/star/${SN}Aligned.sortedByCoord.out.bam\"\n\n\n\nWe will use the official BRAKER3 container.\n\n\nalias braker.pl=\"apptainer run --writable-tmpfs docker://teambraker/braker3:latest braker.pl\""
  },
  {
    "objectID": "hands-on/09-annot-with-braker.html#annotation-strategy-1-proteins-only",
    "href": "hands-on/09-annot-with-braker.html#annotation-strategy-1-proteins-only",
    "title": "Genome Annotation with BRAKER",
    "section": "",
    "text": "In this strategy, we will run BRAKER using only protein evidence from our curated Fungi OrthoDB database. This approach is useful when RNA-Seq data is not available or of poor quality.\n\nA note on using existing species parameters: AUGUSTUS comes with pre-trained parameters for many species, including Fusarium. While it is possible to use these as a starting point for training, the standard BRAKER workflow is to perform de novo training to create a new, highly optimized species model based on the provided evidence. In this tutorial, we will follow the standard workflow. The --species parameter in our braker.pl command gives a unique name to the new model that BRAKER will create.\n\nmkdir -p 09-annot/prot_only\nsed -E 's/^&gt;(\\S+).*$/&gt;\\1/' $assembly &gt; 09-annot/renamed_assembly.fa\nbraker.pl \\\n    --genome=09-annot/renamed_assembly.fa \\\n    --prot_seq=$proteins \\\n    --species=${SN}_prot \\\n    --workingdir=09-annot/prot_only \\\n    --threads=$NCPUS \\\n    --fungus\nBRAKER parameters explained:\n\n--genome: Path to the repeat-masked genome assembly.\n--prot_seq: Path to the protein database (OrthoDB Fungi).\n--species: A unique name for the species model being trained.\n--workingdir: Directory to store the output files.\n--threads: Number of CPU threads to use.\n--fungus: Specifies to use the fungal branch point model, which is appropriate for our organism."
  },
  {
    "objectID": "hands-on/09-annot-with-braker.html#annotation-strategy-2-proteins-rna-seq",
    "href": "hands-on/09-annot-with-braker.html#annotation-strategy-2-proteins-rna-seq",
    "title": "Genome Annotation with BRAKER",
    "section": "",
    "text": "Now, we will run BRAKER using both protein and RNA-Seq evidence. This is the recommended approach as it combines homology-based evidence with transcriptomic evidence, leading to more accurate gene predictions.\nmkdir -p 09-annot/prot_rna\n# Make sure to enforce simple name without space (better if integrated at the mapping step) \nsed -E 's/^&gt;(\\S+).*$/&gt;\\1/' $assembly &gt; 09-annot/renamed_assembly.fa\n\nbraker.pl \\\n    --genome=09-annot/renamed_assembly.fa \\\n    --prot_seq=$proteins \\\n    --bam=$rna_bam \\\n    --species=${SN}_prot_rna \\\n    --workingdir=09-annot/prot_rna \\\n    --threads=$NCPUS \\\n    --fungus\nAdditional BRAKER parameter:\n\n--bam: Path to the sorted BAM file containing aligned RNA-Seq reads."
  },
  {
    "objectID": "hands-on/09-annot-with-braker.html#expected-outputs",
    "href": "hands-on/09-annot-with-braker.html#expected-outputs",
    "title": "Genome Annotation with BRAKER",
    "section": "",
    "text": "After running BRAKER, each working directory (09-annot/prot_only and 09-annot/prot_rna) will contain several output files. The most important ones are:\n\nbraker.gtf: The final gene predictions in GTF format.\nbraker.aa: The predicted protein sequences in FASTA format.\nbraker.codingseq: The predicted coding sequences in FASTA format.\nGeneMark-EP/: Directory containing GeneMark-EP output (for protein-only mode).\nGeneMark-ETP/: Directory containing GeneMark-ETP output (for protein + RNA-Seq mode).\nAugustus/: Directory containing AUGUSTUS output and trained models.\n\nYou can compare the results from both strategies to see the impact of adding RNA-Seq evidence on the final gene annotations."
  },
  {
    "objectID": "hands-on/11-comparative-genomics.html",
    "href": "hands-on/11-comparative-genomics.html",
    "title": "Comparative Genomics",
    "section": "",
    "text": "In this chapter, we will explore comparative genomics to gain insights into the evolutionary relationships and conserved elements among different fungal genomes. We will use tools like PHAST for conservation analysis and OrthoFinder for identifying orthologous gene families.\n\n\n\n\nWe will use several tools for this analysis. Here’s how to set them up using containers.\n\n\n# For PHAST tools\nalias phyloFit=\"apptainer run docker://quay.io/comparative-genomics-toolkit/cactus:v2.9.9 phyloFit\"\nalias phyloP=\"apptainer run docker://quay.io/comparative-genomics-toolkit/cactus:v2.9.9 phyloP\"\nalias phastCons=\"apptainer run docker://quay.io/comparative-genomics-toolkit/cactus:v2.9.9 phastCons\"\n\n# For OrthoFinder\nalias orthofinder=\"apptainer run docker://quay.io/biocontainers/orthofinder:3.1.0--hdfd78af_0 orthofinder\"\n\n\n\n\nYou can also create a conda environment with the necessary tools.\nconda create -n compgenomics -c bioconda phast orthofinder\nconda activate compgenomics\n\n\n\n\nBefore running phastCons or phyloP, we need a phylogenetic model of neutral evolution. This model, typically in a .mod file, can be generated using phyloFit, a tool from the PHAST package.\nphyloFit requires a tree topology. This tree can be derived from: - The phylogenetic tree generated during the scaffolding step (e.g., from JolyTree in chapter 06). - A known species tree from taxonomic databases.\nHere is an example of how to run phyloFit with a given tree and alignment:\n# We can use the tree from the scaffolding chapter\n# The MAF file also comes from the scaffolding chapter\nexport TREE=\"((ME23,(ZUM2407,(V032g,Fo47))),tuto482);\"\nexport MAF_FILE=\"06-scaffolding/cactus/alignment.maf\"\n\necho \"$TREE\" &gt; tree.nwk\n\n# The tree needs to be in Newick format, e.g., \"((sp1,sp2),sp3);\"\n# Let's assume we have a file `tree.nwk` with the tree.\nphyloFit --tree tree.nwk --subst-mod REV --out-root neutral_model ${MAF_FILE}\nThis will generate neutral_model.mod, which we can use in the next steps.\n\n\n\nPHAST (PHylogenetic Analysis with Space/Time models) is a software package for comparative and evolutionary genomics. It includes tools for predicting conserved elements and measuring evolutionary conservation.\n\n\nphastCons is used to predict conserved elements from a multiple alignment. It uses a phylogenetic hidden Markov model (phylo-HMM) to identify segments of a genome that are likely to be under negative selection.\nFor more details, refer to the phastCons tutorial.\nWe can use the Multiple Alignment Format (MAF) file generated during the scaffolding step in chapter 06-scaffolding.qmd and the neutral model we just created.\n# Path to the MAF file from chapter 06\nexport MAF_FILE=\"06-scaffolding/cactus/alignment.maf\"\nexport NEUTRAL_MODEL=\"neutral_model.mod\"\n\n# Run phastCons\nphastCons --target-coverage 0.3 --expected-length 45 $MAF_FILE $NEUTRAL_MODEL --most-cons-regions cons.bed --score\n\n\n\nphyloP computes conservation or acceleration p-values based on a phylogenetic model. It can be used to identify specific sites that have evolved more slowly or more quickly than expected under a neutral model.\nFor more details, see the phyloP tutorial.\n\n\nThis example demonstrates how to run phyloP with an alignment and our neutral model file using the CONACC (CONservation/ACCeleration) mode and the Likelihood Ratio Test (LRT) method.\n# Using the MAF file and the neutral model\nexport MAF_FILE=\"06-scaffolding/cactus/alignment.maf\"\nexport NEUTRAL_MODEL=\"neutral_model.mod\"\n\nphyloP --mode CONACC --method LRT --wig-scores $NEUTRAL_MODEL $MAF_FILE &gt; phyloP_scores.wig\n\n\n\nphyloP also supports analyzing specific branches or subtrees in the phylogenetic tree. This is useful for identifying lineage-specific conservation or acceleration. You can specify a subtree of interest using the --subtree option.\n\n\n\n\n\nOrthoFinder is a fast, accurate, and scalable tool for inferring orthogroups, orthologs, and gene trees. It is widely used for comparative genomics studies.\nFor a detailed guide, refer to the OrthoFinder tutorial.\n\n\nWe need to gather the proteome files for the species we want to compare. For this analysis, we will use the proteomes of four Fusarium species and the one we annotated with BRAKER. It is a good practice to use only the longest transcript variant per gene to avoid inflating gene counts and improve accuracy.\n# Create directories\nmkdir -p 11-comparative-genomics/orthofinder/proteomes\nmkdir -p 11-comparative-genomics/orthofinder/primary_transcripts\n\n# Download proteomes\nwget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/013/085/055/GCF_013085055.1_ASM1308505v1/GCF_013085055.1_ASM1308505v1_protein.faa.gz -O 11-comparative-genomics/orthofinder/proteomes/Fo47.faa.gz\nwget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/048/165/035/GCA_048165035.1_ASM4816503v1/GCA_048165035.1_ASM4816503v1_protein.faa.gz -O 11-comparative-genomics/orthofinder/proteomes/ZUM2407.faa.gz\nwget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/048/164/945/GCA_048164945.1_ASM4816494v1/GCA_048164945.1_ASM4816494v1_protein.faa.gz -O 11-comparative-genomics/orthofinder/proteomes/V032g.faa.gz\nwget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/030/719/095/GCA_030719095.1_ASM3071909v1/GCA_030719095.1_ASM3071909v1_protein.faa.gz -O 11-comparative-genomics/orthofinder/proteomes/ME23.faa.gz\n\n# Decompress files\ngunzip 11-comparative-genomics/orthofinder/proteomes/*.gz\n\n# Copy our annotated proteome\n# Assuming the proteome from BRAKER (prot+rna) is the one to use\ncp 09-annot/prot_rna/braker.aa 11-comparative-genomics/orthofinder/proteomes/our_strain.faa\n\n# Download the script to select the longest transcript\nwget https://raw.githubusercontent.com/davidemms/OrthoFinder/master/tools/primary_transcript.py -O 11-comparative-genomics/orthofinder/primary_transcript.py\nchmod +x 11-comparative-genomics/orthofinder/primary_transcript.py\n\n# Run the script on each proteome file\nfor f in 11-comparative-genomics/orthofinder/proteomes/*.faa; do\n    python 11-comparative-genomics/orthofinder/primary_transcript.py $f\ndone\n\n# Move the resulting primary transcript files to a dedicated directory\nmv 11-comparative-genomics/orthofinder/proteomes/*_primary_transcript.fa 11-comparative-genomics/orthofinder/primary_transcripts/\n\n\n\nOnce the primary transcript files are ready, we can run OrthoFinder.\northofinder -f 11-comparative-genomics/orthofinder/primary_transcripts/ -o 11-comparative-genomics/orthofinder/results/\nOrthoFinder will create a results directory containing orthogroups, gene trees, and a species tree.\n\n\n\n\nThe orthogroups identified by OrthoFinder can be used for various downstream analyses to study gene evolution and function.\n\n\n\nUse introns for robust phylogenies: For closely related species, introns can be used to build robust phylogenies as they are often less constrained by selection than coding sequences (neutral evolution).\nAlign CDS with MACSE / T-Coffee: Align the coding sequences (CDS) of single-copy orthologs. MACSE is particularly useful as it aligns sequences based on their amino acid translation, which helps in maintaining the reading frame. T-Coffee can also be used, driven by the protein alignment to guide the CDS alignment.\n\n\n\n\n\nHyPhy (Hypothesis Testing using Phylogenies): Use the aligned CDS to test various evolutionary hypotheses, such as identifying sites under positive selection.\nPAML (Phylogenetic Analysis by Maximum Likelihood): Another powerful package for phylogenetic analysis of DNA or protein sequences. It can be used to estimate synonymous and non-synonymous substitution rates (dN/dS) to infer selective pressures.\n\nThese analyses can provide deep insights into the evolutionary dynamics of gene families and help in understanding the functional diversification of species."
  },
  {
    "objectID": "hands-on/11-comparative-genomics.html#environment-setup",
    "href": "hands-on/11-comparative-genomics.html#environment-setup",
    "title": "Comparative Genomics",
    "section": "",
    "text": "We will use several tools for this analysis. Here’s how to set them up using containers.\n\n\n# For PHAST tools\nalias phyloFit=\"apptainer run docker://quay.io/comparative-genomics-toolkit/cactus:v2.9.9 phyloFit\"\nalias phyloP=\"apptainer run docker://quay.io/comparative-genomics-toolkit/cactus:v2.9.9 phyloP\"\nalias phastCons=\"apptainer run docker://quay.io/comparative-genomics-toolkit/cactus:v2.9.9 phastCons\"\n\n# For OrthoFinder\nalias orthofinder=\"apptainer run docker://quay.io/biocontainers/orthofinder:3.1.0--hdfd78af_0 orthofinder\"\n\n\n\n\nYou can also create a conda environment with the necessary tools.\nconda create -n compgenomics -c bioconda phast orthofinder\nconda activate compgenomics"
  },
  {
    "objectID": "hands-on/11-comparative-genomics.html#generating-a-neutral-model-with-phylofit",
    "href": "hands-on/11-comparative-genomics.html#generating-a-neutral-model-with-phylofit",
    "title": "Comparative Genomics",
    "section": "",
    "text": "Before running phastCons or phyloP, we need a phylogenetic model of neutral evolution. This model, typically in a .mod file, can be generated using phyloFit, a tool from the PHAST package.\nphyloFit requires a tree topology. This tree can be derived from: - The phylogenetic tree generated during the scaffolding step (e.g., from JolyTree in chapter 06). - A known species tree from taxonomic databases.\nHere is an example of how to run phyloFit with a given tree and alignment:\n# We can use the tree from the scaffolding chapter\n# The MAF file also comes from the scaffolding chapter\nexport TREE=\"((ME23,(ZUM2407,(V032g,Fo47))),tuto482);\"\nexport MAF_FILE=\"06-scaffolding/cactus/alignment.maf\"\n\necho \"$TREE\" &gt; tree.nwk\n\n# The tree needs to be in Newick format, e.g., \"((sp1,sp2),sp3);\"\n# Let's assume we have a file `tree.nwk` with the tree.\nphyloFit --tree tree.nwk --subst-mod REV --out-root neutral_model ${MAF_FILE}\nThis will generate neutral_model.mod, which we can use in the next steps."
  },
  {
    "objectID": "hands-on/11-comparative-genomics.html#predicting-conserved-elements-with-phast",
    "href": "hands-on/11-comparative-genomics.html#predicting-conserved-elements-with-phast",
    "title": "Comparative Genomics",
    "section": "",
    "text": "PHAST (PHylogenetic Analysis with Space/Time models) is a software package for comparative and evolutionary genomics. It includes tools for predicting conserved elements and measuring evolutionary conservation.\n\n\nphastCons is used to predict conserved elements from a multiple alignment. It uses a phylogenetic hidden Markov model (phylo-HMM) to identify segments of a genome that are likely to be under negative selection.\nFor more details, refer to the phastCons tutorial.\nWe can use the Multiple Alignment Format (MAF) file generated during the scaffolding step in chapter 06-scaffolding.qmd and the neutral model we just created.\n# Path to the MAF file from chapter 06\nexport MAF_FILE=\"06-scaffolding/cactus/alignment.maf\"\nexport NEUTRAL_MODEL=\"neutral_model.mod\"\n\n# Run phastCons\nphastCons --target-coverage 0.3 --expected-length 45 $MAF_FILE $NEUTRAL_MODEL --most-cons-regions cons.bed --score\n\n\n\nphyloP computes conservation or acceleration p-values based on a phylogenetic model. It can be used to identify specific sites that have evolved more slowly or more quickly than expected under a neutral model.\nFor more details, see the phyloP tutorial.\n\n\nThis example demonstrates how to run phyloP with an alignment and our neutral model file using the CONACC (CONservation/ACCeleration) mode and the Likelihood Ratio Test (LRT) method.\n# Using the MAF file and the neutral model\nexport MAF_FILE=\"06-scaffolding/cactus/alignment.maf\"\nexport NEUTRAL_MODEL=\"neutral_model.mod\"\n\nphyloP --mode CONACC --method LRT --wig-scores $NEUTRAL_MODEL $MAF_FILE &gt; phyloP_scores.wig\n\n\n\nphyloP also supports analyzing specific branches or subtrees in the phylogenetic tree. This is useful for identifying lineage-specific conservation or acceleration. You can specify a subtree of interest using the --subtree option."
  },
  {
    "objectID": "hands-on/11-comparative-genomics.html#finding-orthologs-with-orthofinder",
    "href": "hands-on/11-comparative-genomics.html#finding-orthologs-with-orthofinder",
    "title": "Comparative Genomics",
    "section": "",
    "text": "OrthoFinder is a fast, accurate, and scalable tool for inferring orthogroups, orthologs, and gene trees. It is widely used for comparative genomics studies.\nFor a detailed guide, refer to the OrthoFinder tutorial.\n\n\nWe need to gather the proteome files for the species we want to compare. For this analysis, we will use the proteomes of four Fusarium species and the one we annotated with BRAKER. It is a good practice to use only the longest transcript variant per gene to avoid inflating gene counts and improve accuracy.\n# Create directories\nmkdir -p 11-comparative-genomics/orthofinder/proteomes\nmkdir -p 11-comparative-genomics/orthofinder/primary_transcripts\n\n# Download proteomes\nwget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/013/085/055/GCF_013085055.1_ASM1308505v1/GCF_013085055.1_ASM1308505v1_protein.faa.gz -O 11-comparative-genomics/orthofinder/proteomes/Fo47.faa.gz\nwget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/048/165/035/GCA_048165035.1_ASM4816503v1/GCA_048165035.1_ASM4816503v1_protein.faa.gz -O 11-comparative-genomics/orthofinder/proteomes/ZUM2407.faa.gz\nwget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/048/164/945/GCA_048164945.1_ASM4816494v1/GCA_048164945.1_ASM4816494v1_protein.faa.gz -O 11-comparative-genomics/orthofinder/proteomes/V032g.faa.gz\nwget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/030/719/095/GCA_030719095.1_ASM3071909v1/GCA_030719095.1_ASM3071909v1_protein.faa.gz -O 11-comparative-genomics/orthofinder/proteomes/ME23.faa.gz\n\n# Decompress files\ngunzip 11-comparative-genomics/orthofinder/proteomes/*.gz\n\n# Copy our annotated proteome\n# Assuming the proteome from BRAKER (prot+rna) is the one to use\ncp 09-annot/prot_rna/braker.aa 11-comparative-genomics/orthofinder/proteomes/our_strain.faa\n\n# Download the script to select the longest transcript\nwget https://raw.githubusercontent.com/davidemms/OrthoFinder/master/tools/primary_transcript.py -O 11-comparative-genomics/orthofinder/primary_transcript.py\nchmod +x 11-comparative-genomics/orthofinder/primary_transcript.py\n\n# Run the script on each proteome file\nfor f in 11-comparative-genomics/orthofinder/proteomes/*.faa; do\n    python 11-comparative-genomics/orthofinder/primary_transcript.py $f\ndone\n\n# Move the resulting primary transcript files to a dedicated directory\nmv 11-comparative-genomics/orthofinder/proteomes/*_primary_transcript.fa 11-comparative-genomics/orthofinder/primary_transcripts/\n\n\n\nOnce the primary transcript files are ready, we can run OrthoFinder.\northofinder -f 11-comparative-genomics/orthofinder/primary_transcripts/ -o 11-comparative-genomics/orthofinder/results/\nOrthoFinder will create a results directory containing orthogroups, gene trees, and a species tree."
  },
  {
    "objectID": "hands-on/11-comparative-genomics.html#downstream-analysis-of-orthologs",
    "href": "hands-on/11-comparative-genomics.html#downstream-analysis-of-orthologs",
    "title": "Comparative Genomics",
    "section": "",
    "text": "The orthogroups identified by OrthoFinder can be used for various downstream analyses to study gene evolution and function.\n\n\n\nUse introns for robust phylogenies: For closely related species, introns can be used to build robust phylogenies as they are often less constrained by selection than coding sequences (neutral evolution).\nAlign CDS with MACSE / T-Coffee: Align the coding sequences (CDS) of single-copy orthologs. MACSE is particularly useful as it aligns sequences based on their amino acid translation, which helps in maintaining the reading frame. T-Coffee can also be used, driven by the protein alignment to guide the CDS alignment.\n\n\n\n\n\nHyPhy (Hypothesis Testing using Phylogenies): Use the aligned CDS to test various evolutionary hypotheses, such as identifying sites under positive selection.\nPAML (Phylogenetic Analysis by Maximum Likelihood): Another powerful package for phylogenetic analysis of DNA or protein sequences. It can be used to estimate synonymous and non-synonymous substitution rates (dN/dS) to infer selective pressures.\n\nThese analyses can provide deep insights into the evolutionary dynamics of gene families and help in understanding the functional diversification of species."
  },
  {
    "objectID": "more/intro-nextflow.html",
    "href": "more/intro-nextflow.html",
    "title": "Intro to NextFlow",
    "section": "",
    "text": "1 Introduction to Workflow design with NextFlow\nContent for Introduction to Workflow design with NextFlow."
  },
  {
    "objectID": "primary-analysis/base-calling-qc.html",
    "href": "primary-analysis/base-calling-qc.html",
    "title": "Base Calling & QC",
    "section": "",
    "text": "Sanger Sequencing is the first game-changing sequencing method, it was developed in the 1970s (Sanger, Nicklen, and Coulson 1977) and was used to sequence the first human genome in 2001. Sanger sequencing is a chain termination method, it uses a mixture of dNTPs and ddNTPs to terminate the DNA synthesis at a random position (Sanger, Nicklen, and Coulson 1977).\n\n\n\nSanger sequencing\n\n\nAs show on the figure above, Sanger sequencing will produce multiple fragments of DNA, each of which is a different length. The introduction of ddNTP will stop the DNA synthesis at a random position that match the base of the ddNTP. Later the fragments are separated by their length using gel electrophoresis. Reading each column in the gel will give the sequence of the DNA. The process is optimized with the use of fluorescent dyes that are incorporated into the DNA where only one reaction is enough the the sequence is interpreted based on the color.\n\n\n\nThe Second generation Sequencing (SGS) is a method of sequencing DNA. While Sanger sequencing is a chain termination method, SGS is a sequencing by synthesis method.\nThis process was further industrialized by the use of capillary electrophoresis to separate the fragments as known today as Sanger sequencing.\nSecond Generation Sequencing (SGS) is a method of sequencing DNA that was developed in the early 2000s. The principle is that during the DNA synthesis, at each base incorporation a attached dye is fluorescent when excited by a laser. However the dye is no longer fluorescent when the next base is incorporated. The sequence is then read by the laser as a series of fluorescent signals.\n\n\n\nSequencing by synthesis\n\n\nThe signal read not on a single template but on a large number of templates simultaneously designated as cluster (polonies for polymerase colonies). The reason why SBS produce relatively short reads is linked with this clustering process. Indeed the polymerase activity is not linear and the signal is not uniform across the cluster. When some template are late it results in a mixing signal from the previous cycle to prevent ultimately the signal analysis.\nThis process was industrialized by companies such as Illumina where templates are deposited on a flowcell where a clustering process allow the forming of cluster for optimal signal acquisition.\n\n\n\nIllumina Technology from https://www.illumina.com/documents/products/techspotlights/techspotlight_sequencing.pdf\n\n\nSo basically the sequencing process is based on Image Analysis. In addition to the base calling, the image analysis also allows to determine the quality of the sequencing. To this end both the cluster intensity and the signal-to-noise ratio are exploited to estimated a quality score. The quality score is a Phred score (log10 of the probability of error) and is reported for each base. As shown below the Quality score is a prediction made from the sequencing of known sequences.\n\n\n\nQ-score correlation with error rate from https://knowledge.illumina.com/instrumentation/general/instrumentation-general-reference_material-list/000006557\n\n\nIt’s interesting to not that since, Illumina developed less expensive technology based on a double channel chemistry with only 2 dye for the 4 bases - and even a single channel chemistry. Nonetheless the methods might induce artefacts such as polyG tails at the end of reads due to the absence of base for short fragments.\n\n\n\nSBS Chemistries comparison\n\n\n\nREF: https://emea.illumina.com/science/technology/next-generation-sequencing/sequencing-technology/2-channel-sbs.html\n\n\n\nFragment: The DNA fragment that will be sequenced. Read: The sequence read from the Fragment. Library: Fragment enriched with its adapters/barcode after library preparation. Insert: The part of the fragment which is not sequenced\n\n\n\nGlossary of components of a library\n\n\n\n\n\nDepending on library size then can be different issues: 1. In case of too short fragments, the reads can overlaps (if paired) 2. In shorted gain, the reads can be contaminated by the adapter 3. If shorter than the read length, there will be no signal and it might be problematic for 2 color chemistry and induce polyG tail artifacts.\n\n\n\nExample of issues depending on library size\n\n\n\n\n\nThe DNA library preparation is a critical step. Important criteria are:\n\nThe fragmentation method\nThe fragment size (higher the better)\nThe Amplification method (Amplification-free is better)\n\n\n\n\n\nThe third generation of sequencing focuses on both long read sequencing and direct DNA sequencing (without prior library preparation, for Epigenetics not covered here) (van Dijk et al. 2018).\nNowdays there are 2 leading provider for long read sequencing (Lang et al. 2020):\nPacBio (Pacific Biosciences) TGS Sequencing:\n\nUses Single Molecule Real-Time (SMRT) technology\nDNA synthesis detected by fluorescent labels in real time\nGenerates long, high-accuracy reads (&gt;10 kb, especially with HiFi mode)\nWell-suited for full-length transcript sequencing, genome assembly, and detecting structural variants\n\nOxford Nanopore TGS Sequencing:\n\nDNA passes through a protein nanopore, changing electrical current\nMeasures current changes to directly read DNA sequence in real time\nCan produce ultra-long reads (some over 100 kb)\nPortable devices (e.g., MinION) and rapid results useful for fieldwork, genome assembly, and epigenetic studies\n\nOf note there are also other technologies to obtain long reads such as True Synthetic Long Reads (TSLR), not covered here.\n\n\nNanopore base calling is fundamentally different from other sequencing technologies as it directly reads DNA sequences through changes in electrical current as nucleotides pass through protein nanopores. The base calling accuracy has improved significantly with advances in both hardware and computational algorithms. Various base calling tools have been developed and benchmarked to optimize accuracy and speed (Wick, Judd, and Holt 2018), with newer neural network-based approaches showing substantial improvements over earlier methods.\n\n\n\n\nDifferent sequencing technologies exhibit distinct error profiles and biases (Ross et al. 2013):\nIllumina (Second Generation):\n\nSubstitution errors are most common (~0.1-1% error rate)\nGC bias affects coverage uniformity\nQuality degrades toward the end of reads\nContext-specific errors in homopolymers\n\nPacBio (Third Generation):\n\nHigher overall error rate (10-15%) but randomly distributed\nMainly insertion and deletion errors\nMinimal GC bias compared to short-read technologies\nHiFi mode significantly improves accuracy\n\nOxford Nanopore (Third Generation):\n\nVariable error rates (5-15%) depending on base calling algorithms (Wick, Judd, and Holt 2018)\nPrimarily indel errors, especially in homopolymer regions\nSystematic errors can be reduced with improved base calling\n\nUnderstanding these error profiles is crucial for downstream analysis and quality control.\n\n\n\n\n\nPrior sequencing it’s best to select an individual which is representative. In case of a species with different chromosomes copy number depending on sex, the sex with the most chromosome shall be prioritized (E.g. female for human). The best would be to sequence both.\nAlso it’s important to select the right tissue considering: - The potential somatic mutations - The concentration of DNA after extraction and purification - With a known ploidy, the smaller the better\nIt might be useful to study the karyotype of the species to select the right tissue.\n\n\n\nThe DNA extraction is a critical step. It’s important to use a protocol that is well adapted to the species and the tissue. The DNA should be of high quality and quantity. The DNA should be free of contaminants and degraded DNA.\n\n\n\n\nTo see how QC is done, look at the QC hands on pratice."
  },
  {
    "objectID": "primary-analysis/base-calling-qc.html#fundamentals",
    "href": "primary-analysis/base-calling-qc.html#fundamentals",
    "title": "Base Calling & QC",
    "section": "",
    "text": "Sanger Sequencing is the first game-changing sequencing method, it was developed in the 1970s (Sanger, Nicklen, and Coulson 1977) and was used to sequence the first human genome in 2001. Sanger sequencing is a chain termination method, it uses a mixture of dNTPs and ddNTPs to terminate the DNA synthesis at a random position (Sanger, Nicklen, and Coulson 1977).\n\n\n\nSanger sequencing\n\n\nAs show on the figure above, Sanger sequencing will produce multiple fragments of DNA, each of which is a different length. The introduction of ddNTP will stop the DNA synthesis at a random position that match the base of the ddNTP. Later the fragments are separated by their length using gel electrophoresis. Reading each column in the gel will give the sequence of the DNA. The process is optimized with the use of fluorescent dyes that are incorporated into the DNA where only one reaction is enough the the sequence is interpreted based on the color."
  },
  {
    "objectID": "primary-analysis/base-calling-qc.html#introduction-to-second-generation-sequencing",
    "href": "primary-analysis/base-calling-qc.html#introduction-to-second-generation-sequencing",
    "title": "Base Calling & QC",
    "section": "",
    "text": "The Second generation Sequencing (SGS) is a method of sequencing DNA. While Sanger sequencing is a chain termination method, SGS is a sequencing by synthesis method.\nThis process was further industrialized by the use of capillary electrophoresis to separate the fragments as known today as Sanger sequencing.\nSecond Generation Sequencing (SGS) is a method of sequencing DNA that was developed in the early 2000s. The principle is that during the DNA synthesis, at each base incorporation a attached dye is fluorescent when excited by a laser. However the dye is no longer fluorescent when the next base is incorporated. The sequence is then read by the laser as a series of fluorescent signals.\n\n\n\nSequencing by synthesis\n\n\nThe signal read not on a single template but on a large number of templates simultaneously designated as cluster (polonies for polymerase colonies). The reason why SBS produce relatively short reads is linked with this clustering process. Indeed the polymerase activity is not linear and the signal is not uniform across the cluster. When some template are late it results in a mixing signal from the previous cycle to prevent ultimately the signal analysis.\nThis process was industrialized by companies such as Illumina where templates are deposited on a flowcell where a clustering process allow the forming of cluster for optimal signal acquisition.\n\n\n\nIllumina Technology from https://www.illumina.com/documents/products/techspotlights/techspotlight_sequencing.pdf\n\n\nSo basically the sequencing process is based on Image Analysis. In addition to the base calling, the image analysis also allows to determine the quality of the sequencing. To this end both the cluster intensity and the signal-to-noise ratio are exploited to estimated a quality score. The quality score is a Phred score (log10 of the probability of error) and is reported for each base. As shown below the Quality score is a prediction made from the sequencing of known sequences.\n\n\n\nQ-score correlation with error rate from https://knowledge.illumina.com/instrumentation/general/instrumentation-general-reference_material-list/000006557\n\n\nIt’s interesting to not that since, Illumina developed less expensive technology based on a double channel chemistry with only 2 dye for the 4 bases - and even a single channel chemistry. Nonetheless the methods might induce artefacts such as polyG tails at the end of reads due to the absence of base for short fragments.\n\n\n\nSBS Chemistries comparison\n\n\n\nREF: https://emea.illumina.com/science/technology/next-generation-sequencing/sequencing-technology/2-channel-sbs.html\n\n\n\nFragment: The DNA fragment that will be sequenced. Read: The sequence read from the Fragment. Library: Fragment enriched with its adapters/barcode after library preparation. Insert: The part of the fragment which is not sequenced\n\n\n\nGlossary of components of a library\n\n\n\n\n\nDepending on library size then can be different issues: 1. In case of too short fragments, the reads can overlaps (if paired) 2. In shorted gain, the reads can be contaminated by the adapter 3. If shorter than the read length, there will be no signal and it might be problematic for 2 color chemistry and induce polyG tail artifacts.\n\n\n\nExample of issues depending on library size\n\n\n\n\n\nThe DNA library preparation is a critical step. Important criteria are:\n\nThe fragmentation method\nThe fragment size (higher the better)\nThe Amplification method (Amplification-free is better)"
  },
  {
    "objectID": "primary-analysis/base-calling-qc.html#third-generation-sequencing",
    "href": "primary-analysis/base-calling-qc.html#third-generation-sequencing",
    "title": "Base Calling & QC",
    "section": "",
    "text": "The third generation of sequencing focuses on both long read sequencing and direct DNA sequencing (without prior library preparation, for Epigenetics not covered here) (van Dijk et al. 2018).\nNowdays there are 2 leading provider for long read sequencing (Lang et al. 2020):\nPacBio (Pacific Biosciences) TGS Sequencing:\n\nUses Single Molecule Real-Time (SMRT) technology\nDNA synthesis detected by fluorescent labels in real time\nGenerates long, high-accuracy reads (&gt;10 kb, especially with HiFi mode)\nWell-suited for full-length transcript sequencing, genome assembly, and detecting structural variants\n\nOxford Nanopore TGS Sequencing:\n\nDNA passes through a protein nanopore, changing electrical current\nMeasures current changes to directly read DNA sequence in real time\nCan produce ultra-long reads (some over 100 kb)\nPortable devices (e.g., MinION) and rapid results useful for fieldwork, genome assembly, and epigenetic studies\n\nOf note there are also other technologies to obtain long reads such as True Synthetic Long Reads (TSLR), not covered here.\n\n\nNanopore base calling is fundamentally different from other sequencing technologies as it directly reads DNA sequences through changes in electrical current as nucleotides pass through protein nanopores. The base calling accuracy has improved significantly with advances in both hardware and computational algorithms. Various base calling tools have been developed and benchmarked to optimize accuracy and speed (Wick, Judd, and Holt 2018), with newer neural network-based approaches showing substantial improvements over earlier methods."
  },
  {
    "objectID": "primary-analysis/base-calling-qc.html#current-sequencing-errors-type-and-rate-per-sequencing-techno",
    "href": "primary-analysis/base-calling-qc.html#current-sequencing-errors-type-and-rate-per-sequencing-techno",
    "title": "Base Calling & QC",
    "section": "",
    "text": "Different sequencing technologies exhibit distinct error profiles and biases (Ross et al. 2013):\nIllumina (Second Generation):\n\nSubstitution errors are most common (~0.1-1% error rate)\nGC bias affects coverage uniformity\nQuality degrades toward the end of reads\nContext-specific errors in homopolymers\n\nPacBio (Third Generation):\n\nHigher overall error rate (10-15%) but randomly distributed\nMainly insertion and deletion errors\nMinimal GC bias compared to short-read technologies\nHiFi mode significantly improves accuracy\n\nOxford Nanopore (Third Generation):\n\nVariable error rates (5-15%) depending on base calling algorithms (Wick, Judd, and Holt 2018)\nPrimarily indel errors, especially in homopolymer regions\nSystematic errors can be reduced with improved base calling\n\nUnderstanding these error profiles is crucial for downstream analysis and quality control."
  },
  {
    "objectID": "primary-analysis/base-calling-qc.html#how-to-get-dna-for-sequencing",
    "href": "primary-analysis/base-calling-qc.html#how-to-get-dna-for-sequencing",
    "title": "Base Calling & QC",
    "section": "",
    "text": "Prior sequencing it’s best to select an individual which is representative. In case of a species with different chromosomes copy number depending on sex, the sex with the most chromosome shall be prioritized (E.g. female for human). The best would be to sequence both.\nAlso it’s important to select the right tissue considering: - The potential somatic mutations - The concentration of DNA after extraction and purification - With a known ploidy, the smaller the better\nIt might be useful to study the karyotype of the species to select the right tissue.\n\n\n\nThe DNA extraction is a critical step. It’s important to use a protocol that is well adapted to the species and the tissue. The DNA should be of high quality and quantity. The DNA should be free of contaminants and degraded DNA."
  },
  {
    "objectID": "primary-analysis/base-calling-qc.html#qcs",
    "href": "primary-analysis/base-calling-qc.html#qcs",
    "title": "Base Calling & QC",
    "section": "",
    "text": "To see how QC is done, look at the QC hands on pratice."
  },
  {
    "objectID": "secondary-analysis/assembly-evaluation.html",
    "href": "secondary-analysis/assembly-evaluation.html",
    "title": "Assembly Evaluation",
    "section": "",
    "text": "1 Assembly Evaluation\nContent for Assembly Evaluation."
  },
  {
    "objectID": "secondary-analysis/contigs-de-novo-assembly.html",
    "href": "secondary-analysis/contigs-de-novo-assembly.html",
    "title": "Contigs de novo Assembly",
    "section": "",
    "text": "1 Contigs de novo assembly\nContent for Contigs de novo assembly."
  },
  {
    "objectID": "tertiary-analysis/comparative-genomics.html",
    "href": "tertiary-analysis/comparative-genomics.html",
    "title": "Comparative Genomics",
    "section": "",
    "text": "1 Comparative Genomics\nContent for Comparative Genomics."
  }
]